<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE TEI.2 SYSTEM "tei_bawe.dtd"><TEI.2 id="_0146f" n="version 1.0"><teiHeader><fileDesc><titleStmt><title>Assignment 6 - Summary of Course Contents</title></titleStmt><extent/><publicationStmt><distributor>British Academic Written English (BAWE) corpus</distributor><availability><p>The British Academic Written English (BAWE) corpus was developed at the Universities of Warwick, Reading and Oxford Brookes, under the directorship of Hilary Nesi and Sheena Gardner (formerly of the Centre for Applied Linguistics [previously called CELTE], Warwick), Paul Thompson (Department of Applied Linguistics, Reading) and Paul Wickens (Westminster Institute of Education, Oxford Brookes), with funding from the ESRC. Subject to the rights of the these institutions in the BAWE corpus, and pursuant to the ESRC agreement, the BAWE corpus is available to researchers for research purposes PROVIDED THAT the following conditions are met:</p><p>1. The corpus files are not distributed in either their original form or in modified form.</p><p>2. The texts are used for research purposes only; they should not be reproduced in teaching materials.</p><p>3. The texts are not reproduced in full for a wider audience/readership, although researchers are free to quote short passages of text (up to 200 running words from any given text).</p><p>4. The BAWE corpus developers (contact: Hilary Nesi) are informed of all projects, dissertations, theses, presentations or publications arising from analysis of the corpus.</p><p>5. Researchers acknowledge their use of the corpus using the following form of words: "The data in this study come from the British Academic Written English (BAWE) corpus, which was developed at the Universities of Warwick, Reading and Oxford Brookes under the directorship of Hilary Nesi and Sheena Gardner (formerly of the Centre for Applied Linguistics [previously called CELTE], Warwick), Paul Thompson (Department of Applied Linguistics, Reading) and Paul Wickens (Westminster Institute of Education, Oxford Brookes), with funding from the ESRC (RES-000-23-0800)."</p></availability></publicationStmt><notesStmt><note resp="British Academic Written English (BAWE) corpus project">Language used in quote: <foreign id="English">English</foreign></note><note resp="British Academic Written English (BAWE) corpus project">header: date
footer: page numbers</note><note resp="British Academic Written English (BAWE) corpus project">appendix: "My Matlab files"</note></notesStmt><sourceDesc><p n="level">4</p><p n="date">2004-11</p><p n="module title">Intelligent Systems Engineering</p><p n="module code">ES477</p><p n="genre family">Research report</p><p n="discipline">Engineering</p><p n="disciplinary group">PS</p><p n="grade">D</p><p n="number of authors">1</p><p n="number of words">5323</p><p n="number of s-units">210</p><p n="number of p">88</p><p n="number of tables">5</p><p n="number of figures">16</p><p n="number of block quotes">15</p><p n="number of formulae">21</p><p n="number of lists">1</p><p n="number of paragraphs formatted like lists">0</p><p n="abstract present">abstract present</p><p n="average words per s-unit">25.3</p><p n="average s-units per p">2.4</p><p n="macrotype of assignment">simple assignment</p></sourceDesc></fileDesc><encodingDesc><p>TEI P4 (documented in: BAWE.documentation.pdf)</p></encodingDesc><profileDesc><particDesc><person><p n="gender">m</p><p n="year of birth">1983</p><p n="first language">English</p><p n="education">UKA</p><p n="course">Electronics &amp; Communications</p><p n="student ID">0146</p></person></particDesc></profileDesc></teiHeader><text><front><titlePage><docTitle><titlePart rend="underlined bold">Intelligent Systems Engineering - Assignment 6</titlePart></docTitle></titlePage></front><body><div1 type="abstract"><head rend="underlined bold">Abstract (overview/synopsis of work and results/conclusion)</head><p n="p1.88"><s n="s1.4;p1.88">A study into the various approaches available in Intelligent Systems Engineering has been carried out. </s><s n="s2.4;p1.88">The context for this piece of work, as well as describing the theoretical aspects and simulation results, has been that of the XNOR problem. </s><s n="s3.4;p1.88">It was found that Expert Systems, Genetic Algorithms and Fuzzy Logic would be acceptable approaches to this problem. </s><s n="s4.4;p1.88">The report highlights key points and includes an appendix with some new programs that were created. </s></p></div1><div1 type="section"><head rend="underlined bold">Introduction</head><p n="p2.88"><s n="s1.2;p2.88">This report aims to describe the principles and relative merits of some Intelligent Systems Engineering approaches. </s><s n="s2.2;p2.88">The approaches to be discussed are Expert Systems, Unsupervised Learning, Supervised Learning, Genetic Algorithms, Fuzzy Logic and Neuro-Fuzzy. </s></p><p n="p3.88"><s n="s1.1;p3.88">Besides looking at some of the theoretical aspects and most of the simulation results for these approaches an attempt will also be made. </s></p><p n="p4.88"><s n="s1.4;p4.88">Context: solution to XNOR problem. </s><s n="s2.4;p4.88">This has been chosen since it is a simple problem that given the 15 pages suggested page limit of this report will not require multiple graphs. </s><s n="s3.4;p4.88">For fuzzy problems the idea is taken further by having multiple input gates and the input values been in the range -2 to +2 instead of fixed at -1 to represent logic low and +1 to represent logic high. </s><s n="s4.4;p4.88">Here is the XNOR gate: </s></p><figure id="BAWE_0146f-fig.001"><head rend="bold italic"> Figure 1: XNOR gate and truth table</head></figure><table id="BAWE_0146f-tab.001"><head rend="bold italic">Figure 2: Equivalent XNOR truth table</head><row><cell/></row></table><p n="p5.88"><s n="s1.1;p5.88">This can be represented in a matrix as: <formula notation="" id="BAWE_0146f-form.001"/> </s></p></div1><div1 type="section"><head rend="bold">a) Expert Systems:</head><p n="p6.88"><s n="s1.6;p6.88">Expert systems attempt to make knowledge based decisions often using if...then type rules. </s><s n="s2.6;p6.88">They should only be used for applications that cannot be easily or cheaply solved by existing software or human intelligence. </s><s n="s3.6;p6.88">They work from large knowledge bases that contain data selected to help the expert system perform its function. </s><s n="s4.6;p6.88">In essence, a rule-based expert system, uses a combination of it's 'if...then' rules to make decisions. </s><s n="s5.6;p6.88">Other ways to implement expert systems are Frame-based, Procedure-oriented, Object-oriented, Logic-based and Access-oriented. </s><s n="s6.6;p6.88">An example of the rule-based type is found in the file in <hi rend="bold italic">'exsys.m'</hi>, here are its contents: </s></p><quote lang="English">%EXSYS A simple example of how an expert system works debt=500;request=30000; income=14000;rating='good';job=6; TCF=1; % Rules (Confidence factors are multiplied) if debt+request>5*income,decision='deny',end if debt+request&lt;=5*income,amount='okay',end if strcmp(rating,'excellent') &amp; strcmp(amount,'okay'), ... decision='approve',TCF=.95*TCF;end if strcmp(rating,'poor') &amp; strcmp(amount,'okay'),decision='deny',end if strcmp(rating,'good') &amp; strcmp(amount,'okay'),risk='okay',TCF=.9*TCF;end if job>=5 &amp; strcmp(risk,'okay'),decision='approve',TCF=.85*TCF;end if job&lt;5 &amp; strcmp(risk,'okay'),decision='cosign',TCF=.75*TCF;end fprintf('Confidence factor: %5.3f\n',TCF); </quote><figure id="BAWE_0146f-fig.002"><head rend="bold italic">Figure 3: Flow chart of 'exsys.m'</head></figure><p n="p7.88"><s n="s1.4;p7.88">The <hi rend="italic">'strcmp'</hi> function [<hi rend="italic">strcmp(string1, string2)</hi>] returns 1 if string1 and string2 are the same, returns 0 otherwise. </s><s n="s2.4;p7.88">As can be seen this is using the confidence factor (TCF) to provide a representation of how sure of the use of the piece of knowledge used at each step will result in the correct output. </s><s n="s3.4;p7.88">As more rules are used the confidence therefore decreases. </s><s n="s4.4;p7.88">The output is shown below: </s></p><quote lang="English"><hi rend="bold">>> exsys amount = okay</hi><hi rend="bold"> risk = okay decision = approve</hi><hi rend="bold"> Confidence factor: 0.765</hi> >> </quote><p n="p8.88"><s n="s1.3;p8.88">This could of course be decided by human intelligence, however, the main goal of expert systems is to provide human-like answers but much more quickly. </s><s n="s2.3;p8.88">This approach to intelligent systems cannot or should not be used for mathematical or logical problems such as the XOR problem. </s><s n="s3.3;p8.88">It can be used for types of problems that some of the other approaches cannot, such as assisting supervisors and managers with situation assessment and long-term planning. </s></p><p n="p9.88"><s n="s1.5;p9.88">The problem with expert systems is that they take an increasingly exponential number of man-years to develop depending on the complexity of the knowledge-base. </s><s n="s2.5;p9.88">This is due to the methods available for knowledge input and the way knowledge is extracted from data and interpreted. </s><s n="s3.5;p9.88">This particular example program also has another flaw in that it can give contradictory results given particular input data. </s><s n="s4.5;p9.88">This is because some outputs can be given more than once if certain conditions arise. </s><s n="s5.5;p9.88">To improve this particular expert system it would therefore be necessary to modify the rules to make sure that it can under no circumstances contradict itself. </s></p><div2><head rend="underlined italic">Can an expert systems be used to solve the XNOR problem? </head><p n="p10.88"><s n="s1.1;p10.88">To do this a new MATLAB program is created: </s></p><quote lang="English">%XNOR EXSYS - Test to see if an expert system can be used for the XNOR problem input1 = 1 input2 = 0 if input1==input2 fprintf('input1 XNOR input2 = 1'); else fprintf('input1 XNOR input2 = 0'); end </quote><figure id="BAWE_0146f-fig.003"><head rend="bold italic">Figure 4: Flow chart of 'xnorexsys.m'</head></figure><p n="p11.88"><s n="s1.1;p11.88">The output is: </s></p><quote lang="English">>> <hi rend="bold"> input1 = 1</hi><hi rend="bold"> input2 = 0</hi><hi rend="bold"> input1 XNOR input2 = 0</hi> >></quote><p n="p12.88"><s n="s1.2;p12.88">Of course the input variables <hi rend="italic">'input1'</hi> and <hi rend="italic">'input2'</hi> can be changed to test other outputs. </s><s n="s2.2;p12.88">However, it is pretty obvious that it will work but that the use of expert systems for the XNOR problem is a bit overkill as it is just a maths problem. </s></p></div2></div1><div1 type="section"><head rend="bold">b) Unsupervised learning</head><p n="p13.88"><s n="s1.3;p13.88">This approach is concerned with the use of neurons to learn to generate a correct output from an input. </s><s n="s2.3;p13.88">The neuron is first trained on lots of data which modifies its weights and then tested on similar data to produce the correct result, that is for the input data to be clustered or grouped together. </s><s n="s3.3;p13.88">The program <hi rend="bold italic">'uhlr.m'</hi> demonstrates this: </s></p><figure id="BAWE_0146f-fig.004"><head rend="bold italic">Figure 5: Neuron arrangement for the Unsupervised Hebbian learning</head></figure><quote lang="English"><hi rend="bold">>> uhlr Vector 1: y = sgn( 0.50) = 1</hi><hi rend="bold"> new weight vector : W=[ 2.00 1.50 -0.50 0.00] Vector 2: y = sgn( 0.50) = 1</hi><hi rend="bold"> new weight vector : W=[ 1.50 2.50 -0.50 1.50] Vector 3: y = sgn( -1.75) = -1</hi><hi rend="bold"> new weight vector : W=[ 2.50 2.50 0.50 2.00]</hi> >></quote><p n="p14.88"><s n="s1.1;p14.88">For an <hi rend="italic">n</hi>×<hi rend="italic">m</hi> matrix of input data, </s></p><p n="p15.88"><s n="s1.1;p15.88"><formula notation="" id="BAWE_0146f-form.002"/> so there is 3 input vectors <hi rend="italic">x</hi>, <formula notation="" id="BAWE_0146f-form.003"/>, <formula notation="" id="BAWE_0146f-form.004"/>, <formula notation="" id="BAWE_0146f-form.005"/>. </s></p><p n="p16.88"><s n="s1.1;p16.88">The transfer function for the neuron looks like this: </s></p><figure id="BAWE_0146f-fig.005"><head rend="bold italic">Figure 6: Thresholding diagram for the neurons</head></figure><p n="p17.88"><s n="s1.2;p17.88">Thresholding is at zero, so the input has to exceed 0 for the neuron to fire. </s><s n="s2.2;p17.88">This is implemented in the program through the use of <hi rend="italic">'sgn'</hi> or <hi rend="italic">'hardlims'</hi> within the <hi rend="italic">'hebbu( )'</hi> function. </s></p><p n="p18.88"><s n="s1.1;p18.88">Define <formula notation="" id="BAWE_0146f-form.006"/> [Equation 1] </s></p><p n="p19.88"><s n="s1.1;p19.88">The change in weights is: </s></p><p n="p20.88"><s n="s1.1;p20.88"><formula notation="" id="BAWE_0146f-form.007"/> [Equation 2] </s></p><p n="p21.88"><s n="s1.1;p21.88">Weights increase if yx is positive </s></p><p n="p22.88"><s n="s1.1;p22.88">Weights decrease otherwise </s></p><p n="p23.88"><s n="s1.1;p23.88">Assumptions: </s></p><p n="p24.88"><s n="s1.1;p24.88">Learning rate, η, is 1 </s></p><p n="p25.88"><s n="s1.1;p25.88">No prior knowledge about the grouping (no target is defined), the neuron needs to learn it so we can test it. </s></p><p n="p26.88"><s n="s1.2;p26.88">The maths showing the production of the outputs for the three vectors has been shown in Assignment 2 and will therefore not be repeated here. </s><s n="s2.2;p26.88">Also see Lecture 5 notes for details on the step-by-step maths. </s></p><p n="p27.88"><s n="s1.1;p27.88">For <hi rend="italic">x</hi><hi rend="sub">1</hi>: <formula notation="" id="BAWE_0146f-form.008"/> For <hi rend="italic">x</hi><hi rend="sub">2</hi>: <formula notation="" id="BAWE_0146f-form.009"/> </s></p><p n="p28.88"><s n="s1.1;p28.88">For <hi rend="italic">x</hi><hi rend="sub">3</hi>: <formula notation="" id="BAWE_0146f-form.010"/> </s></p><table id="BAWE_0146f-tab.002"><head>Neural net output:</head><row><cell/></row></table><p n="p29.88"><s n="s1.1;p29.88">So the results show that the unsupervised learning has found that <hi rend="italic">x</hi><hi rend="sub">1</hi> and <hi rend="italic">x</hi><hi rend="sub">2</hi> belong to one class (they both have an output of 1) and <hi rend="italic">x</hi><hi rend="sub">3</hi> (has an output of -1) to another. </s></p><p n="p30.88"><s n="s1.4;p30.88">To see if the result is justified would require a plot of the input data to be made and the clustering of the data vectors assessed by inspection. </s><s n="s2.4;p30.88">Since it is not possible to visualize 4-D data, I have created the program <hi rend="bold italic">'uhlr4Dto2D.m'</hi> (a copy of this can be found in the appendix). </s><s n="s3.4;p30.88">This plots the 4-D data in two 2-D graphs shown in Figure 7. </s><s n="s4.4;p30.88">The black dashed lines indicate possible separation lines and show that indeed it is possible to classify <hi rend="italic">x</hi><hi rend="sub">1</hi> and <hi rend="italic">x</hi><hi rend="sub">2</hi> into one class and <hi rend="italic">x</hi><hi rend="sub">3</hi> in a separate second class. </s></p><figure id="BAWE_0146f-fig.006"><head rend="bold italic">Figure 7: Two 2-D plots of the 4-D data to show clustering</head></figure><div2><head rend="underlined italic">Can unsupervised learning be used to solve the XNOR problem? </head><p n="p31.88"><s n="s1.2;p31.88">To see how it holds up to the XNOR problem the following new program, <hi rend="bold italic">'xnoruhlr.m'</hi>, was created. </s><s n="s2.2;p31.88">Note also that now the weights do not start with defined values but are given random values so that the neural network can modify them and in theory truly 'learn'. </s></p><quote lang="English">% UHLR trains a neuron with the Unsupervised Hebbian learning rule % Test for XNOR functionality % Define the input dataset P=[ -1 1 -1 1 -1 -1 1 1]; % Initialises the weight vector w [w b]=rands(1,2); % Now execute the Hebbian Learning Rule nepcs=1; % Number of epochs : 1 lr=1; % Learning rate : 1 hebbu(w,P,lr,nepcs) </quote><figure id="BAWE_0146f-fig.007"><head rend="bold italic">Figure 8: Unsupervised learning, typical XNOR classification attempt</head></figure><p n="p32.88"><s n="s1.4;p32.88">When repeatedly executed the output from this produced every possible combination of classification of the outputs possible, therefore, it was unable to classify the outputs for an XNOR function by itself. </s><s n="s2.4;p32.88">Therefore unsupervised learning by the Hebbian learning rule is entirely unsuitable in the context of the XNOR problem. </s><s n="s3.4;p32.88">Of course, use of a bias made no difference to this problem as was seen in Assignment 2 in <hi rend="bold italic">'uhlror.m'</hi> where it tries to use the unsupervised Hebbian learning rule on the OR-function. </s><s n="s4.4;p32.88">In fact after changing the <hi rend="bold italic">'uhlr.m' </hi>file to my <hi rend="bold italic">'xnoruhlr.m' </hi>as shown above (see also Figure 8) it comes out identical to <hi rend="bold italic">'uhlror.m'</hi> because for unsupervised learning, data vectors each of only length 2 could represent the input to any 2-input logic gate so of course it cannot produce the desired output. </s></p><p n="p33.88"><s n="s1.3;p33.88">Another type of unsupervised learning is to use the Kohonen learning rule. </s><s n="s2.3;p33.88">This works by identifying common features in the arrangement of the data. </s><s n="s3.3;p33.88">The XNOR problem was simulated in <hi rend="bold italic">'xnorklr.m' </hi>(see appendix) but when applied to the XNOR problem proved fruitless purely because the input data for the XNOR problem is linearly separable. </s></p></div2></div1><div1 type="section"><head>c) <hi rend="underlined bold">Supervised learning via shlr.m, plr.m, alr.m, dlr.m and mlp.m</hi></head><p n="p34.88"><s n="s1.1;p34.88">Supervised learning uses target vectors to define what the output should be for the given inputs so that the neural network can learn and modify its weight values so that the input is always mapped to the correct output. </s></p><p n="p35.88"><s n="s1.1;p35.88">Target = <hi rend="italic">t</hi> or a vector, <hi rend="italic">ti</hi> </s></p><p n="p36.88"><s n="s1.1;p36.88">Output = <hi rend="italic">y</hi> = <hi rend="italic">t</hi>, or in vectors, <hi rend="italic">yi</hi> = <hi rend="italic">ti</hi> </s></p><p n="p37.88"><s n="s1.1;p37.88">The change in weight <formula notation="" id="BAWE_0146f-form.011"/> </s></p><p n="p38.88"><s n="s1.1;p38.88">Therefore new weight <formula notation="" id="BAWE_0146f-form.012"/> </s></p><figure id="BAWE_0146f-fig.008"><head rend="bold italic">Figure 9: Neurons for supervised Hebbian learning</head></figure><p n="p39.88"><s n="s1.2;p39.88">As for the unsupervised learning, the thresholding is at zero so the input has to exceed 0 for the neuron to fire, hence using combinations of positive and negative inputs allows the neurons to fire or not fire and so create the learning network. </s><s n="s2.2;p39.88">This is implemented in the program through the use of sgn or hardlims within the hebbu( ) function. </s></p><p n="p40.88"><s n="s1.2;p40.88">The program <hi rend="bold italic">'shlr.m' </hi>uses the same input data as for, that is values for <hi rend="italic">x</hi><hi rend="sub">1</hi>, <hi rend="italic">x</hi><hi rend="sub">2</hi>, <hi rend="italic">x</hi><hi rend="sub">3</hi>, η ( = 1) and <hi rend="italic">w</hi>. </s><s n="s2.2;p40.88">But now there is the vector t; </s></p><p n="p41.88"><s n="s1.1;p41.88"><formula notation="" id="BAWE_0146f-form.013"/> </s></p><p n="p42.88"><s n="s1.1;p42.88">For <hi rend="italic">x</hi><hi rend="sub">1</hi>: <formula notation="" id="BAWE_0146f-form.014"/> </s></p><p n="p43.88"><s n="s1.1;p43.88">For <hi rend="italic">x</hi><hi rend="sub">1</hi>: <formula notation="" id="BAWE_0146f-form.015"/> </s></p><p n="p44.88"><s n="s1.1;p44.88">For <hi rend="italic">x</hi><hi rend="sub">2</hi>: <formula notation="" id="BAWE_0146f-form.016"/> </s></p><p n="p45.88"><s n="s1.1;p45.88">For <hi rend="italic">x</hi><hi rend="sub">1</hi>: <formula notation="" id="BAWE_0146f-form.017"/> </s></p><p n="p46.88"><s n="s1.1;p46.88">For <hi rend="italic">x</hi><hi rend="sub">2</hi>: <formula notation="" id="BAWE_0146f-form.018"/> </s></p><p n="p47.88"><s n="s1.1;p47.88">When the program is simulated these theoretical calculations are shown to be correct: </s></p><quote lang="English"><hi rend="bold">Vector 1: target : 1 ; output: y = sgn( 0.50) = 1 Vector 2: target : 1 ; output: y = sgn( -0.50) = -1</hi><hi rend="bold"> wrong output, new weight vector : W=[ 0.50 1.00 -1.00 1.50] Vector 3: target : -1 ; output: y = sgn( -0.25) = -1</hi><hi rend="bold"> Vector 1: target : 1 ; output: y = sgn( 1.50) = 1 Vector 2: target : 1 ; output: y = sgn( 3.00) = 1</hi><hi rend="bold"> Learning successful!</hi> >></quote><p n="p48.88"><s n="s1.2;p48.88">So it only required 1 weight change (<hi rend="italic">w</hi><hi rend="sub">1 </hi>→<hi rend="italic">w</hi><hi rend="sub">2</hi>) as opposed to the unsupervised Hebbian learning rule which changed the weight 3 times. </s><s n="s2.2;p48.88">The supervised Hebbian learning rule can be used when there are targets, but the unsupervised Hebbian learning rule may also be used to locate other patterns in the data that would not be found with a supervised learning since it is trying to match its outputs to the target. </s></p><div2><head rend="underlined italic">Can the supervised Hebbian learning rule (SHLR) do the XNOR problem? </head><p n="p49.88"><s n="s1.1;p49.88">New program <hi rend="bold italic">'xnorshlr.m'</hi> attempts to answer this question. </s></p><quote lang="English">% SHLR trains a neuron with the Supervised Hebbian learning rule %Test for XNOR functionality % Define the input dataset P=[ -1 1 -1 1 -1 -1 1 1]; % Define the targets T=[1 -1 -1 1]; % Initialises the weight vector w [w b]=rands(1,2); % Now execute the Hebbian Learning Rule maxepcs=5; % Number of epochs : 5 lr=1; % Learning rate : 1 hebbs(w,P,T,lr,maxepcs) %Bias not used</quote><figure id="BAWE_0146f-fig.009"><head rend="bold italic">Figure 10: Supervised learning, typical XNOR classification attempt</head></figure><p n="p50.88"><s n="s1.3;p50.88">Figure 10 shows that the program simply cannot learn because the relationship between inputs and outputs is not linearly separable. </s><s n="s2.3;p50.88">On the above plot this can be seen since it is impossible to place a separation line between the two classes ( + ) and ( ° ). </s><s n="s3.3;p50.88">Again, as for <hi rend="bold italic">'xnoruhlr.m'</hi> the use of a bias made no difference. </s></p></div2></div1><div1 type="section"><head>d) <hi rend="underlined bold">Genetic algorithms via optsq.m and gaxor.m</hi></head><p n="p51.88"><s n="s1.6;p51.88">Genetic algorithms can be used to solve problems that would otherwise require an exhaustive approach to solution finding. </s><s n="s2.6;p51.88">This can have substantial benefits in terms of savings in both time and cost. </s><s n="s3.6;p51.88">In <hi rend="italic">'optsq.m'</hi> the function to be optimised using a simple genetic algorithm is defined as <hi rend="italic">x</hi><hi rend="sup">2</hi>. </s><s n="s4.6;p51.88">The four stages are: Reproduction, Mating, Crossover and Mutation. </s><s n="s5.6;p51.88">The population is a random 5 bit binary number. </s><s n="s6.6;p51.88">These represent a population of chromosomes that are to be improved, that is maximised, so ideally they will all be set to the highest value as a result of the genetic algorithm. </s></p><quote lang="English">Cost function encoding as binary successful Fitness statistics Generation Maximum Minimum Mean Std. dev. 0 676 0 299.25 301.891 1 784 16 463.25 361.905 2 784 324 655.25 222.35 3 784 729 742.75 27.5 4 841 676 743.75 69.4808 Genetic algorithm converged. Initial population: 0 0 0 0 0 (0) 1 0 1 0 0 (20) 0 1 0 1 1 (11) 1 1 0 1 0 (26) Generation 1 Reproduction: 1 0 1 0 0 (20) 0 1 0 1 1 (11) 1 1 0 1 0 (26) 1 0 1 0 0 (20) Mating: 1 0 1 0 0 (20) 1 1 0 1 0 (26) 0 1 0 1 1 (11) 1 0 1 0 0 (20) Cross-over: 1 0 | 0 1 0 (18) 1 1 | 1 0 0 (28) 0 | 0 1 0 0 (4) 1 | 1 0 1 1 (27) Generation 2 Reproduction: 1 1 1 0 0 (28) 1 1 1 0 0 (28) 1 0 0 1 0 (18) 1 1 0 1 1 (27) Mating: 1 1 1 0 0 (28) 1 1 0 1 1 (27) 1 0 0 1 0 (18) 1 1 1 0 0 (28) Cross-over: 1 1 | 0 1 1 (27) 1 1 | 1 0 0 (28) 1 0 0 1 | 0 (18) 1 1 1 0 | 0 (28) Generation 3 Reproduction: 1 1 0 1 1 (27) 1 1 1 0 0 (28) 1 1 0 1 1 (27) 1 1 0 1 1 (27) Mating: 1 1 0 1 1 (27) 1 1 0 1 1 (27) 1 1 1 0 0 (28) 1 1 0 1 1 (27) Cross-over: 1 1 0 1 | 1 (27) 1 1 0 1 | 1 (27) 1 | 1 0 1 1 (27) 1 | 1 1 0 0 (28) Generation 4 Reproduction: 1 1 1 0 0 (28) 1 1 0 1 1 (27) 1 1 0 1 1 (27) 1 1 0 1 1 (27) Mating: 1 1 0 1 1 (27) 1 1 0 1 1 (27) 1 1 1 0 0 (28) 1 1 0 1 1 (27) Cross-over: 1 1 0 1 | 1 (27) 1 1 0 1 | 1 (27)<hi rend="bold"> 1 1 1 0 | 1 (29)</hi> 1 1 0 1 | 0 (26)</quote><p n="p52.88"><s n="s1.2;p52.88">As the program runs it shows the result at each stage of reproduction, mating and crossover. </s><s n="s2.2;p52.88">Mutation is rare and shows up as a difference in the population before and after reproduction. </s></p><p n="p53.88"><s n="s1.1;p53.88">The first population is 1, 20, 11 and 26. </s></p><div2><head rend="bold">Reproduction:</head><p n="p54.88"><s n="s1.4;p54.88">The fitness of each chromosome is evaluated using the x^2 function. </s><s n="s2.4;p54.88">A chromosomes ability to reproduce is proportional to its fitness. </s><s n="s3.4;p54.88">20 has the highest fitness value since it reproduces twice, whereas 11 and 26 reproduce only once and 1 does not reproduce at all. </s><s n="s4.4;p54.88">Therefore, in reproduction selection principles are applied. </s></p></div2><div2><head rend="bold">Mating:</head><p n="p55.88"><s n="s1.2;p55.88">Pairs of chromosomes are mated by randomly choosing two for crossover. </s><s n="s2.2;p55.88">These were chosen to be (20,26) and (11,20). </s></p></div2><div2><head rend="bold">Crossover:</head><p n="p56.88"><s n="s1.4;p56.88">For each of the pairs the crossover site is determined randomly. </s><s n="s2.4;p56.88">In this example this means which bit will be exchanged. </s><s n="s3.4;p56.88">For 20 and 26 this was the 4 <hi rend="sup">th</hi> most significant bit and so 8 was subtracted from 26 and 8 added to 20 resulting in 18 and 28. </s><s n="s4.4;p56.88">Similarly, for 11 and 20 except their most significant bits were swapped so 16 was subtracted from 20 giving 4 and 16 added to 11 resulting in 27. </s></p><p n="p57.88"><s n="s1.3;p57.88">This process repeats with the new generation of 18, 28, 4 and 27. </s><s n="s2.3;p57.88">This time 28 reproduces twice and 27 and 18 reproduce once each. </s><s n="s3.3;p57.88">And so on and so on... </s></p><p n="p58.88"><s n="s1.3;p58.88">In the end the output does not achieve the maximum possible output of 31 but achieves only 29. </s><s n="s2.3;p58.88">This is because the program is a bit limited with only 4 chromosomes and 5 bits of coding. </s><s n="s3.3;p58.88">Increasing these parameters in the program produced better results. </s></p><p n="p59.88"><s n="s1.1;p59.88"><hi rend="bold">'gaxor.m'</hi> simulates the XOR problem. </s></p><p n="p60.88"><s n="s1.2;p60.88">This aims to find an MLP architecture for solving the XOR problem. </s><s n="s2.2;p60.88">The arrangement of neurons is shown below and it is used to train up to 10 generations of 16 chromosomes. </s></p><figure id="BAWE_0146f-fig.010"><head rend="bold italic"> Figure 11: MLP architecture</head></figure><table id="BAWE_0146f-tab.003"><head rend="bold italic">Figure 12: Constraint matrix</head><row><cell/></row></table><p n="p61.88"><s n="s1.7;p61.88">This is the constraint matrix showing what is linked to what and where a bias can be added. </s><s n="s2.7;p61.88">For example there is a 1 in the top left of the table so there is a link going from unit 1 to unit 3. </s><s n="s3.7;p61.88">Not all possible connections are allowed as it is a forward-feeding network. </s><s n="s4.7;p61.88">The training parameters such as learning rate and number of 'epochs' are definable. </s><s n="s5.7;p61.88">The 'Fitnn' function trains the MLP using the gradient descent algorithm. </s><s n="s6.7;p61.88">It needs smallest sum-squared error to get outputs closest to targets. </s><s n="s7.7;p61.88">The fitness function is f(x) = 4*NE - E, where X = number associated with architecture, NE = number of examples and E = sum squared error. </s></p><quote lang="English">>> <hi rend="bold"> Cost function encoding as binary successful.</hi> <hi rend="bold"> Fitness statistics Generation Maximum Minimum Mean Std. dev.</hi> <hi rend="bold"> 0 15.9873 11.9847 12.4127 1.06036</hi> <hi rend="bold"> 1 15.9872 11.9362 12.5791 1.3802</hi> <hi rend="bold"> 2 15.9576 11.9787 12.4239 1.06675</hi> <hi rend="bold"> 3 15.987 11.9798 12.5913 1.10035</hi> <hi rend="bold"> 4 13.9735 11.9772 12.478 0.763031</hi> <hi rend="bold"> 5 15.9705 11.9729 12.9844 1.10497</hi> <hi rend="bold"> 6 15.9716 11.9457 12.8949 1.13119</hi> <hi rend="bold"> 7 15.9729 11.9673 12.7641 1.38685</hi> <hi rend="bold"> Genetic algorithm converged.</hi> <hi rend="bold"> Connectivity constraint matrix: from : 1 2 3 4 5 bias</hi><hi rend="bold"> 3 1 1 0 0 0 1 4 1 1 0 0 0 1</hi><hi rend="bold"> 5 0 1 1 1 0 0 Fitness:</hi><hi rend="bold"> 15.9840 Output:</hi><hi rend="bold"> -0.9473 0.9299 0.9473 -0.9255</hi> >></quote><p n="p62.88"><s n="s1.6;p62.88">The fitness value is fairly high indicating that a solution to the XOR problem has been implemented to a very high degree. </s><s n="s2.6;p62.88">Indeed inspection of the output shows (to 2 d.p.) -0.95, 0.93, 0.95, -0.93. </s><s n="s3.6;p62.88">This is approximately -1, 1, 1, -1 which is the target vector. </s><s n="s4.6;p62.88">It is worth noting that upon repeat executions of the program different solutions are found that still work. </s><s n="s5.6;p62.88">That is, the connectivity matrix can be different and also the biases to produce the same result. </s><s n="s6.6;p62.88">To apply this to the XNOR problem would be trivial and obviously work as it does for XOR and so the discussion of this result will be raised in the Discussions &amp; Conclusions section of this report to allow a more concise comparison with the other possible approaches. </s></p></div2></div1><div1 type="section"><head>e)<hi rend="bold"> Fuzzy logic via bfso.m, finf.m and flc.m</hi></head><p n="p63.88"><s n="s1.2;p63.88">Basic Fuzzy Set Operators are demonstrated in <hi rend="bold italic">'bfso.m'</hi> uses the fuzzy subsets F1={A/0.5 B/0.7 C/0.3 D/0.1} and F2={A/0.1 B/0.9 C/0.5 D/0.7 E/0.9} to demonstrate some Basic Fuzzy Subset Operators. </s><s n="s2.2;p63.88">These are: </s></p><list type="bulleted"><item>Intersection: min operator, covers possibility of both occurring, i.e. AND-function</item><item>Union: max operator, covers possibility of either occurring, i.e. OR-function</item><item>Complement: f(x)=1-x, i.e. NOT-function</item></list><p n="p64.88"><s n="s1.6;p64.88">Note how possibility is different to probability. </s><s n="s2.6;p64.88">This is why the fuzzy subsets add up to greater than 1, if they were probabilities then they would have to add to 1. </s><s n="s3.6;p64.88">This is essentially how fuzzy logic works by allowing overlap of the fuzzy subsets. </s><s n="s4.6;p64.88">The product of fuzzy subsets are added and pairs of elements from F1 and F2 form new elements. </s><s n="s5.6;p64.88">The minimum of the memberships of the two elements is the new elements membership. </s><s n="s6.6;p64.88">For example (D, E)/0.1 comes from D/0.1 in F1 and E/0.9 in F2 because D has the lower possibility of 0.1 compared to E's possibility of 0.9. </s></p><p n="p65.88"><s n="s1.2;p65.88">Fuzzy Associative Memories are demonstrated in <hi rend="bold italic">'finf.m'</hi> which defines normalised (across the row of numbers) vectors. </s><s n="s2.2;p65.88">To implement the AND and OR functions the min and max operators are used to build the Fuzzy Associative Memory (FAM). </s></p><table id="BAWE_0146f-tab.004"><head><hi rend="bold italic"> Figure 13: y1 AND y2</hi> From 1 <hi rend="sup">st</hi> FAM: output 1 when both i/p are close to max. </head><row><cell/></row></table><table id="BAWE_0146f-tab.005"><head><hi rend="bold italic"> Figure 14: y1 OR y2</hi> From 2 <hi rend="sup">nd</hi> FAM: output 1 when either i/p close to max.</head><row><cell/></row></table><quote lang="English">>> <hi rend="bold"> Normalised vectors: 1.0000 0.2500 0</hi><hi rend="bold"> 1.0000 0.6667 0 1.0000 0.6667 0</hi><hi rend="bold"> 0.2500 1.0000 0 y1 AND y2:</hi><hi rend="bold"> L M H L 0 0 0</hi><hi rend="bold"> M 0 0.2500 0.2500 H 0 0.6667 1.0000</hi><hi rend="bold"> y1 OR y2: L M H</hi><hi rend="bold"> L 0 0.6667 1.0000 M 0.2500 0.6667 1.0000</hi><hi rend="bold"> H 1.0000 1.0000 1.0000</hi> >></quote><p n="p66.88"><s n="s1.1;p66.88">It would not be possible to use this type of fuzzy logic for the XNOR problem since it only uses the min and max operators which are of no help. </s></p><p n="p67.88"><s n="s1.4;p67.88"><hi rend="bold italic">'Flc.m' </hi>demonstrates Fuzzy Logic Control. </s><s n="s2.4;p67.88">This shows a good demonstration of how the fuzzy subsets can be shown graphically as in Figure 15. </s><s n="s3.4;p67.88">The output is an automatic grouping of the input data, the 4-D data seen throughout much of the examples. </s><s n="s4.4;p67.88">The membership functions are chosen by the program in such a way that given the input data it will be able to use a combination of the fuzzy subsets and if-then statements to produce classification outputs. </s></p><quote lang="English">>> <hi rend="bold"> FLC Fuzzy Logic Control example seen in class Original dataset:</hi><hi rend="bold"> 1.0000 -0.5000 -1.0000 //y1 (dashed line, dotted line, dash-dot line) 1.5000 1.0000 0 //y2 (dashed line, dotted line, dash-dot line)</hi><hi rend="bold"> 0.5000 0 -1.0000 0 1.5000 -0.5000</hi><hi rend="bold"> Outputs: 1 1 -1</hi><hi rend="bold"> Rules used: 1. If (y1 is High) and (y2 is High) then (output is High) (1)</hi><hi rend="bold"> 2. If (y1 is Low) and (y2 is Low) then (output is Low) (1)</hi> <hi rend="bold"> 3. If (y1 is Low) and (y2 is High) then (output is High) (1)</hi> >></quote><p n="p68.88"><s n="s1.2;p68.88">So the system classifies the input vectors successfully when compared to for example Figure 7 which shows the likely clustering of this set of input data. </s><s n="s2.2;p68.88">This is forward chaining or Modus Ponens where the consequences are discovered from the causes (the output from the input), otherwise it will be backward chaining or Modus Tollens (search for causes to give outputs). </s></p><figure id="BAWE_0146f-fig.011"><head rend="bold italic">Figure 15: Fuzzy subset membership functions and input data</head></figure><p n="p69.88"><s n="s1.2;p69.88">In Figure 15 the vertical lines in the plot of y1 represent the first co-ordinate of each of the three vectors. </s><s n="s2.2;p69.88">Looking at the first vector: <formula notation="" id="BAWE_0146f-form.019"/> </s></p><p n="p70.88"><s n="s1.2;p70.88">We can see the first two co-ordinates shown as the long dashed lines in the plots of y1 and y2 and that both of these belong to the fuzzy subsets 'High' (shown by their position within the membership functions. </s><s n="s2.2;p70.88">Hence, looking at the three rules used we can see that it is correct for the first output to be a 1 since only rule 1 has been activated. </s></p><p n="p71.88"><s n="s1.1;p71.88">For the second vector: <formula notation="" id="BAWE_0146f-form.020"/> </s></p><p n="p72.88"><s n="s1.3;p72.88">The first two co-ordinates have been shown as the dotted (or short-dashed) line in the plots for y1 and y2. - </s><s n="s2.3;p72.88">0.5 belongs to the 'Low' fuzzy subset of y1 and 1.0 belongs to the 'High' (and possibly 'Medium'???) fuzzy subset of y2. </s><s n="s3.3;p72.88">Therefore rule 3 has been activated and the output is correctly shown as high, or a 1. </s></p><p n="p73.88"><s n="s1.1;p73.88">For the third vector: <formula notation="" id="BAWE_0146f-form.021"/> </s></p><p n="p74.88"><s n="s1.2;p74.88">First two co-ordinates shown as dot-dash lines in plots of y1 and y2. - </s><s n="s2.2;p74.88">1.0 belongs to the 'Low' fuzzy subset of y1 and 0.0 belongs to the 'Low' fuzzy subset of y2 </s></p><p n="p75.88"><s n="s1.1;p75.88">Unfortunately this type of Fuzzy Logic is also unsuitable for solving the XNOR problem since the rules used are to determine classification which just wont work for XNOR input data. </s></p></div1><div1 type="section"><head>f) <hi rend="underlined bold">Neuro-Fuzzy via nfa.m, nfo.m and nfn.m</hi></head><p n="p76.88"><s n="s1.2;p76.88">The Fuzzy Propagation Algorithm (FPA) uses fuzzified data as input to a neural network. </s><s n="s2.2;p76.88">The 3 applications to be studied in this section are Neuro-Fuzzy AND, NOT and OR. </s></p><figure id="BAWE_0146f-fig.012"><head rend="bold italic">Figure 16: FPA system diagrame/f</head></figure><figure id="BAWE_0146f-fig.013"><head rend="bold italic"> Figure 17: FPA neuron arrangement</head></figure><p n="p77.88"><s n="s1.7;p77.88"><hi rend="bold italic">'nfn.m' </hi>implements the Neuro-Fuzzy NOT function. </s><s n="s2.7;p77.88">It defines the signal to complement, <hi rend="italic">x</hi> = 0.25, and defines the signal and weight vectors <hi rend="italic">p</hi> = [<hi rend="italic">x</hi> 1] and <hi rend="italic">w</hi> = [-1 1] respectively. </s><s n="s3.7;p77.88">It then executes <hi rend="bold italic">'fpa.m'</hi> which implements the FPA for any signal and weight. </s><s n="s4.7;p77.88">It works by first sorting the inputs using the Matlab function 'sort'. </s><s n="s5.7;p77.88">The differences are computed by subtracting each value in the signal vector from the following one. </s><s n="s6.7;p77.88">The weights are summed cumulatively by adding them from 1 to n, starting with the last one. </s><s n="s7.7;p77.88">The weights are thresholded before the output is created. </s></p><quote lang="English">>> <hi rend="bold"> Reordered signals and weights: Signals : 0.25 1.00</hi> <hi rend="bold"> Weights : -1.00 1.00</hi> <hi rend="bold"> Difference between neighbouring pairs: 0.25 0.75</hi> <hi rend="bold"> Combined weights: 0.00 1.00</hi> <hi rend="bold"> Thresholded weights: 0 1</hi> <hi rend="bold"> Output: 0.75</hi> >> </quote><figure id="BAWE_0146f-fig.014"><head rend="bold italic">Figure 18: Neuron and weights for Neuro-Fuzzy NOT</head></figure><p n="p78.88"><s n="s1.3;p78.88">The signals were already in ascending order, [ 0.25 1 ], so they were not re-ordered. </s><s n="s2.3;p78.88">The difference calculation only affects the second value, 1.0 - 0.25 = 0.75 = complement of input. </s><s n="s3.3;p78.88">The new weights are found (-1 + 1 = 0 and 1 stays as it is) and thresholding does nothing here since they are already 0 and 1. </s></p><p n="p79.88"><s n="s1.2;p79.88">Output = weighted sum of differences = (0*0.25) + (1*0.75) = 0.75 = the complement of the input. </s><s n="s2.2;p79.88">Hence it is performing the NOT function correctly. </s></p><p n="p80.88"><s n="s1.1;p80.88">Neuro-Fuzzy OR is implemented in <hi rend="bold">'nfo.m'</hi> which is similar to <hi rend="bold italic">'nfn.m' </hi>except the signal to be 'OR-ed' is of course different, <hi rend="italic">p</hi> = [1 .25 0], and the weight vectors are simply a row of 1's. </s></p><figure id="BAWE_0146f-fig.015"><head rend="bold italic">Figure 19: Neuron and weights for Neuro-Fuzzy OR</head></figure><quote lang="English">>> <hi rend="bold"> Reordered signals and weights: Signals : 0.00 0.25 1.00</hi> <hi rend="bold"> Weights : 1.00 1.00 1.00</hi> <hi rend="bold"> Difference between neighbouring pairs: 0.00 0.25 0.75</hi> <hi rend="bold"> Combined weights: 3.00 2.00 1.00</hi> <hi rend="bold"> Thresholded weights: 1 1 1</hi> <hi rend="bold"> Output: 1.00</hi> >></quote><p n="p81.88"><s n="s1.4;p81.88">This time the inputs are re-ordered from [ 1 0.25 0 ] to [ 0 0.25 1 ]. </s><s n="s2.4;p81.88">Again, the difference calculation does not affect the first value, the other differences are computed as 0.25 - 0 = 0.25 and 1 - 0.25 = 0.75. </s><s n="s3.4;p81.88">So we have [ 0 0.25 0.75 ] as the differences. </s><s n="s4.4;p81.88">The weights are added cumulatively as before </s></p><p n="p82.88"><s n="s1.2;p82.88">( 1 + 1 + 1 = 3, 1 + 1 = 2 and 1 on the end stays the same) and thresholded to [ 1 1 1 ] as the final weight vectors. </s><s n="s2.2;p82.88">The output is again the sum of the differences multiplied by their respective weights: </s></p><p n="p83.88"><s n="s1.2;p83.88">Output = (1*0) + (1*0.25) + (1*0.75) = 1 = last value in re-ordered inputs = largest/maximum value of input. </s><s n="s2.2;p83.88">Hence it is performing the OR function correctly. </s></p><p n="p84.88"><s n="s1.1;p84.88"><hi rend="bold">'nfa.m'</hi> implements the Neuro-Fuzzy AND function and again is similar to the OR program with the signal to be 'AND-ed' the same, <hi rend="italic">p</hi> = [1 .25 0], but now the weight vectors are all 1/<hi rend="italic">n</hi> where <hi rend="italic">n</hi> is the number inputs in this case 3 so the vector of weights is [ 0.333 0.333 0.333 ]. </s></p><figure id="BAWE_0146f-fig.016"><head rend="bold italic">Figure 20: Neuron and weights for Neuro-Fuzzy AND</head></figure><quote lang="English">>> <hi rend="bold"> Reordered signals and weights: Signals : 0.00 0.25 1.00</hi> <hi rend="bold"> Weights : 0.33 0.33 0.33</hi> <hi rend="bold"> Difference between neighbouring pairs: 0.00 0.25 0.75</hi> <hi rend="bold"> Combined weights: 1.00 0.67 0.33</hi> <hi rend="bold"> Thresholded weights: 1 0 0</hi> <hi rend="bold"> Output: 0.00</hi> >></quote><p n="p85.88"><s n="s1.7;p85.88">This inputs are re-ordered exactly the same as in the OR program. </s><s n="s2.7;p85.88">Hence the differences are also the same. </s><s n="s3.7;p85.88">The weights are added cumulatively ( 0.333 + 0.333 + 0.333 = 1, 0.333 + 0.333 = 0.667 and 0.333 on the end stays the same as in the original weight vector). </s><s n="s4.7;p85.88">These are thresholded (set to 1 if greater than or equal to 1, otherwise set to 0) to [ 1 0 0 ] as the final weight vectors. </s><s n="s5.7;p85.88">The output is again the sum of the differences multiplied by their respective weights: Output = (1*0) + (0*0.25) + (0*0.75) = 0. </s><s n="s6.7;p85.88">So the output is the first difference = the first entry in the inputs = the smallest input value. </s><s n="s7.7;p85.88">Hence it is performing the AND function correctly, since at least one of the inputs is 'low'. </s></p></div1><div1 type="section"><head rend="underlined bold">Discussion and conclusions</head><p n="p86.88"><s n="s1.7;p86.88">Maybe look at assignment in context that you are making a proposal to your boss to help him/her make a decision about which of these alternatives to employ to solve a particular problem; for example in the context of your project. </s><s n="s2.7;p86.88">So we wish to identify the key factors that are important in a solution based on each of these alternatives in the context of the problem. </s><s n="s3.7;p86.88">For example is the problem likely to benefit from learning. </s><s n="s4.7;p86.88">If so which type? </s><s n="s5.7;p86.88">What are the input/output requirements? </s><s n="s6.7;p86.88">Etc. </s><s n="s7.7;p86.88">Illustrate, where possible, this with an attempt to implement the selected ISE approach using relevant data; simulated or otherwise. </s></p><p n="p87.88"><s n="s1.8;p87.88">There are advantages and disadvantages of using each approach to Intelligent Systems depending on the intended application. </s><s n="s2.8;p87.88">It turns out that Expert Systems are best for representing expert knowledge and knowledge representation. </s><s n="s3.8;p87.88">Neural networks such as the supervised and unsupervised examples studied are best for learning, nonlinear and fault tolerant applications. </s><s n="s4.8;p87.88">This is because the 'knowledge' that the neural network retains as it learns is spread across the whole network so that an unusual input to one part of it should not affect its ability to provide the correct output. </s><s n="s5.8;p87.88">Genetic algorithms are also good for nonlinear problems and have fault tolerance like neural networks. </s><s n="s6.8;p87.88">They are best of all though, for fast optimisation. </s><s n="s7.8;p87.88">Fuzzy logic is good for anything apart from learning and optimisation. </s><s n="s8.8;p87.88">The fact that it was not possible to implement an XNOR solution using fuzzy logic is probably due to not knowing what modifications would be required to the existing programs rather than it not been possible. </s></p><p n="p88.88"><s n="s1.4;p88.88">For the XNOR problem it was found that only expert systems could solve it. </s><s n="s2.4;p88.88">The neural nets suffered because it was a non-linearly separable problem. </s><s n="s3.4;p88.88">The genetic algorithm could easily be modified using the MLP approach and would totally work for the XNOR problem, this ties in with one of the things they should be good at, that is, solving non-linear problems. </s><s n="s4.4;p88.88">The fuzzy approach to the XNOR problem suffered mostly due to a lack of MATLAB programming ability and no clue how to change the programs to simulate an XNOR problem. </s></p></div1></body><back><div1 type="bibliography"><head rend="underlined bold">References/Bibliography</head><p>The lecture notes.</p><p>Previous assignments.</p><p>MATLAB help files.</p></div1><div1 type="appendix"><head><hi rend="underlined bold">APPENDIX</hi> - My Matlab files</head><p/></div1></back></text></TEI.2>
