<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE TEI.2 SYSTEM "tei_bawe.dtd"><TEI.2 id="_0403a" n="version 1.0"><teiHeader><fileDesc><titleStmt><title>Coding Theory and Cryptography</title></titleStmt><extent/><publicationStmt><distributor>British Academic Written English (BAWE) corpus</distributor><availability><p>The British Academic Written English (BAWE) corpus was developed at the Universities of Warwick, Reading and Oxford Brookes, under the directorship of Hilary Nesi and Sheena Gardner (formerly of the Centre for Applied Linguistics [previously called CELTE], Warwick), Paul Thompson (Department of Applied Linguistics, Reading) and Paul Wickens (Westminster Institute of Education, Oxford Brookes), with funding from the ESRC. Subject to the rights of the these institutions in the BAWE corpus, and pursuant to the ESRC agreement, the BAWE corpus is available to researchers for research purposes PROVIDED THAT the following conditions are met:</p><p>1. The corpus files are not distributed in either their original form or in modified form.</p><p>2. The texts are used for research purposes only; they should not be reproduced in teaching materials.</p><p>3. The texts are not reproduced in full for a wider audience/readership, although researchers are free to quote short passages of text (up to 200 running words from any given text).</p><p>4. The BAWE corpus developers (contact: Hilary Nesi) are informed of all projects, dissertations, theses, presentations or publications arising from analysis of the corpus.</p><p>5. Researchers acknowledge their use of the corpus using the following form of words: "The data in this study come from the British Academic Written English (BAWE) corpus, which was developed at the Universities of Warwick, Reading and Oxford Brookes under the directorship of Hilary Nesi and Sheena Gardner (formerly of the Centre for Applied Linguistics [previously called CELTE], Warwick), Paul Thompson (Department of Applied Linguistics, Reading) and Paul Wickens (Westminster Institute of Education, Oxford Brookes), with funding from the ESRC (RES-000-23-0800)."</p></availability></publicationStmt><notesStmt><note resp="British Academic Written English (BAWE) corpus project">Language used in quote: <foreign id="English">English</foreign></note><note resp="British Academic Written English (BAWE) corpus project">Evaluated as candidate compound assignment. Assigned to Assigned to S1a: collection of essays/discussions: compound.</note><note resp="British Academic Written English (BAWE) corpus project">deleted: page numbers</note></notesStmt><sourceDesc><p n="level">4</p><p n="date">unknown</p><p n="module title">unknown</p><p n="module code">unknown</p><p n="genre family">Explanation + Explanation</p><p n="discipline">Mathematics</p><p n="disciplinary group">PS</p><p n="grade">unknown</p><p n="number of authors">unknown</p><p n="number of words">5137</p><p n="number of s-units">323</p><p n="number of p">179</p><p n="number of tables">14</p><p n="number of figures">8</p><p n="number of block quotes">1</p><p n="number of formulae">15</p><p n="number of lists">1</p><p n="number of paragraphs formatted like lists">11</p><p n="abstract present">no abstract</p><p n="average words per s-unit">15.9</p><p n="average s-units per p">1.8</p><p n="macrotype of assignment">compound assignment consisting of 2 parts (see notesStmt for details)</p></sourceDesc></fileDesc><encodingDesc><p>TEI P4 (documented in: BAWE.documentation.pdf)</p></encodingDesc><profileDesc><particDesc><person><p n="gender">m</p><p n="year of birth">1986</p><p n="first language">English</p><p n="education">UKA</p><p n="course">Mathematics and Business Studies</p><p n="student ID">0403</p></person></particDesc></profileDesc></teiHeader><text><front><titlePage><docTitle><titlePart rend="underlined bold">Coding Theory and Cryptography</titlePart></docTitle></titlePage><div1 type="front text"><head rend="underlined bold italic">Introduction</head><p>Coding theory and Cryptography are opposite sides of a coin. Coding theory is making information <hi rend="italic">easy</hi> to read and Cryptography is completely the opposite; making information <hi rend="italic">hard</hi> to read.</p><p>Examples of Coding theory and Cryptography are in abundance in the modern world. Whenever I press a key on this keyboard or click 'Save', some part of Coding theory is used to check that the data has been transmitted and stored correctly. If I were to click 'Protect' on the menu bar, Cryptography would come into play to make sure that only I could read my work.</p></div1></front><body><div1 type="text"><head rend="underlined bold italic">Coding Theory</head><div2><head rend="bold italic">Introduction</head><p n="p1.179"><s n="s1.5;p1.179">In the 21 <hi rend="sup">st</hi> century we expect electronic equipment to <hi rend="italic">work</hi>. </s><s n="s2.5;p1.179">When we load documents, view pictures, play music or print letters, we expect everything to be correct. </s><s n="s3.5;p1.179">We don't want to see random characters in the middle of a 20 page report or hear clicks in our favourite song. </s><s n="s4.5;p1.179">Electronic equipment however <hi rend="underlined">is</hi> prone to errors - frequently things go wrong in the binary code that is transmitted inside a computer, which causes a 1 to be read as a 0 or vice-versa. </s><s n="s5.5;p1.179">Computers are not the only problem; sending information across vast distances such as space or a phone line faces the same difficulty. </s></p></div2><div2><head rend="bold italic">Motivation</head><p n="p2.179"><s n="s1.1;p2.179">Imagine the following situation where a sender wants to transmit 1 byte (8 bits) of data to a receiver via the 'airwaves': </s></p><figure id="BAWE_0403a-pic.001"/><p n="p3.179"><s n="s1.2;p3.179">Assuming there is a possibility that some part of the message could have been changed, how does the receiver know that '01111010' is what the sender transmitted? </s><s n="s2.2;p3.179">This is the motivation; solving these problems is what Coding theory is all about. </s></p></div2><div2><head rend="bold italic">A brief history of Coding Theory</head><p n="p4.179"><s n="s1.5;p4.179">It can be argued that the 'father' of Coding Theory is Richard Hamming. </s><s n="s2.5;p4.179">Hamming worked at the Bell laboratories on mechanical relay machines. </s><s n="s3.5;p4.179">The computers he worked on could <hi rend="italic">detect</hi> an error but could not fix it and would abandon the current job. </s><s n="s4.5;p4.179">As Hamming only had access to the computer at limited times this was very frustrating. </s><s n="s5.5;p4.179">Hamming developed a <hi rend="italic">single error correcting code</hi> which enabled the computer to detect and correct one error in a code. </s></p><p n="p5.179"><s n="s1.1;p5.179">Other mathematicians and programmers who shared Hamming's problems developed new and more complicated codes that would allow <hi rend="italic">multiple errors</hi> to be corrected. </s></p><p n="p6.179"><s n="s1.2;p6.179">There are now many different types of codes and each one is suited to a particular job. </s><s n="s2.2;p6.179">Compact Discs have interleaved codes, which can correct up to 4,000 consecutive errors. </s></p><p n="p7.179"><s n="s1.2;p7.179">A complicated code would preserve data very well, but it would result in lower efficiency as extra information would have to be stored that had no relevance to the original 'message'. </s><s n="s2.2;p7.179">However, if the risk of data loss is small then a simple code would do. </s></p></div2><div2><head rend="bold italic">Codes, codewords and code alphabets</head><p n="p8.179"><s rend="underlined" n="s1.1;p8.179">Definition: </s></p><p n="p9.179"><s n="s1.1;p9.179">Let A = {a <hi rend="sub">1</hi>, a <hi rend="sub">2</hi>, ..., a <hi rend="sub">r</hi>} be a finite set, which we call a <hi rend="italic">code alphabet</hi>. </s></p><p n="p10.179"><s n="s1.1;p10.179">Then an r-ary code (bin-ary, tern-ary etc) over A is a subset C of the set A* of all words over A. </s></p><p n="p11.179"><s n="s1.1;p11.179">The elements of C are called codewords. </s></p><p n="p12.179"><s n="s1.1;p12.179">The number r is called the radix of the code. </s></p><p n="p13.179"><s n="s1.4;p13.179">Binary is made up of 0's and 1's. </s><s n="s2.4;p13.179">So for binary, the code alphabet is {0, 1}. </s><s n="s3.4;p13.179">The radix of binary is 2. </s><s n="s4.4;p13.179">For most of this essay I will be working with the part of Coding theory that is associated with binary. </s></p></div2><div2><head rend="bold italic">Modelling errors</head><p n="p14.179"><s n="s1.1;p14.179">There are many ways of modelling errors within a message; one model is white noise. </s></p><p n="p15.179"><s n="s1.4;p15.179">Most people know white noise as the sound their TV makes at the end of a video. </s><s n="s2.4;p15.179">The sound coming out of the speaker is completely random; it sounds chaotic. </s><s n="s3.4;p15.179">White noise within computer circuits, telephone lines and radio transmissions is very similar to this. </s><s n="s4.4;p15.179">Random changes happen to the information that is being sent. </s></p><p n="p16.179"><s n="s1.1;p16.179">There are two assumptions for the white noise model: </s></p><p rend="ordered" n="p17.179"><s n="s1.2;p17.179">The probability of an error occurring at position x is exactly the same as an error occurring at any other position. </s><s n="s2.2;p17.179">This probability is p. </s></p><p rend="ordered" n="p18.179"><s n="s1.1;p18.179">Errors in different positions are independent of each other. </s></p><p n="p19.179"><s n="s1.1;p19.179">In real life, equipment tends to malfunction at a particular time, thus contradicting the idea that errors are independent of each other, however it is impossible to predict when a piece of equipment will malfunction, therefore the white noise model is a reasonable assumption. </s></p><p n="p20.179"><s n="s1.1;p20.179">The probability of an error occurring at position x is p. </s></p><p n="p21.179"><s n="s1.1;p21.179">The probability of no errors in n positions is: (1-p)<hi rend="sup">n</hi> </s></p><p n="p22.179"><s n="s1.1;p22.179">The probability of one error is: np(1-p)<hi rend="sup">n-1</hi> </s></p><p n="p23.179"><s n="s1.1;p23.179">The probability of k errors is: <formula notation="" id="BAWE_0403a-form.001"/> </s></p><p n="p24.179"><s n="s1.1;p24.179">The probability of i events happening is modelled by the Binomial Distribution: </s></p><p n="p25.179"><s n="s1.1;p25.179"><formula notation="" id="BAWE_0403a-form.002"/>, <formula notation="" id="BAWE_0403a-form.003"/>. </s></p></div2><div2><head rend="bold italic">Detecting errors - The parity check</head><p n="p26.179"><s n="s1.2;p26.179">If the electronic equipment can repeat a transmission, then just <hi rend="italic">detecting</hi> an error would be enough. </s><s n="s2.2;p26.179">We could then simply repeat the transmission until we get it right. </s></p><p n="p27.179"><s n="s1.1;p27.179">A simple error checking code is a parity check. </s></p><p n="p28.179"><s n="s1.2;p28.179">Here, we count the number of 1's in a codeword, and append a 0 if it is an even number and a 1 if it is odd. </s><s n="s2.2;p28.179">For example: </s></p><table id="BAWE_0403a-tab.001"><row><cell/></row></table><p n="p29.179"><s n="s1.3;p29.179">This codeword can then be sent and when it reaches the other end, a parity check can be run. </s><s n="s2.3;p29.179">In the case of '1100', we would send '11000'. </s><s n="s3.3;p29.179">If an error occurred in transmission and '11010' was received, the parity would not match so we could ask for the data to be sent again. </s></p><p n="p30.179"><s n="s1.1;p30.179">However with this method no two errors (or in fact, any even number of errors) would be detected. </s></p></div2><div2><head rend="bold italic">Detecting errors - repetition codes</head><p n="p31.179"><s n="s1.3;p31.179">Another relatively simple code is a repetition code. </s><s n="s2.3;p31.179">This involves sending each codeword <hi rend="italic">more than once.</hi> </s><s n="s3.3;p31.179">Sending n copies of our codeword allows detection of n-1 errors. </s></p><p n="p32.179"><s n="s1.2;p32.179">For a block of binary with n digits, there are 2 <hi rend="sup">n</hi> codewords available. </s><s n="s2.2;p32.179">For example, a 3-block binary code has 8 codewords: </s></p><quote lang="English">000, 001, 010, 011, 100, 101, 110, 111</quote><p n="p33.179"><s n="s1.1;p33.179">If our code word was '0110' then we would send '01100110'. </s></p><p n="p34.179"><s n="s1.2;p34.179">If the symmetry of our new codeword is destroyed in transmission then we would know that an error had occurred. </s><s n="s2.2;p34.179">In the case above, if '01101110' was received, we would know an error occurred. </s></p><p n="p35.179"><s n="s1.1;p35.179">Again, like the Parity check, this error detecting code has no way of detecting even numbers of errors if two errors occur that preserve symmetry: </s></p><figure id="BAWE_0403a-pic.002"/><p n="p36.179"><s n="s1.3;p36.179">This method of error checking clearly causes a decrease in efficiency. </s><s n="s2.3;p36.179">Digits are sent that contribute nothing to the meaning of the message. </s><s n="s3.3;p36.179">Now that we have two different codes, it would be useful to have some way of comparing them. </s></p></div2><div2><head rend="bold italic">Redundancy</head><p n="p37.179"><s n="s1.4;p37.179">In the repetition codes discussed above, a certain number of digits convey no extra meaning. </s><s n="s2.4;p37.179">We define <hi rend="italic">redundancy</hi> as the number of digits used divided by the minimum necessary. </s><s n="s3.4;p37.179">So if we were sending 0110, the minimum number of digits necessary would be 4. </s><s n="s4.4;p37.179">If we were sending it by a repetition code, then there would be 4 digits not contributing anything to the meaning of the data. </s></p><p n="p38.179"><s n="s1.1;p38.179">For this example, the redundancy is 8 / 2 = 4. </s></p><p n="p39.179"><s n="s1.1;p39.179">For the parity check code, redundancy has a general formula: </s></p><p n="p40.179"><s n="s1.1;p40.179"><formula notation="" id="BAWE_0403a-form.004"/> </s></p><p n="p41.179"><s n="s1.1;p41.179">As n increases, this redundancy gets smaller, however, its ability to detect errors in a noisy channel decrease. </s></p></div2><div2><head rend="bold italic">Information rate</head><p n="p42.179"><s n="s1.3;p42.179">The information rate is the fraction of <hi rend="italic">data</hi> over the whole message size. </s><s n="s2.3;p42.179">In the repetition code above, 4 bits are useful data, and the rest useless. </s><s n="s3.3;p42.179">Therefore the information rate is 4 / 8 = 0.50. </s></p><p n="p43.179"><s n="s1.1;p43.179">In the parity check code, the information rate is (n - 1) / n. </s></p></div2><div2><head rend="bold italic">Error correcting codes - Rectangular</head><p n="p44.179"><s n="s1.3;p44.179">Hamming thought that if a code can <hi rend="italic">detect</hi> an error, then why can it not <hi rend="italic">correct it</hi>? </s><s n="s2.3;p44.179">This is the basis of error correcting codes. </s><s n="s3.3;p44.179">One such code is called the 'Rectangular' error correcting code. </s></p><p n="p45.179"><s n="s1.2;p45.179">For this code to work we have to assume that the probability of two or more errors is <hi rend="underlined">very small.</hi> </s><s n="s2.2;p45.179">If this is true for length of the message we have chosen, then we can put the codeword into a rectangle and use a simple parity check. </s></p><p n="p46.179"><s n="s1.1;p46.179">For example, if we wanted to send the codeword 010101101, we lay it into the white boxes in the square below, from left to right and top to bottom. </s></p><table id="BAWE_0403a-tab.002"><row><cell/></row></table><p n="p47.179"><s n="s1.3;p47.179">The yellow boxes are then filled by parity digits. </s><s n="s2.3;p47.179">For the first row '010' is odd which means the parity digit is 1. </s><s n="s3.3;p47.179">This is completed for all rows and columns. </s></p><table id="BAWE_0403a-tab.003"><row><cell/></row></table><p n="p48.179"><s n="s1.1;p48.179">Then we would transmit all of the data, reading left to right and top to bottom: </s></p><figure id="BAWE_0403a-pic.003"/><p n="p49.179"><s n="s1.3;p49.179">The red digit on the receiving end shows that an error occurred. </s><s n="s2.3;p49.179">However the receiver does not yet know this. </s><s n="s3.3;p49.179">If we place the received codeword into a square and run our own parity checks, we can see that two parity digits are wrong: </s></p><table id="BAWE_0403a-tab.004"><row><cell/></row></table><p n="p50.179"><s n="s1.1;p50.179">The parity check failed in row y and column x, which means the error lies in cell (x, y): </s></p><table id="BAWE_0403a-tab.005"><row><cell/></row></table><p n="p51.179"><s n="s1.2;p51.179">We can simply change this cell to a 0, and we now have 010101101, which is the corrected codeword. </s><s n="s2.2;p51.179">We need n <hi rend="sup">2</hi> bits to send this code, but we only use (n-1)<hi rend="sup">2</hi>, so the redundancy of the code is: </s></p><p n="p52.179"><s n="s1.1;p52.179"><formula notation="" id="BAWE_0403a-form.005"/> </s></p><p n="p53.179"><s n="s1.2;p53.179">If the error occurred in any of the parity digits, the bottom right parity digit would be wrong, so no correction on the code would be needed. </s><s n="s2.2;p53.179">This code only allows for <hi rend="italic">one</hi> error to be corrected; multiple error correcting codes are discussed later. </s></p></div2><div2><head rend="bold italic">Error correcting codes - Triangular</head><p n="p54.179"><s n="s1.1;p54.179">Exactly the same principle can be applied but instead of filling a square, we fill a triangle: </s></p><table id="BAWE_0403a-tab.006"><row><cell/></row></table><p n="p55.179"><s n="s1.3;p55.179">Again, this would only work if the chance of two or more errors was very small. </s><s n="s2.3;p55.179">If an error occurred and the parity check failed in row y and column x, the error would lie in cell (x, y). </s><s n="s3.3;p55.179">The redundancy of this code is: </s></p><p n="p56.179"><s n="s1.1;p56.179"><formula notation="" id="BAWE_0403a-form.006"/> </s></p></div2><div2><head rend="bold italic">Perfect Hamming Codes</head><div3><head rend="underlined italic">The Syndrome</head><p n="p57.179"><s n="s1.2;p57.179">A perfect Hamming code uses a syndrome. </s><s n="s2.2;p57.179">In order to explain what a perfect Hamming code is we first need to explore the syndrome. </s></p><p n="p58.179"><s n="s1.3;p58.179">A parity check either returns a 0 or a 1. </s><s n="s2.3;p58.179">If we run different parity checks multiple times we can end up with a string of 0's and 1's. </s><s n="s3.3;p58.179">This is called a syndrome. </s></p><p n="p59.179"><s n="s1.1;p59.179">We expect a syndrome to be able to tell us <hi rend="underlined">one</hi> of n + 1 things: </s></p><p rend="ordered" n="p60.179"><s n="s1.1;p60.179">The location of any error in one of the n bits of the message </s></p><p rend="ordered" n="p61.179"><s n="s1.1;p61.179">The case that no error has occurred </s></p><p n="p62.179"><s n="s1.3;p62.179">If we define the number of parity checks to be m, then we have m bits in the syndrome. </s><s n="s2.3;p62.179">Under binary, this means it can take on one of 2 <hi rend="sup">m</hi> values. </s><s n="s3.3;p62.179">Hence we want: </s></p><p n="p63.179"><s n="s1.1;p63.179"><formula notation="" id="BAWE_0403a-form.007"/> </s></p><p n="p64.179"><s n="s1.1;p64.179">This inequality is used to decide the minimum number of check digits necessary for a given code. </s></p></div3><div3><head rend="underlined italic">Hamming Codes</head><p n="p65.179"><s n="s1.2;p65.179">There is an elegant and ultimately beautiful way of creating a code such that the above inequality holds, this is said to be a (n, m) Hamming Code. </s><s n="s2.2;p65.179">Later on we will find some of these codes are <hi rend="italic">perfect</hi>. </s></p><p n="p66.179"><s n="s1.1;p66.179">In the explanation and demonstration of a Hamming Code, we will use a codeword of length 4, and a binary code alphabet. </s></p><p n="p67.179"><s n="s1.2;p67.179">Hamming suggested using a sequence of parity checks. </s><s n="s2.2;p67.179">Using the inequality discussed above we can see that only 3 check digits are needed for a codeword of length 4: </s></p><p n="p68.179"><s n="s1.1;p68.179"><formula notation="" id="BAWE_0403a-form.008"/> </s></p><p n="p69.179"><s n="s1.3;p69.179">Consequently, somewhere 'out there' Hamming knew that there was a single-error correcting code with an information rate of 0.57 (4 / 7). </s><s n="s2.3;p69.179">Hamming's syndrome must take on a value between 1 and 7 to indicate the position of the error or 0 if no error occurred. </s><s n="s3.3;p69.179">In binary: </s></p><table id="BAWE_0403a-tab.007"><row><cell/></row></table><p n="p70.179"><s n="s1.5;p70.179">The first parity check creates the first digit of the syndrome (when we say first, we mean the 2 <hi rend="sup">0</hi> digit - which is actually the first digit from the <hi rend="underlined">right</hi>). </s><s n="s2.5;p70.179">Therefore if an error occurred in the check the final syndrome is restricted to having a 1 in this position - 1, 3, 5 and 7. </s><s n="s3.5;p70.179">So the first parity check should be done on these positions in the codeword. </s><s n="s4.5;p70.179">Applying this logic to the second parity check gives positions 2, 3, 6 and 7 (all syndromes with a 1 in the middle digit). </s><s n="s5.5;p70.179">The third party check should be run on positions 4, 5, 6 and 7. </s></p><table id="BAWE_0403a-tab.008"><row><cell/></row></table><p n="p71.179"><s n="s1.2;p71.179">The parity checks have to be independent; no two parity checks should check each other. </s><s n="s2.2;p71.179">The only way to do this is to place them in a position that is only checked by the <hi rend="italic">i</hi>th parity check - places 1,2 and 4 only have one 1. </s></p><p n="p72.179"><s n="s1.1;p72.179">This leaves places 3, 5, 6 and 7 for our codeword; I will use 1010: </s></p><table id="BAWE_0403a-tab.009"><row><cell/></row></table><p n="p73.179"><s n="s1.1;p73.179">Now we run the first parity check on positions 1, 3, 5, 7: </s></p><p n="p74.179"><s n="s1.1;p74.179">0 + 1 + 0 + 0 = 1 which is odd, so the parity digit is 1. </s></p><p n="p75.179"><s n="s1.1;p75.179">This digit is placed into position 1: </s></p><table id="BAWE_0403a-tab.010"><row><cell/></row></table><p n="p76.179"><s n="s1.1;p76.179">Now we run the second parity check on positions 2, 3, 6, 7: </s></p><p n="p77.179"><s n="s1.1;p77.179">0 + 1 + 1 + 0 = 2 which is even, so the parity digit is 0. </s></p><p n="p78.179"><s n="s1.1;p78.179">This digit is placed into position 2: </s></p><table id="BAWE_0403a-tab.011"><row><cell/></row></table><p n="p79.179"><s n="s1.1;p79.179">Now we run the third parity check on positions 4, 5, 6, 7: </s></p><p n="p80.179"><s n="s1.1;p80.179">0 + 0 + 1 + 0 = 1 which is odd, so the parity digit is 1. </s></p><p n="p81.179"><s n="s1.1;p81.179">This digit is placed into position 4: </s></p><table id="BAWE_0403a-tab.012"><row><cell/></row></table><p n="p82.179"><s n="s1.2;p82.179">How do we know the syndrome indicates the position of an error? </s><s n="s2.2;p82.179">Let H be a 3 x 7 matrix whose columns are the binary numbers from 1 to 7: </s></p><p n="p83.179"><s n="s1.1;p83.179"><formula notation="" id="BAWE_0403a-form.009"/> </s></p><p n="p84.179"><s n="s1.2;p84.179">Notice the links between the places of 1's and the positions we ran each of the three parity checks on. </s><s n="s2.2;p84.179">The bottom row corresponds to the first parity check, the middle row to the second, and the top row to the third. </s></p><p n="p85.179"><s n="s1.1;p85.179">If we multiply a codeword on the left by H (and mod 2 on the final 3 x 1 matrix), it is equivalent to running the three parity checks: </s></p><p n="p86.179"><s n="s1.1;p86.179"><formula notation="" id="BAWE_0403a-form.010"/> </s></p><p n="p87.179"><s n="s1.2;p87.179">The syndrome 0, 0, 0 indicates no error is present. </s><s n="s2.2;p87.179">Let us test the code by artificially creating an error in the codeword: </s></p><figure id="BAWE_0403a-pic.004"/><p n="p88.179"><s n="s1.1;p88.179">Now when we multiply on the left by H we get: </s></p><p n="p89.179"><s n="s1.1;p89.179"><formula notation="" id="BAWE_0403a-form.011"/> </s></p><p n="p90.179"><s n="s1.1;p90.179">This tells us an error occurred in the 2nd digit of the codeword, so we can correct it. </s></p></div3></div2><div2><head rend="bold italic">Multiple-error correcting codes</head><p n="p91.179"><s n="s1.2;p91.179">Possibly one of the strongest multiple-error correcting codes is the Reed-Solomon code. </s><s n="s2.2;p91.179">This code was invented in 1960 when digital technology at that time was actually not advanced enough to implement the code. </s></p><p n="p92.179"><s n="s1.3;p92.179">It is possible to correct up to 4000 errors on a CD by making use of an interleaved code. </s><s n="s2.3;p92.179">The code however has a weakness; if multiple errors occur in a 'symbol' (like a 'byte' of information) then the Reed-Solomon code sees this as one error. </s><s n="s3.3;p92.179">This makes the code suitable for lines where errors occur in bursts but inefficient on lines where single errors occur randomly. </s></p></div2><div2><head rend="bold italic">Hamming Distance - linking with Metric Spaces</head><p n="p93.179"><s n="s1.2;p93.179">We can define the distance between two codewords of binary as the number of digits in which they differ. </s><s n="s2.2;p93.179">For example, the distance between 000 and 111 is 3, and between 010 and 011 is 1. </s></p><p n="p94.179"><s n="s1.1;p94.179">We define the distance function d(x, y) to be the distance between x and y. d(x, y) is a metric because it satisfies all the axioms: </s></p><list type="ordered"><item>d(x, y) >= 0 for all x, y</item><item>If d(x, y) = 0 x=y</item><item>d(x, y) = d(y, x) for all x, y</item><item>d(x, z) &lt;= d(x, y) + d(y, z) for all x, y, z</item></list><p n="p95.179"><s n="s1.1;p95.179">With the Hamming distance, it is possible to view codes geometrically. </s></p></div2><div2><head rend="bold italic">Geometry of the Hamming Distance</head><div3><head rend="underlined">Minimum Distance 2</head><p n="p96.179"><s n="s1.1;p96.179">If we use a (2, 1) repetition code where we send each bit twice, then there are 4 possible codewords that could be received, with the middle two being errors: </s></p><p n="p97.179"><s n="s1.1;p97.179"><formula notation="" id="BAWE_0403a-form.012"/> </s></p><p n="p98.179"><s n="s1.1;p98.179">Placing these into a square using the Hamming Distance between them as the length of the side we get: </s></p><figure id="BAWE_0403a-pic.005"/><p n="p99.179"><s n="s1.2;p99.179">An error in sending the codewords 00 or 11 would send them along one of the edges to 01 or 10. </s><s n="s2.2;p99.179">As we are using a (2, 1) repetition code then we only expect 00 or 11, so we know an error occurred. </s></p><p n="p100.179"><s n="s1.2;p100.179">In order for a code to be <hi rend="underlined">single error detecting</hi>, its minimum distance must be at least 2. </s><s n="s2.2;p100.179">Geometrically, this means the distance between 00 and 11. </s></p></div3><div3><head rend="underlined">Minimum Distance 3</head><p n="p101.179"><s n="s1.1;p101.179">If we use a (3, 1) repetition code, there are 8 possible codewords that could be received: </s></p><p n="p102.179"><s n="s1.1;p102.179"><formula notation="" id="BAWE_0403a-form.013"/> </s></p><p n="p103.179"><s n="s1.1;p103.179">Placing these into a cube, using the Hamming Distance as the length of the sides we get: </s></p><figure id="BAWE_0403a-pic.006"/><p n="p104.179"><s n="s1.2;p104.179">If a single error were to occur, it would send 000 or 111 to a non-codeword symbol; however this non-codeword would still be closer to its original form. </s><s n="s2.2;p104.179">This means we could <hi rend="italic">correct</hi> the error. </s></p><p n="p105.179"><s n="s1.2;p105.179">In order for a code to be <hi rend="underlined">single error correcting</hi> its minimum distance must be at least 3. </s><s n="s2.2;p105.179">This code could <hi rend="underlined">detect</hi> two errors. </s></p><p n="p106.179"><s n="s1.2;p106.179">This continues up the minimum distance scale. </s><s n="s2.2;p106.179">A minimum distance of n means we can detect n / 2 errors and correct (n - 1) / 2 errors. </s></p></div3></div2><div2><head rend="bold italic">The elegant link between Coding Theory, mathematics and oranges</head><p n="p107.179"><s n="s1.1;p107.179">Pure mathematics plays a big part in solving the problems coding theory faces; even in the simplest codes, mathematics is used. </s></p><p n="p108.179"><s n="s1.2;p108.179">Linear Algebra and Metric Spaces are also involved in Coding theory. </s><s n="s2.2;p108.179">Codes are the kernel of a linear transformation and it is possible to define a metric between two binary codes. </s></p><p n="p109.179"><s n="s1.2;p109.179"><hi rend="italic">Perfect codes</hi> do exist. </s><s n="s2.2;p109.179">These perfect codes have a special geometrical property: </s></p><p n="p110.179"><s n="s1.1;p110.179">Given a set of points S in n-dimensional space then </s></p><p n="p111.179"><s n="s1.1;p111.179">disjoint n-dimensional spheres can be placed around about </s></p><p n="p112.179"><s n="s1.1;p112.179">each element of the code such that they contain </s></p><p n="p113.179"><s n="s1.1;p113.179">all of the points of S, each point in exactly ONE sphere. </s></p><p n="p114.179"><s n="s1.2;p114.179">This is known as the 'Sphere Packing Problem' (SPP). </s><s n="s2.2;p114.179">The 3-Dimensional SPP is demonstrated in the following formulation: </s></p><p n="p115.179"><s n="s1.1;p115.179">"How can oranges be packed into a box so that as many as possible can be taken?" </s></p><p n="p116.179"><s n="s1.1;p116.179">In other words: 'minimising the unused space'. </s></p><p n="p117.179"><s n="s1.1;p117.179">The (7, 4) Hamming Code, discussed earlier, has these properties of packing spheres so is <hi rend="italic">perfect.</hi> </s></p></div2><div2><head rend="bold italic">Summary for Coding theory</head><p n="p118.179"><s n="s1.3;p118.179">Within these pages I have explored and explained just part of the complex area of maths known as coding theory. </s><s n="s2.3;p118.179">Coding theory will always play a part in our lives, especially in this computer era. </s><s n="s3.3;p118.179">Interestingly, coding theory exists outside of computers; nature also chose the 'parity check' for DNA and proteins. </s></p><p n="p119.179"><s n="s1.1;p119.179">Satellites sent into space will begin to use more complex codes so that very weak signals from the vast distances of space can be received and understood and the advances in quantum computing will call for stronger codes to be invented and used. </s></p></div2></div1><div1 type="text"><head rend="underlined bold italic">Cryptography</head><div2><head rend="bold italic">Introduction</head><p n="p120.179"><s n="s1.2;p120.179">Cryptography is an area of maths that deals with information security. </s><s n="s2.2;p120.179">In the modern world this is closely linked with computers, however cryptography existed many years before the traditional computer. </s></p><p n="p121.179"><s n="s1.3;p121.179">Cryptography involves taking a piece of data and encrypting it, using a key, to make the data impossible to read unless the user can decrypt the data. </s><s n="s2.3;p121.179">This can be compared to putting a piece of information into a box and locking it. </s><s n="s3.3;p121.179">Only somebody with the same key could unlock the box and read the information. </s></p></div2><div2><head rend="bold italic">Motivation</head><p n="p122.179"><s n="s1.4;p122.179">Secrets between people have existed probably since the start of mankind. </s><s n="s2.4;p122.179">Sometimes secrets have to be passed on to a particular person; how do we do this without letting anyone else hear our secret? </s><s n="s3.4;p122.179">Naturally this is quite hard; people can overhear a conversation or intercept a letter. </s><s n="s4.4;p122.179">Is it possible to come up with a system so that even <hi rend="italic">if</hi> somebody heard or saw our secret, they would not be able to understand it? </s></p><p n="p123.179"><s n="s1.1;p123.179">People quickly found that it was possible, either by creating a new language or coming up with a 'code' that nobody could understand unless you were taught it. </s></p></div2><div2><head rend="bold italic">The early days of Cryptography</head><p n="p124.179"><s n="s1.2;p124.179">In the beginning, cryptography was concerned with <hi rend="italic">language</hi>. </s><s n="s2.2;p124.179">Most codes were language based, this means that all that was really needed was a pen and paper. </s></p><p n="p125.179"><s n="s1.2;p125.179">There were two main systems in use; transposition ciphers and substitution ciphers. </s><s n="s2.2;p125.179">Transposition ciphers involved taking a message and rearranging the letters in the message and substitution ciphers entailed replacing letters by other letters, or indeed groups of letters. </s></p><div3><head rend="underlined">Caesar Cipher</head><p n="p126.179"><s n="s1.5;p126.179">One of the earliest ciphers was the Caesar cipher. </s><s n="s2.5;p126.179">This involved shifting each letter in the plain text (the original message) along the alphabet a certain number of times. </s><s n="s3.5;p126.179">Caesar used a shift of three. </s><s n="s4.5;p126.179">For example, A would go to D, B to E, C to F... </s><s n="s5.5;p126.179">Z to C. This makes the Caesar cipher a substitution cipher. </s></p><p n="p127.179"><s n="s1.1;p127.179">Multiple encryptions would add no more security to the data, as a shift of 1 applied twice is exactly the same as applying a shift of 2 only once. </s></p><p n="p128.179"><s n="s1.2;p128.179">Unfortunately, we have no way of knowing how successful the Caesar cipher was at keeping secrets safe. </s><s n="s2.2;p128.179">In Caesars favour, most of the people at that time could not read, let alone create some way to decipher his messages. </s></p><p n="p129.179"><s n="s1.4;p129.179">Frequency analysis was discovered in the 9 <hi rend="sup">th</hi> Century AD. It involves counting each letter in the message and tallying up the totals. </s><s n="s2.4;p129.179">For English, 'e' is the most common letter. </s><s n="s3.4;p129.179">Shift ciphers, (the general form of a Caesar cipher) can be broken using frequency analysis, the most common letter in the cipher text would be the equivalent of an 'e'. </s><s n="s4.4;p129.179">It would be possible to then count the distance between this letter and 'e', giving the shift value. </s></p><p n="p130.179"><s n="s1.2;p130.179">Brute force can also be used to break shift ciphers. </s><s n="s2.2;p130.179">If 'jgnnq' was the cipher text, we could draw up a table cracking the code: </s></p><table id="BAWE_0403a-tab.013"><row><cell/></row></table><p n="p131.179"><s n="s1.1;p131.179">Immediately to the human eye the plaintext 'hello' stands out. </s></p></div3><div3><head rend="underlined">The Ancient Greek scytale</head><p n="p132.179"><s n="s1.3;p132.179">This is a scytale, probably one of the first transposition ciphers invented. </s><s n="s2.3;p132.179">A tape would be wrapped around a stick and the message written down one side. </s><s n="s3.3;p132.179">Random letters would then be filled around the other sides so when the tape was unravelled the original message is completely hidden. </s></p><figure id="BAWE_0403a-pic.007"/><p n="p133.179"><s n="s1.2;p133.179">Of course if you had no idea how the message was encrypted this would be hard to break. </s><s n="s2.2;p133.179">If you did know a scytale was used however, you could read the message using different spacing between letters...or just grab the nearest stick. </s></p></div3><div3><head rend="underlined">Vigenère Cipher</head><p n="p134.179"><s n="s1.2;p134.179">The Vigenère cipher is similar to the Caesar cipher but uses a different shift according to the place in the 'key'. </s><s n="s2.2;p134.179">It was invented in 1553 by Giovan Batista Belaso, but was formally given its (incorrect) name 'Vigenère Cipher' in the 19 <hi rend="sup">th</hi> Century. </s></p><p n="p135.179"><s n="s1.1;p135.179">A Vigenère square is used to encode and decode the messages: </s></p><figure id="BAWE_0403a-pic.008"/><p n="p136.179"><s n="s1.2;p136.179">If we wanted to encrypt the text "Cryptography" we would choose any key as long as both parties knew it. </s><s n="s2.2;p136.179">We will use "MATHS". </s></p><p n="p137.179"><s n="s1.3;p137.179">The key is repeatedly written out below the plain text. </s><s n="s2.3;p137.179">To encode the letter 'C' we would look down the column C and across the row 'M' until they met; giving us 'O'. </s><s n="s3.3;p137.179">This is repeated for the whole message: </s></p><p n="p138.179"><s n="s1.2;p138.179">This gives the cipher text 'ORRWLAGKHHTY'. </s><s n="s2.2;p138.179">This method of encryption does not make the cipher text unbreakable when using frequency analysis as discussed earlier, just slightly harder. </s></p><table id="BAWE_0403a-tab.014"><row><cell/></row></table><p n="p139.179"><s n="s1.1;p139.179">The opposite method would be used to decrypt the message. </s></p><p n="p140.179"><s n="s1.1;p140.179">The main concept with the Vigenère cipher is, the longer the key, the harder it is to break. </s></p><p n="p141.179"><s n="s1.4;p141.179">Shortly after the Vigenère cipher was invented, the 'one time pad' concept was born. </s><s n="s2.4;p141.179">This combined using a Vigenère cipher with a key that was entirely random and as long as the original message. </s><s n="s3.4;p141.179">This proved to be <hi rend="underlined">mathematically</hi> unbreakable. </s><s n="s4.4;p141.179">It however faced certain downfalls: </s></p><p rend="ordered" n="p142.179"><s n="s1.1;p142.179">It was very hard for the recipient of the message to be given the 'one time pad' (the key) without it falling into the wrong hands. </s></p><p rend="ordered" n="p143.179"><s n="s1.1;p143.179">A true random one time pad was near impossible to create - humans can only be 'pseudo random'. </s></p><p n="p144.179"><s n="s1.1;p144.179">Tests were later invented to help determine the length of the key used; this greatly helps any code-breaker when trying to decrypt a message that uses the Vigenère cipher. </s></p></div3><div3><head rend="underlined">The idea behind the perfect cipher</head><p n="p145.179"><s n="s1.2;p145.179">While frequency analysis proved to be a powerful tool with code breaking, the Vigenère cipher and others like it were still used often. </s><s n="s2.2;p145.179">There were so many ciphers in use that the code breaker would have to try many techniques before they found the right one. </s></p><p n="p146.179"><s n="s1.2;p146.179">However the perfect cipher would be one where even <hi rend="italic">if</hi> the code breaker knew it had been used it was still secure. </s><s n="s2.2;p146.179">This idea is known as "Kerchoff's Law". </s></p></div3><div3><head rend="underlined">Secrets and even bigger secrets</head><p n="p147.179"><s n="s1.2;p147.179">I believe if you asked random people in the street the words they associated with "cryptography", somewhere in the top 10 most common words "Enigma" would appear. </s><s n="s2.2;p147.179">A significant contribution to our success in the Second World War came from our mathematicians in Bletchley Park, helping break important German secrets. </s></p><p n="p148.179"><s n="s1.5;p148.179">The Germans however had a much more powerful and subtle cipher. </s><s n="s2.5;p148.179">It was known as the Geheimschreiber. </s><s n="s3.5;p148.179">This translates to "the secret writer". </s><s n="s4.5;p148.179">The Enigma machine required an operator to manually type the message into a typewriter keyboard and note down the corresponding letters that were displayed. </s><s n="s5.5;p148.179">The Geheimschreiber was <hi rend="underlined">much</hi> more efficient, taking teleprinter input and sending its output to the receiver's Geheimschreiber machine ready for automatic decryption. </s></p><p n="p149.179"><s n="s1.2;p149.179">The need to break the output of the Geheimschreiber machines output led Bletchley Park to create the worlds first large scale computer, known as the Colossus. </s><s n="s2.2;p149.179">The code was broken, and provided the Allies a much needed insight into the mind of the Third Reich. </s></p></div3></div2><div2><head rend="bold italic">Cryptography now</head><p n="p150.179"><s n="s1.1;p150.179">There are two main areas of modern cryptography, symmetric key and public (asymmetric) key cryptography. </s></p><div3><head rend="underlined">Symmetric Key</head><p n="p151.179"><s n="s1.2;p151.179">Put simply, symmetric key cryptography describes an encryption system where both 'Alice' and 'Bob' (names frequently used in Cryptography instead of person 'A' and person 'B' to aid understanding) have the same key. </s><s n="s2.2;p151.179">This was understood to be the only method of encryption until 1976. </s></p><p n="p152.179"><s n="s1.2;p152.179">Block ciphers take a message and output encrypted text of the same length. </s><s n="s2.2;p152.179">This is deemed not to be secure enough for today's standards, as an encrypted text should never be the same twice. </s></p><p n="p153.179"><s n="s1.4;p153.179">One such cipher is DES (Data Encryption Standard) invented by IBM in 1975. </s><s n="s2.4;p153.179">It was later disregarded as a single key could be broken in less than 24 hours by using a brute force attack. </s><s n="s3.4;p153.179">A stronger form AES (Advanced Encryption Standard) was first published in 1998. </s><s n="s4.4;p153.179">This proved so strong the US Government approved its use on classified information. </s></p><p n="p154.179"><s n="s1.1;p154.179">It has been claimed that AES is breakable, but designers of AES looked at the proposed 'break' and were quick to comment on some of the hackers estimates for the time needed. </s></p><p n="p155.179"><s n="s1.6;p155.179">One technique that was previously used on the internet to exchange secret information involved a 'double padlock' technique. </s><s n="s2.6;p155.179">Alice would lock her message with her own padlock and key and then post this package to Bob. </s><s n="s3.6;p155.179">Bob would lock this package with his (different) padlock and key and post it back to Alice. </s><s n="s4.6;p155.179">Alice would then unlock her padlock and send the package back to Bob. </s><s n="s5.6;p155.179">This would only leave Bob's padlock on the package, so Bob could unlock it with his key. </s><s n="s6.6;p155.179">At no stage could anyone read the information and also Alice and Bob did not have to exchange keys - a risky event. </s></p><p n="p156.179"><s n="s1.2;p156.179">This method was slow and required many transmissions of the data, increasing the chance it would be intercepted. </s><s n="s2.2;p156.179">Another type of encryption was necessary. </s></p></div3><div3><head rend="underlined">Public Key Cryptography</head><p n="p157.179"><s n="s1.6;p157.179">Public Key cryptography is also known as asymmetric key cryptography, because of the difference between the keys needed for encryption and decryption. </s><s n="s2.6;p157.179">It is hard to imagine a padlock where you need two different keys to lock and unlock it, but the idea is relatively simple. </s><s n="s3.6;p157.179">Alice would calculate the key to encrypt the data and using another method, calculate the appropriate related decryption key. </s><s n="s4.6;p157.179">Alice can then safely transmit the encryption key knowing it does not matter whose hands this information falls into. </s><s n="s5.6;p157.179">Bob would then encrypt his message using the encryption (Public) key and transmit the message back to Alice. </s><s n="s6.6;p157.179">Alice can now use her private key to decrypt the message. </s></p><p n="p158.179"><s n="s1.1;p158.179">A popular cryptosystem using Public Key cryptography is RSA. It was published in 1976 although GCHQ in England later announced they had secretly discovered it in 1970. </s></p><p n="p159.179"><s n="s1.1;p159.179">The name RSA comes from its inventors: Ron <hi rend="bold">R</hi>ivest, Adi <hi rend="bold">S</hi>hamir and Len <hi rend="bold">A</hi>dleman. </s></p><p n="p160.179"><s n="s1.1;p160.179">To generate the public and private key, Alice needs to do 5 operations: </s></p><p rend="ordered" n="p161.179"><s n="s1.1;p161.179">Choose two large prime numbers p and q such that p ≠ q, randomly and independently of each other </s></p><p rend="ordered" n="p162.179"><s n="s1.1;p162.179">Compute n = pq </s></p><p rend="ordered" n="p163.179"><s n="s1.1;p163.179">Compute the totient Φ(n) = (p - 1)(q - 1) </s></p><p rend="ordered" n="p164.179"><s n="s1.1;p164.179">Choose an integer e such that 1 &lt; e &lt; Φ(n) which is coprime to Φ(n) </s></p><p rend="ordered" n="p165.179"><s n="s1.1;p165.179">Compute d such that de ≡ 1 ( mode Φ(n)) </s></p><p n="p166.179"><s n="s1.2;p166.179">When these steps are complete, the public key consists of n and e. </s><s n="s2.2;p166.179">The private key consists of n and d. </s></p><p n="p167.179"><s n="s1.3;p167.179">To encrypt a message, Bob would turn his message into a number m. </s><s n="s2.3;p167.179">With m &lt; n, to do this he would use a padding scheme that had been agreed upon previously. </s><s n="s3.3;p167.179">The encrypted message 'c' is calculated as follows: </s></p><p n="p168.179"><s n="s1.1;p168.179"><formula notation="" id="BAWE_0403a-form.014"/> </s></p><p n="p169.179"><s n="s1.2;p169.179">Bob knows n and e, as they are included in the Public key. </s><s n="s2.2;p169.179">Bob can now transmit 'c' safely to Alice. </s></p><p n="p170.179"><s n="s1.1;p170.179">To decrypt the message, Alice uses the following calculation: </s></p><p n="p171.179"><s n="s1.1;p171.179"><formula notation="" id="BAWE_0403a-form.015"/> </s></p><p n="p172.179"><s n="s1.2;p172.179">Alice previously calculated d, as it forms part of her private key. </s><s n="s2.2;p172.179">Using m she can find M, the original message. </s></p></div3></div2><div2><head rend="bold italic">The future of Cryptography</head><p n="p173.179"><s n="s1.6;p173.179">Computer speed is growing exponentially, every year computing speed quadruples at the very least. </s><s n="s2.6;p173.179">Is there a ceiling to computing speed? </s><s n="s3.6;p173.179">With the predicted upcoming advance into quantum computing most people would argue that if there <hi rend="italic">is</hi> a ceiling, it is still a <hi rend="underlined">very</hi> long way off. </s><s n="s4.6;p173.179">This is the main concern to cryptography. </s><s n="s5.6;p173.179">Cryptanalysis is the art of breaking codes, either by finding a weakness or using brute force. </s><s n="s6.6;p173.179">The brute force approach is best done with a powerful computer - the faster the computer the quicker the code is broken. </s></p><p n="p174.179"><s n="s1.2;p174.179">No matter how powerful a computer is however, some cryptography techniques are unbreakable, for example the one-time pad technique. </s><s n="s2.2;p174.179">If a code is not 100% unbreakable, the operations needed to decrypt it will be exponential to the number of operations needed to encrypt it. </s></p><p n="p175.179"><s n="s1.2;p175.179">Not all cryptanalysis methods require exploiting weaknesses - a side-channel attack involves analysing the time it takes to encrypt a message. </s><s n="s2.2;p175.179">Knowing this you may be able to deduce which cipher was used. </s></p><p n="p176.179"><s n="s1.3;p176.179">30 years ago, we did not know Public Key cryptography was possible, because of this the future of cryptography is unclear. </s><s n="s2.3;p176.179">I predict a 'key' will always be necessary but that the types of key we use will change. </s><s n="s3.3;p176.179">Imagine a system where the key is your fingerprint or an iris scan - hard to implement but equally hard to break. </s></p></div2><div2><head rend="bold italic">Summary</head><p n="p177.179"><s n="s1.3;p177.179">In many countries Cryptography is <hi rend="underlined">illegal</hi>. </s><s n="s2.3;p177.179">The government will set an encryption standard so that if needed they could read all messages being sent within their networks. </s><s n="s3.3;p177.179">America does not let people export encryption products that use a key of longer than 40-bits - a very weak encryption. </s></p><p n="p178.179"><s n="s1.2;p178.179">However, Governments themselves are hungry for cryptography, often pouring money into its research - they want their secrets to <hi rend="italic">stay</hi> secret. </s><s n="s2.2;p178.179">Parallel to this, a Government will also have a team investigating cryptanalysis, just like the people at Bletchley Park during the War. </s></p><p n="p179.179"><s n="s1.1;p179.179">The reason for this as Robin Morgan (a poet and writer) says is 'Information is power'. </s></p></div2></div1></body><back><div1 type="bibliography"><head>References</head><p>Sharon Heumann - <seg type="URL" n="www.mdstud.chalmers.se/%7Emd7sharo/coding/main/"/>. Equation images are also taken from this website.</p><p>Richard Pinch - Coding Theory: the first 50 years - <seg type="URL" n="http://plus.maths.org/issue3/codes/"/> </p><p>Wikipedia - <seg type="URL" n="http://en.wikipedia.org/wiki/Cryptography"/></p><p>The Times - "Colossus that bestrode an Enigma" - Saturday 11<hi rend="sup">th</hi> March 2006</p><p>Simon Singh - "The Code Book: The Secret History of Codes and Code-Breaking"</p></div1></back></text></TEI.2>
