<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE TEI.2 SYSTEM "tei_bawe.dtd"><TEI.2 id="_3052f" n="version 1.0"><teiHeader><fileDesc><titleStmt><title>Final report</title></titleStmt><extent/><publicationStmt><distributor>British Academic Written English (BAWE) corpus</distributor><availability><p>The British Academic Written English (BAWE) corpus was developed at the Universities of Warwick, Reading and Oxford Brookes, under the directorship of Hilary Nesi and Sheena Gardner (formerly of the Centre for Applied Linguistics [previously called CELTE], Warwick), Paul Thompson (Department of Applied Linguistics, Reading) and Paul Wickens (Westminster Institute of Education, Oxford Brookes), with funding from the ESRC. Subject to the rights of the these institutions in the BAWE corpus, and pursuant to the ESRC agreement, the BAWE corpus is available to researchers for research purposes PROVIDED THAT the following conditions are met:</p><p>1. The corpus files are not distributed in either their original form or in modified form.</p><p>2. The texts are used for research purposes only; they should not be reproduced in teaching materials.</p><p>3. The texts are not reproduced in full for a wider audience/readership, although researchers are free to quote short passages of text (up to 200 running words from any given text).</p><p>4. The BAWE corpus developers (contact: Hilary Nesi) are informed of all projects, dissertations, theses, presentations or publications arising from analysis of the corpus.</p><p>5. Researchers acknowledge their use of the corpus using the following form of words: "The data in this study come from the British Academic Written English (BAWE) corpus, which was developed at the Universities of Warwick, Reading and Oxford Brookes under the directorship of Hilary Nesi and Sheena Gardner (formerly of the Centre for Applied Linguistics [previously called CELTE], Warwick), Paul Thompson (Department of Applied Linguistics, Reading) and Paul Wickens (Westminster Institute of Education, Oxford Brookes), with funding from the ESRC (RES-000-23-0800)."</p></availability></publicationStmt><notesStmt><note resp="British Academic Written English (BAWE) corpus project">Language used in quote: <foreign id="English">English</foreign></note><note resp="British Academic Written English (BAWE) corpus project">Language used in quote: <foreign id="French">French</foreign></note></notesStmt><sourceDesc><p n="level">3</p><p n="date">2005-12</p><p n="module title">BSc computing project</p><p n="module code">U08096</p><p n="genre family">Methodology recount</p><p n="discipline">Computer Science</p><p n="disciplinary group">PS</p><p n="grade">M</p><p n="number of authors">1</p><p n="number of words">11375</p><p n="number of s-units">639</p><p n="number of p">300</p><p n="number of tables">0</p><p n="number of figures">17</p><p n="number of block quotes">2</p><p n="number of formulae">65</p><p n="number of lists">0</p><p n="number of paragraphs formatted like lists">16</p><p n="abstract present">abstract present</p><p n="average words per s-unit">17.8</p><p n="average s-units per p">2.1</p><p n="macrotype of assignment">simple assignment</p></sourceDesc></fileDesc><encodingDesc><p>TEI P4 (documented in: BAWE.documentation.pdf)</p></encodingDesc><profileDesc><particDesc><person><p n="gender">m</p><p n="year of birth">1985</p><p n="first language">French</p><p n="education">OSa</p><p n="course">Computing</p><p n="student ID">3052</p></person></particDesc></profileDesc></teiHeader><text><front><div1 type="front text"><p rend="bold italic">Contents</p></div1><div1 type="toc" n="3"><p/></div1><div1 type="toc" n="1"><head rend="bold italic">Figures Table</head><p/></div1></front><body><div1 type="abstract"><head rend="bold italic">Abstract</head><p n="p1.300"><s n="s1.5;p1.300">Nowadays, a lot of scientific research domains (such as medicine, cancer researches, aerospace, etc...) use some applications solving several complex mathematical algorithms. </s><s n="s2.5;p1.300">Then, these applications need a lot of computational resources to be executed. </s><s n="s3.5;p1.300">The research in computing developed some solutions supplying a lot of computational resources such as distributed computer (cluster), parallel execution, etc... </s><s n="s4.5;p1.300">Hence, these solutions can be used to execute complex scientific applications which need a lot of resources. </s><s n="s5.5;p1.300">These applications would then be executed in a parallel way. </s></p><p n="p2.300"><s n="s1.3;p2.300">The main purpose of this project has been to develop a "school scientific" application using the <name type="university"/> distributed computer called Beowulf. </s><s n="s2.3;p2.300">This application, developed in C using MPI, computes the matrices multiplication using two ways (sequential one and parallel one) and solves a system of linear equations (as well as the inverse of a matrix) using the iterative Jacobi method. </s><s n="s3.3;p2.300">This application enables us to understand why and in which conditions a parallel algorithm is often better than a sequential one, explaining these two ways and calculating the efficiency of the parallel solution. </s></p><p n="p3.300"><s n="s1.2;p3.300">Hence, this project involves a part of researches dealing with computing technologies (such as distributed computing, parallel execution, etc...) as well as mathematical concepts using matrices and system of linear equations. </s><s n="s2.2;p3.300">Another main part of this report deals with the mathematical application which has been created explaining how such mathematical algorithms have been developed in C using MPI. </s></p><quote lang="French"><hi rend="bold italic">Résumé</hi> De nos jours, la recherché dans beaucoup de domaines scientifique (tel que la médicine, la recherche contre le cancer, l'aérospatial, etc...) utilisent des applications qui résolvent des algorithmes mathématiques complexes et qui nécessitent énormément de ressources de calcul. La recherche en Informatique a permis de mettre au point des solutions fournissant de telles ressources, tels que les grappes de PC (cluster), les exécutions parallèles, etc... Ainsi, ces solutions peuvent être utilisées afin de permettre l'exécution d'applications scientifiques complexes qui demandent énormément de ressources de calcul. Ces applications peuvent ainsi être exécutées de manière parallèle. Le principal objectif de ce projet a été de développer une application scientifique "école" utilisant la grappe de PC, nommée Beowulf, appartenant à l'<name type="university"/>. Cette application, qui a été développé en C utilisant la librairie MPI, a pour but de calculer des produits de matrices utilisant deux méthodes (une méthode séquentielle et une autre parallèle) et de résoudre des systèmes d'équations linéaires en utilisant la méthode itérative de Jacobi. Grâce à cette application, nous pourrons comprendre pourquoi et dans quels cas un algorithme parallèle est préfér éà un algorithme séquentiel, ceci en expliquant les deux méthodes et en calculant l'efficacité de la solution parallèle. Ce projet a impliqué une importante phase de recherche concernant les technologies Informatiques utilisées pour exécuter des applications parallèles (tel que des grappes de PC (cluster), les exécutions parallèles, etc...) ainsi que certains concepts mathématiques concernant les matrices et les systèmes d'équations linéaires. Une autre partie importante de ce rapport explique comment l'application mathématique développée au sein de ce projet a été créée en langage C utilisant la librairie MPI.</quote></div1><div1 type="front-back-matter"><head rend="bold italic">Thanks to</head><p>
I first would like to thank <name type="tutor name"/>, my project supervisor, for helping and advising me so much about this project. 
</p><p>
I would also like to thank <name type="university"/> and <name type="university"/> for allowing me to study one year in England as an exchange student. 
By this way, I thank <name type="other"/>, director of international students, for helping when I arrived and advising with all the administrative tasks. 
</p><p>
I would also like to thank the <name type="other"/> administrators for managing it and for allowing me to access to this cluster. 
Without this access, this project would have never been done. 
</p></div1><div1 type="section"><head rend="bold italic">Introduction:</head><div2><head rend="bold italic">Project presentation:</head><p n="p4.300"><s n="s1.8;p4.300">Nowadays a lot of scientific domains (especially medicine, aerospace, fluid mechanic (e.g.: CFD), physics, chemistry, etc...) need more and more computational power. </s><s n="s2.8;p4.300">Indeed some scientific calculations need a long time to be executed, even on a powerful computer (sometimes several weeks or months). </s><s n="s3.8;p4.300">For example, such applications could use a lot of complicated formulas using a floating point which need a high precision or some genetic algorithms are used which need a long time to go throughout the generations, etc... </s><s n="s4.8;p4.300">That is why several research groups throughout the world are developing some solutions. </s><s n="s5.8;p4.300">These solutions are getting more and more efficient and allow the scientific calculation to be executed quickly. </s><s n="s6.8;p4.300">In this project, we will present and use some technologies (e.g.: MPI, Cluster, distributed computing, ...) developed for the purpose of providing a lot of calculation power for scientific application. </s><s n="s7.8;p4.300">Through some typical mathematical applications such as multiplication of matrices (with large matrices) and solutions to systems of linear equations (using the iterative Jacobi method, solving the inverse of a matrix), we will explain how these technologies work and how we can use them. </s><s n="s8.8;p4.300">Hence, this project involves the development of a mathematical application using distributed computing. </s></p></div2><div2><head rend="bold italic">Why did I choose this project? </head><p n="p5.300"><s n="s1.3;p5.300">The first time I heard about distributed computing used to solve very complex applications executing on several processors, I was very interested and intrigued and I wanted to know more about this subject. </s><s n="s2.3;p5.300">Through this project, I decided then to discover "this world" of distributed computing. </s><s n="s3.3;p5.300">Furthermore, Brookes University has its own cluster named Beowulf, so it was a great opportunity to be able to test concretely such applications on a real multi-computers system. </s></p><p n="p6.300"><s n="s1.2;p6.300">For me, this project involves a lot of new concepts and technologies which I knew a little bit before, but I had never used them practically. </s><s n="s2.2;p6.300">I really wanted to know more about it, and I had the opportunity to use it concretely thanks to the Brookes University cluster. </s></p></div2><div2><head rend="bold italic">Body of this report:</head><p n="p7.300"><s n="s1.2;p7.300">This project involved a long part of theory, because some computing and mathematic concepts needed to be understood before creating an application using these concepts. </s><s n="s2.2;p7.300">That is why this report contains a long part dealing with some theoretical concepts and another main part dealing with the produced application. </s></p><p n="p8.300"><s n="s1.3;p8.300">First of all, we will introduce in details the technologies that we will use to develop a mathematical application and we will then see the reasons why a parallel solution is often better than a sequential one. </s><s n="s2.3;p8.300">After that we will present the mathematical issues and explain how it is possible to compute them in a sequential way and in a parallel way via some algorithms given in a pseudo-code. </s><s n="s3.3;p8.300">Eventually, we will explain in details the produced solution developed through this project, which solves the mathematical issues explained before. </s></p></div2></div1><div1 type="section"><head rend="bold italic">Technologies involved by a parallel and distributed application:</head><quote lang="English"><hi rend="italic">"From the highest to the humblest tasks, all are of equal honour; all have their part to play"</hi>, Winston Churchill<ref target="BAWE_3052f-ftnote.001"/></quote><note place="foot" id="BAWE_3052f-ftnote.001"><p n="pn1.1"><s n="s1.1;pn1.1">From [Bib B] </s></p></note><p n="p9.300"><s n="s1.4;p9.300">In this part, we will explain in a first time what is exactly a parallel execution, the reasons why we prefer to use it rather than a sequential execution (in some case). </s><s n="s2.4;p9.300">Then we will introduce the "hardware" environment in which such parallel applications can be used. </s><s n="s3.4;p9.300">Afterwards we will explain how it is possible to create, run and manage such applications in this environment. </s><s n="s4.4;p9.300">After that, we will be able to explain which technological solution has been used in this project. </s></p><div2><head rend="bold italic">Parallel application (parallelism):</head><div3><head rend="italic">Generalities:</head><p n="p10.300"><s n="s1.5;p10.300">A sequential execution executes several instructions one after another. </s><s n="s2.5;p10.300">As it is called "sequential", it is simply a sequence of some instructions. </s><s n="s3.5;p10.300">However, a lot of scientific applications need a lot of calculation time, that means they need a lot of calculation resources (basically CPU). </s><s n="s4.5;p10.300">Hence, in order to satisfy these resources requests, the parallelism notion is introduced to enable such application to be executed faster. </s><s n="s5.5;p10.300">In the general case, the parallelism is the division in several parts (partially or totally independent) of the program. </s></p><p n="p11.300"><s n="s1.2;p11.300">There are two different kind of parallelism: the intern parallelism and the extern one. </s><s n="s2.2;p11.300">In both, the initial program is cut in several parts, </s></p><p n="p12.300"><s n="s1.4;p12.300"><hi rend="italic">The intern parallelism:</hi> this kind of parallelism is typically the one which is used on a monoprocessor machine. </s><s n="s2.4;p12.300">Actually every parts of the program would be executed on a dedicated process or thread. </s><s n="s3.4;p12.300">Then the operating system will schedule the execution time of each part. </s><s n="s4.4;p12.300">So, when a part is running, then the others are waiting for the end of this one, and every process will be executed one after another, until every process gets fully executed. </s></p><p n="p13.300"><s n="s1.4;p13.300"><hi rend="italic">The extern parallelism:</hi> this kind of parallelism is more efficient than the previous one. </s><s n="s2.4;p13.300">Indeed, this parallelism is performed either on a multiprocessor computer (typically a supercomputer<ref target="BAWE_3052f-ftnote.002"/>) or on a cluster<ref target="BAWE_3052f-ftnote.003"/> (a distributed system, for example Beowulf) or even on a grid computing<ref target="BAWE_3052f-ftnote.004"/>. </s><s n="s3.4;p13.300">Each part of the parallel program will be executed on a dedicated processor. </s><s n="s4.4;p13.300">Hence, each part will have its own processor and so will be enabled to use the whole power of this processor<ref target="BAWE_3052f-ftnote.005"/>. </s></p><note place="foot" id="BAWE_3052f-ftnote.002"><p n="pn1.1"><s rend="italic" n="s1.1;pn1.1"> See section 2.3. </s></p></note><note place="foot" id="BAWE_3052f-ftnote.003"><p n="pn1.1"><s rend="italic" n="s1.1;pn1.1"> See section 2.3. </s></p></note><note place="foot" id="BAWE_3052f-ftnote.004"><p n="pn1.1"><s rend="italic" n="s1.1;pn1.1"> See section 2.3. </s></p></note><note place="foot" id="BAWE_3052f-ftnote.005"><p n="pn1.1"><s rend="italic" n="s1.1;pn1.1">Depending on the platform architecture (cluster or supercomputer), the memory will be shared between the part or not. </s></p></note><p n="p14.300"><s n="s1.5;p14.300">However, each part comes from the same program. </s><s n="s2.5;p14.300">So they will have to be synchronized when each one will be executed. </s><s n="s3.5;p14.300">In order to do that, a procedure called <hi rend="italic">Rendez-Vous</hi> has to be done. </s><s n="s4.5;p14.300">We also really have to be careful about the mutual exclusion. </s><s n="s5.5;p14.300">Indeed, if several parts want to use a global variable (which is declared in the main program), a mutual exclusion system should be used. </s></p><p n="p15.300"><s n="s1.1;p15.300">The schema below shows that how a parallel execution is performed, </s></p><figure id="BAWE_3052f-fig.001"><head rend="bold italic">Figure 1.1. Execution of a parallel program in the general case</head></figure><p n="p16.300"><s n="s1.2;p16.300">As we can see on the schema (Figure 1.1.), the program is executed by only one process until the program gets separated in two different processes which are executed at the same time (parallel execution). </s><s n="s2.2;p16.300">Then, these two processes have to be synchronized. </s></p><p n="p17.300"><s n="s1.2;p17.300">So, when we want to use a parallel execution, the programmer has to find a way to parallelise a sequential algorithm. </s><s n="s2.2;p17.300">Then we will be able to calculate the performance gain by using n processors in parallel instead of only one in a sequential way. </s></p><p n="p18.300"><s n="s1.2;p18.300">In order to create and to use such parallel applications, we have to use some libraries which allow and manage the parallelism. </s><s n="s2.2;p18.300">The two most popular are MPI<ref target="BAWE_3052f-ftnote.006"/> and PVM<ref target="BAWE_3052f-ftnote.007"/>. </s></p><note place="foot" id="BAWE_3052f-ftnote.006"><p n="pn1.1"><s n="s1.1;pn1.1"><hi rend="bold">M</hi>essage <hi rend="bold">P</hi>assing <hi rend="bold">I</hi>nterface </s></p></note><note place="foot" id="BAWE_3052f-ftnote.007"><p n="pn1.1"><s n="s1.1;pn1.1"><hi rend="bold">P</hi>arallel <hi rend="bold">V</hi>irtual <hi rend="bold">M</hi>achine </s></p></note></div3><div3><head rend="italic">Parallel Virtual Machine (PVM):</head><p n="p19.300"><s n="s1.4;p19.300">PVM is a tool which managing the parallelism on a distributed platform (cluster). </s><s n="s2.4;p19.300">This tool works as a daemon. </s><s n="s3.4;p19.300">Indeed, to execute a parallel program which has been developed using some PVM methods, a PVM daemon has to be running on every nodes of the cluster. </s><s n="s4.4;p19.300">To start this daemon on the different available nodes, the following command is used, </s></p><p n="p20.300"><s n="s1.1;p20.300"><formula notation="" id="BAWE_3052f-form.001"/> </s></p><p n="p21.300"><s n="s1.1;p21.300">Where <hi rend="italic">hostfile</hi> is a simple text file which contains the name of all the nodes needed and capable of be used </s></p><p n="p22.300"><s n="s1.1;p22.300">An example of a <hi rend="italic">hostfile</hi> could be as follows, </s></p><p n="p23.300"><s n="s1.1;p23.300"><formula notation="" id="BAWE_3052f-form.002"/> </s></p><p n="p24.300"><s n="s1.1;p24.300">Then the daemon PVM (pvmd) would be launched on all these machines (except if one of them is down) in the background (daemon). </s></p><p n="p25.300"><s n="s1.1;p25.300">It is possible to have an access to the PVM console in order to administer PVM using several commands which show the current machines running the pvm daemon, allow adding some machines in the virtual machine, showing the different jobs which are running, etc... </s></p><p n="p26.300"><s n="s1.1;p26.300">To launch the PVM console, the following command can be used, </s></p><p n="p27.300"><s n="s1.1;p27.300"><formula notation="" id="BAWE_3052f-form.003"/> </s></p><p n="p28.300"><s n="s1.3;p28.300">Above is a short description of PVM, but this tool won't be used in this project. </s><s n="s2.3;p28.300">It has been presented because of its popularity in the "distributed computing world". </s><s n="s3.3;p28.300">Actually another tool called MPI had been used in this project. </s></p></div3><div3><head rend="italic">Message Passing Interface (MPI):</head><p n="p29.300"><s n="s1.5;p29.300">MPI is a set of rules which define the way to send some messages between each process executing a parallel program. </s><s n="s2.5;p29.300">This library is known to be portable; hence it can be used on almost every kind of distributed system. </s><s n="s3.5;p29.300">However in order to use MPI routines, a MPI implementation (for example LAM/MPI, OpenMP, ...) has to be installed on the cluster. </s><s n="s4.5;p29.300">These implementations are free for almost all of them. </s><s n="s5.5;p29.300">In this project, MPI will be used to perform the parallelism, using the implementation LAM-MPI<ref target="BAWE_3052f-ftnote.008"/> which is already installed on the Brookes University Beowulf cluster. </s></p><note place="foot" id="BAWE_3052f-ftnote.008"><p n="pn1.1"><s n="s1.1;pn1.1"><hi rend="bold">L</hi>ocal <hi rend="bold">A</hi>rea <hi rend="bold">M</hi>ulticomputer </s></p></note><p n="p30.300"><s n="s1.2;p30.300">Before being able to launch a parallel program using MPI on the Beowulf cluster, the tool LAM-MPI has to be started on every node available which will be used. </s><s n="s2.2;p30.300">The following command line manages it, </s></p><p n="p31.300"><s n="s1.1;p31.300"><formula notation="" id="BAWE_3052f-form.004"/> </s></p><p n="p32.300"><s n="s1.1;p32.300">Where <hi rend="italic">'-v'</hi> means 'verbose' to show the result of this command and <hi rend="italic">hostfile</hi> is a text file which contains the name of every nodes available (similar file as the <hi rend="italic">hostfile</hi> in the section 1.1.1. used by PVM). </s></p><p n="p33.300"><s n="s1.1;p33.300">It is possible to stop LAM by the following command, </s></p><p n="p34.300"><s n="s1.1;p34.300"><formula notation="" id="BAWE_3052f-form.005"/> </s></p><p n="p35.300"><s n="s1.2;p35.300">LAM deals with the messages sent by the processes to each others. </s><s n="s2.2;p35.300">It also manages the parallelism. </s></p><p n="p36.300"><s n="s1.1;p36.300">To compile a program using MPI, the compiler called <hi rend="italic">mpicc</hi> is used, </s></p><p n="p37.300"><s n="s1.1;p37.300"><formula notation="" id="BAWE_3052f-form.006"/> </s></p><p n="p38.300"><s n="s1.2;p38.300">When LAM is running, we can launch a parallel program which has been, of course, developed using MPI methods. </s><s n="s2.2;p38.300">To execute a parallel program, the following command is used, </s></p><p n="p39.300"><s n="s1.1;p39.300"><formula notation="" id="BAWE_3052f-form.007"/> </s></p><p n="p40.300"><s n="s1.2;p40.300">Where '<hi rend="italic">-np'</hi> means "number of processors", <hi rend="italic">n</hi> is the number of processors needed and <hi rend="italic">Application</hi> is the parallel program which we want to execute. </s><s n="s2.2;p40.300">Then, the parallel application is executed and the standard output/input are used to print and read data. </s></p></div3></div2><div2><head rend="bold italic">Performance analysis:</head><div3><head rend="italic">Speedup:</head><p n="p41.300"><s n="s1.2;p41.300">In general, as parallel algorithm is used in order to improve the execution performance. </s><s n="s2.2;p41.300">So, it is useful to calculate something which could tell us the benefit we gain to use such an algorithm instead of a sequential one. </s></p><p n="p42.300"><s n="s1.2;p42.300">The speedup is a number which determines how much the parallel algorithm is better (faster) than the sequential one. </s><s n="s2.2;p42.300">The speedup is determined as follows, </s></p><p n="p43.300"><s n="s1.1;p43.300"><formula notation="" id="BAWE_3052f-form.008"/> </s></p><p n="p44.300"><s n="s1.1;p44.300">Where <formula notation="" id="BAWE_3052f-form.009"/> is the speedup for p processors, <formula notation="" id="BAWE_3052f-form.010"/> is the execution time of the sequential algorithm (so, on only 1 processor) and <formula notation="" id="BAWE_3052f-form.011"/> is the execution time of the parallel algorithm on <hi rend="italic">p</hi> processors. </s></p><p n="p45.300"><s n="s1.2;p45.300">As it is natural to think, <formula notation="" id="BAWE_3052f-form.012"/> would be always greater than 1. </s><s n="s2.2;p45.300">Otherwise, that would mean the parallel algorithm is less fast than the sequential one, and if this is the case, it is really not recommended to use a parallel algorithm. </s></p><p n="p46.300"><s n="s1.2;p46.300">In the opposite way, we can see that the ideal value which can be obtained is <hi rend="italic">p</hi>. </s><s n="s2.2;p46.300">In this case, we have a <hi rend="italic">linear speedup</hi>. </s></p><div4><head rend="italic">Amdahl's law:</head><p n="p47.300"><s n="s1.2;p47.300">It is important that to know the speedup can be determined by a law stated by Amdahl<ref target="BAWE_3052f-ftnote.009"/>. </s><s n="s2.2;p47.300">However, this law won't be used for this project but it is important to speak about it. </s></p><note place="foot" id="BAWE_3052f-ftnote.009"><p n="pn1.1"><s n="s1.1;pn1.1">Gene Myron Amdahl was a computer architect during the 50's </s></p></note><p n="p48.300"><s n="s1.6;p48.300">This law is stated on the thought which says that the speedup has to be determined by the algorithm and not only by the number of processors. </s><s n="s2.6;p48.300">Basically, a sequential algorithm can be improved by parallelising the code. </s><s n="s3.6;p48.300">However, it is possible not to be able to parallelise the algorithm anymore. </s><s n="s4.6;p48.300">So the speedup has to be determined knowing this point. </s><s n="s5.6;p48.300">Hence, only a defined part of the algorithm will be parallelised, and the other part will stayed sequential. </s><s n="s6.6;p48.300">For example, only 40% of an algorithm might be parallelisable. </s></p><p n="p49.300"><s n="s1.1;p49.300">This law is stated as follows, </s></p><p n="p50.300"><s n="s1.1;p50.300"><formula notation="" id="BAWE_3052f-form.013"/> </s></p><p n="p51.300"><s n="s1.1;p51.300">Where <formula notation="" id="BAWE_3052f-form.014"/> is the speedup for p processors and f is the percentage of the parallelisable part of the algorithm. </s></p><p n="p52.300"><s n="s1.2;p52.300">In this project, we won't use this law. </s><s n="s2.2;p52.300">Indeed, the speedup will be calculated experimentally. </s></p></div4></div3><div3><head rend="italic">Efficiency:</head><p n="p53.300"><s n="s1.2;p53.300">The efficiency is simply a number (in general between 0 and 1) which determines how well the processors are used when they are executing the parallel algorithm. </s><s n="s2.2;p53.300">The value is determined as follows, </s></p><p n="p54.300"><s n="s1.1;p54.300"><formula notation="" id="BAWE_3052f-form.015"/> </s></p><p n="p55.300"><s n="s1.2;p55.300">Where <formula notation="" id="BAWE_3052f-form.016"/> is the speedup for p processors, P is the number of processors and <formula notation="" id="BAWE_3052f-form.017"/> is the efficiency for <hi rend="italic">p</hi> processors. </s><s n="s2.2;p55.300">If the speedup is linear, then the efficiency is equal to 1. </s></p><p n="p56.300"><s n="s1.1;p56.300">These values are very useful to determine whether or not the parallel algorithm is more efficient than the sequential one. </s></p></div3><div3><head rend="italic">Parallel or sequential algorithm? </head><p n="p57.300"><s n="s1.6;p57.300">For a given problem, the best way to compute it has to be chosen. </s><s n="s2.6;p57.300">In fact, in some case the sequential algorithm is more efficient than the parallel one. </s><s n="s3.6;p57.300">It is natural to wonder why. </s><s n="s4.6;p57.300">Indeed, if we have n processors, we naturally think that the algorithm will be executed faster than with only 1 processor. </s><s n="s5.6;p57.300">But it's not necessary true. </s><s n="s6.6;p57.300">Actually, some external factors modify the thought which are the communication between the processes using a local area network (so, some physics factor could modify the data transfer time), the synchronisation between the processes etc... </s></p><p n="p58.300"><s n="s1.1;p58.300">Hence, it is important to calculate the speedup and the efficiency in order to know whether a parallel solution is efficient or not, for a given problem. </s></p></div3></div2><div2><head rend="bold italic">Different environment available to execute a parallel application:</head><p n="p59.300"><s n="s1.5;p59.300">As we said before, a mathematical application needs a lot of calculation resources. </s><s n="s2.5;p59.300">So, such applications are often parallel and each part of the program is executed on a different processor. </s><s n="s3.5;p59.300">So, we need a platform which contains several processors to run this kind of application. </s><s n="s4.5;p59.300">Actually several platform could be used. </s><s n="s5.5;p59.300">Two kinds of environments exist: a multiprocessor supercomputer (multiprocessors computer) and a cluster<ref target="BAWE_3052f-ftnote.010"/>. </s></p><note place="foot" id="BAWE_3052f-ftnote.010"><p n="pn1.1"><s n="s1.1;pn1.1">A list of the 500 most powerful supercomputers in the world is available on the website <seg type="URL" n="www.top500.org"/> </s></p></note><div3><head rend="italic">Multiprocessor supercomputer:</head><p n="p60.300"><s n="s1.5;p60.300">A super-computer is a single computer which contains several processors (multiprocessors). </s><s n="s2.5;p60.300">That means each processors has its own cache memory (of course, because the cache memory is include in the processors) but the RAM<ref target="BAWE_3052f-ftnote.011"/> is shared among each processors. </s><s n="s3.5;p60.300">It could be a drawback because some access right has to be managed. </s><s n="s4.5;p60.300">Furthermore such a computer needs to have a large capacity of RAM, so it gets very expensive. </s><s n="s5.5;p60.300">A famous example of a multiprocessor supercomputer is the Earth Simulator located in Japan. </s></p><note place="foot" id="BAWE_3052f-ftnote.011"><p n="pn1.1"><s n="s1.1;pn1.1"><hi rend="bold">R</hi>andom <hi rend="bold">A</hi>ccess <hi rend="bold">M</hi>emory </s></p></note><p n="p61.300"><s n="s1.1;p61.300">Below is a picture (Figure 1.2.) of this supercomputer, </s></p><figure id="BAWE_3052f-pic.001"><head rend="bold italic">Figure 1.2. The Earth Simulator located in Japan</head></figure></div3><div3><head rend="italic">Cluster:</head><p n="p62.300"><s n="s1.9;p62.300">A cluster is another kind of supercomputer. </s><s n="s2.9;p62.300">Actually this is a set of personal computer not necessary very powerful. </s><s n="s3.9;p62.300">All these computers are bound via a very high speed network (a LAN<ref target="BAWE_3052f-ftnote.012"/>) in order to manage the communication between all of them. </s><s n="s4.9;p62.300">So, the power of a cluster comes from the union of these computers. </s><s n="s5.9;p62.300">Each computer in the cluster is called "a node". </s><s n="s6.9;p62.300">As each node is a single computer, each processor has its own memory. </s><s n="s7.9;p62.300">So the RAM is not shared like in a multiprocessor supercomputer. </s><s n="s8.9;p62.300">Hence, it is consequently a cheaper solution. </s><s n="s9.9;p62.300">Each node being a single personal computer not necessary expensive, this solution can be created for a smaller cost than the multiprocessor solution. </s></p><note place="foot" id="BAWE_3052f-ftnote.012"><p n="pn1.1"><s n="s1.1;pn1.1"><hi rend="bold">L</hi>ocal <hi rend="bold">A</hi>rea <hi rend="bold">N</hi>etwork </s></p></note><p n="p63.300"><s n="s1.6;p63.300">Among the nodes is a root node which allows the users to get connected on the cluster. </s><s n="s2.6;p63.300">Furthermore this root node allows an administrator to configure the cluster and is also a data server. </s><s n="s3.6;p63.300">The root node is linked to an extern network (usually Internet) to enable the users to get connected via an extern connection. </s><s n="s4.6;p63.300">Brookes University has its own cluster called <hi rend="italic">Beowulf</hi> <ref target="BAWE_3052f-ftnote.013"/>. </s><s n="s5.6;p63.300">Each node is called <hi rend="italic">Beowulfn</hi> where <hi rend="bold italic">n</hi> is the number of the node (0≤n&lt;9). </s><s n="s6.6;p63.300">The root node is Beowulf0. </s></p><note place="foot" id="BAWE_3052f-ftnote.013"><p n="pn1.1"><s n="s1.1;pn1.1">The name Beowulf comes from the famous hero of the epic poems </s></p></note><p n="p64.300"><s n="s1.1;p64.300">A schema (Figure 1.3.) explains how the general architecture of a cluster is, using the Brookes University Beowulf as an example, </s></p><figure id="BAWE_3052f-fig.002"><head rend="bold italic">Figure 1.3. General architecture of a cluster (example: Brookes Beowulf)</head></figure><p n="p65.300"><s n="s1.1;p65.300">This picture (Figure 1.4.) is the Beowulf cluster at Brookes University, </s></p><figure id="BAWE_3052f-pic.002"><head rend="bold italic">Figure 1.4. The Beowulf cluster at Brookes University</head></figure><p n="p66.300"><s n="s1.4;p66.300">On of the main advantage of a cluster is the transparency for the users. </s><s n="s2.4;p66.300">That means when a user uses the cluster, he doesn't need to know how is structured the cluster, which node is currently running or not etc... </s><s n="s3.4;p66.300">Through a remote connection (via a Unix console), the cluster seems to be only one single computer for the user. </s><s n="s4.4;p66.300">Then, when the user wants to run a parallel application, he just has to launch it without thinking about the parallelism, the application will be executed on the different node and the result would be available for the user as if it was execute only on one computer. </s></p><p n="p67.300"><s n="s1.2;p67.300">Another kind of environment with another scale dealing with grid computing can also execute parallel applications. </s><s n="s2.2;p67.300">To have more details about this environment, see <hi rend="italic">appendix 1</hi>. </s></p></div3></div2></div1><div1 type="section"><head rend="bold italic">From the mathematical issues to the algorithmic solutions:</head><p n="p68.300"><s n="s1.5;p68.300">As this project involves some mathematical concept, we have to introduce them before explaining how compute them. </s><s n="s2.5;p68.300">So, in this part the main mathematic concepts concerning matrices are explained in order to introduce the problem of systems of linear equations. </s><s n="s3.5;p68.300">Hence, the principles of matrices will be described as well as different methods used to solve a system of linear equations. </s><s n="s4.5;p68.300">After that, the first step to compute such mathematical problems will be shown via some sequential algorithms given in pseudo-code. </s><s n="s5.5;p68.300">These algorithms will be explained and the way to parallelise such algorithms will be described. </s></p><div2><head rend="bold italic">Prerequisite mathematical concepts:</head><div3><head rend="italic">Matrices:</head><p n="p69.300"><s n="s1.3;p69.300">The problem which we will try to compute in this project will use some matrices. </s><s n="s2.3;p69.300">Matrices are a very important and useful mathematical element in computing, especially in numerical computing. </s><s n="s3.3;p69.300">In fact, a matrix is simply an array (in general 2 dimension but can be more) of scalars. </s></p><p n="p70.300"><s n="s1.1;p70.300">Here is the general for of a matrix, </s></p><p n="p71.300"><s n="s1.1;p71.300"><formula notation="" id="BAWE_3052f-form.018"/> </s></p><p n="p72.300"><s n="s1.1;p72.300">We can also write, </s></p><p n="p73.300"><s n="s1.1;p73.300"><formula notation="" id="BAWE_3052f-form.019"/> </s></p><p n="p74.300"><s n="s1.2;p74.300">If <hi rend="italic">n=m</hi>, then the matrix is called square matrix. </s><s n="s2.2;p74.300">In the project, only square matrices have been used. </s></p></div3><div3><head rend="italic">Matrices multiplication:</head><p n="p75.300"><s n="s1.4;p75.300">A matrices addition and subtraction are very simple. </s><s n="s2.4;p75.300">If A is a matrix and B another one, A has to have the same size as B, otherwise the addition/subtraction is impossible. </s><s n="s3.4;p75.300">Then, every element of the matrix A are added (or subtracted) to every element of the matrix B which is "at the same place". </s><s n="s4.4;p75.300">Mathematically, that means, </s></p><p n="p76.300"><s n="s1.1;p76.300">Let A and B be two matrices with the size <hi rend="italic">n x n</hi> such as, </s></p><p n="p77.300"><s n="s1.1;p77.300"><formula notation="" id="BAWE_3052f-form.020"/> </s></p><p n="p78.300"><s n="s1.1;p78.300">Then, the addition of A and B is, </s></p><p n="p79.300"><s n="s1.1;p79.300"><formula notation="" id="BAWE_3052f-form.021"/> </s></p><p n="p80.300"><s n="s1.2;p80.300">As we can see, this operation is very simple. </s><s n="s2.2;p80.300">The addition is commutative. </s></p><p n="p81.300"><s n="s1.4;p81.300">The multiplication is a bit more complex but very natural. </s><s n="s2.4;p81.300">A precondition concerning the matrices dimensions has to be true to be allowed to calculate the product. </s><s n="s3.4;p81.300">To multiply the matrix A (m x n) by B (u x v), then n has to be equal to u, otherwise the product doesn't exist. </s><s n="s4.4;p81.300">The product is expressed as follows, </s></p><p n="p82.300"><s n="s1.1;p82.300">Let A be a matrix m x n and B be a matrix u x v, then the product A by B is, </s></p><p n="p83.300"><s n="s1.1;p83.300"><formula notation="" id="BAWE_3052f-form.022"/> </s></p><p n="p84.300"><s n="s1.3;p84.300">Unlike the addition, the multiplication is not commutative. </s><s n="s2.3;p84.300">That means the product <formula notation="" id="BAWE_3052f-form.023"/> is different than the product <formula notation="" id="BAWE_3052f-form.024"/>. </s><s n="s3.3;p84.300">It is even possible that <hi rend="italic">AxB</hi> exists but <hi rend="italic">BxA</hi> doesn't exist; in this case, <hi rend="italic">A</hi> and <hi rend="italic"> B</hi> are not square, otherwise A and B are two square matrices. </s></p><p n="p85.300"><s n="s1.3;p85.300">In the <hi rend="italic">appendix 2</hi>, is an illustrated example. </s><s n="s2.3;p85.300">Even if it seems very trial, this example can help a lot to understand the way to compute a parallel algorithm doing this operation. </s><s n="s3.3;p85.300">In fact, as we will see in another part, the "visual understanding" will help a lot to find a way to create a parallel algorithm. </s></p><p n="p86.300"><s n="s1.1;p86.300">In the general case, with A and B two square matrices of dimension <hi rend="italic">n</hi>, the number of simple operations <hi rend="italic">Nb</hi> needed is, </s></p><p n="p87.300"><s n="s1.1;p87.300"><formula notation="" id="BAWE_3052f-form.025"/> </s></p><p n="p88.300"><s n="s1.1;p88.300">The detail of the equation is shown (via a concrete example) in the <hi rend="italic">appendix 2</hi>). </s></p><p n="p89.300"><s n="s1.2;p89.300">This function <hi rend="italic">Nb(n)</hi> is cube, that means more the number n is great, more <hi rend="italic">Nb</hi> is great. </s><s n="s2.2;p89.300">Furthermore, the cube function increases very quickly, so the number of operations needed gets very great with a great n. </s></p><p n="p90.300"><s n="s1.1;p90.300">Below is the graph of the function (Figure 2.1.), </s></p><figure id="BAWE_3052f-fig.003"><head rend="bold italic">Figure 2.1. Number of operation needed depending on the matrices size</head></figure><p n="p91.300"><s n="s1.4;p91.300">If we use a computed sequential solution to calculate the product of two matrices, we can notice that the computer will have to do a lot of operation for <hi rend="italic">n</hi> very great. </s><s n="s2.4;p91.300">So, a sequential solution "will stay a long time in a loop operation" and so this would take a lot of computational resources. </s><s n="s3.4;p91.300">That's why a parallel solution will be done. </s><s n="s4.4;p91.300">However, more details will be given in another part of this document. </s></p></div3></div2><div2><head rend="bold italic">Systems of linear equations:</head><p n="p92.300"><s n="s1.1;p92.300">In this part, we will explain what a system of linear equations is mathematically; afterwards we will present the Jacobi method which is used to solve such systems. </s></p><div3><head rend="italic">What is a system of linear equations? </head><p n="p93.300"><s n="s1.2;p93.300">A system of linear equations is a set of linear equation containing several unknown variables. </s><s n="s2.2;p93.300">In the general case, such a system can be mathematically expressed as follows, </s></p><p n="p94.300"><s n="s1.1;p94.300"><formula notation="" id="BAWE_3052f-form.026"/> </s></p><p n="p95.300"><s n="s1.1;p95.300">As it is easy to think, we can write such a system using matrices as below, </s></p><p n="p96.300"><s n="s1.1;p96.300"><formula notation="" id="BAWE_3052f-form.027"/> </s></p><p n="p97.300"><s n="s1.1;p97.300">Where A is the matrix containing all the coefficients, B is a column matrix containing the results of each equations and eventually X is also a column matrix which contains all the unknown variables. </s></p><p n="p98.300"><s n="s1.1;p98.300">Hence, using the matrices notation, the problem of system of linear equation is to solve the following equation and then to find the unknown variable (vector X), </s></p><p n="p99.300"><s n="s1.1;p99.300"><formula notation="" id="BAWE_3052f-form.028"/> </s></p><p n="p100.300"><s n="s1.4;p100.300">When <hi rend="italic">n</hi> is not too great, it is possible to find the solution solving the system "by hand", which means without specific methods except using back substitution, but this kind of resolution gets very problematical and often need a long time. </s><s n="s2.4;p100.300">Furthermore, such resolution without formal method is very complex and not efficient to compute. </s><s n="s3.4;p100.300">However some more efficient methods have been found such as Gauss-Jordan method, the Gaussian elimination method<ref target="BAWE_3052f-ftnote.014"/>, the iterative Jacobi method etc... </s><s n="s4.4;p100.300">In the next part is explained one of them which is called the Jacobi method<ref target="BAWE_3052f-ftnote.015"/>. </s></p><note place="foot" id="BAWE_3052f-ftnote.014"><p n="pn1.1"><s n="s1.1;pn1.1">The Gaussian method is explained in Bib[D] </s></p></note><note place="foot" id="BAWE_3052f-ftnote.015"><p n="pn1.1"><s n="s1.1;pn1.1"><hi rend="italic">"The method is named after German mathematician Carl Gustav Jakob Jacobi."</hi>,adapted from the Wikipedia Encyclopedia </s></p></note></div3><div3><head rend="italic">The Jacobi method:</head><div4><head rend="italic">Jacobi method:</head><p n="p101.300"><s n="s1.3;p101.300">The Jacobi method [<hi rend="italic">Bib A</hi>] is a method solving a system of linear equation <hi rend="italic">AX=B</hi>. </s><s n="s2.3;p101.300">This method is iterative, that means the solution would be approached to a final solution after some iterations. </s><s n="s3.3;p101.300">The system will converge forward a final solution but will never reach it (actually, a determined tolerance value has to be used as a threshold to stop the iteration). </s></p><p n="p102.300"><s n="s1.1;p102.300">Hence a solution would be found after each step (<hi rend="italic">k+1</hi>) and if each solution is enough closed from the last one (that would mean the tolerance convergence is reached), then the vector is saved as the final solutions vector and the algorithm finishes. </s></p><p n="p103.300"><s n="s1.1;p103.300">Before explaining mathematically how this method works, some little definitions has to be defined, </s></p><p rend="ordered" n="p104.300"><s n="s1.2;p104.300"><hi rend="italic">- Diagonal matrix:</hi> Such a matrix has non null element only on its diagonal. </s><s n="s2.2;p104.300">So, this kind of matrices can be defined as follows, </s></p><p n="p105.300"><s n="s1.1;p105.300"><formula notation="" id="BAWE_3052f-form.029"/> </s></p><p rend="ordered" n="p106.300"><s n="s1.2;p106.300"><hi rend="italic">- Strictly Lower triangular matrix:</hi> Such a matrix has non null element only on the lower part of the matrix (under the diagonal). </s><s n="s2.2;p106.300">So, this kind of matrices can be defined as follows, </s></p><p n="p107.300"><s n="s1.1;p107.300"><formula notation="" id="BAWE_3052f-form.030"/> </s></p><p rend="ordered" n="p108.300"><s n="s1.2;p108.300"><hi rend="italic">- Upper triangular matrix:</hi> Such a matrix has non null element only on the upper part of the matrix (above the diagonal). </s><s n="s2.2;p108.300">So, this kind of matrices can be defined as follows, </s></p><p n="p109.300"><s n="s1.1;p109.300"><formula notation="" id="BAWE_3052f-form.031"/> </s></p><p n="p110.300"><s n="s1.1;p110.300">As it is natural to think, we can write M = L + D + U where M is a matrix, D its diagonal, U its upper matrix and L its lower matrix. </s></p><p n="p111.300"><s n="s1.2;p111.300">Knowing these definitions, we can define the Jacobi method. </s><s n="s2.2;p111.300">We want to solve AX=B with X the vector of unknown variables, </s></p><p n="p112.300"><s n="s1.1;p112.300"><formula notation="" id="BAWE_3052f-form.032"/> </s></p><p n="p113.300"><s n="s1.1;p113.300">Hence, we have the definition of the Jacobi method which is, </s></p><p n="p114.300"><s n="s1.1;p114.300"><formula notation="" id="BAWE_3052f-form.033"/> </s></p><p n="p115.300"><s n="s1.1;p115.300">And so, for one row (of the result matrix), </s></p><p n="p116.300"><s n="s1.1;p116.300"><formula notation="" id="BAWE_3052f-form.034"/> </s></p><p n="p117.300"><s n="s1.2;p117.300">As we can see, this method is a good one to be computed, because it is iterative. </s><s n="s2.2;p117.300">When the system converges, we will determine if the convergence is sufficient, that means, whether we are enough closed from the solution or not. </s></p><p n="p118.300"><s n="s1.2;p118.300">The initials values of <formula notation="" id="BAWE_3052f-form.035"/> for <hi rend="italic">k=0</hi> are guessed. </s><s n="s2.2;p118.300">In others words, <formula notation="" id="BAWE_3052f-form.036"/> will be initialized with the value 1.0 or 0.0. </s></p></div4><div4><head rend="italic">Condition to stop the algorithm:</head><p n="p119.300"><s n="s1.2;p119.300">The algorithm will be stopped when the last solution will be enough closed from the previous solution. </s><s n="s2.2;p119.300">That would mean we are much closed from the "true" solution because it is a convergent iteration. </s></p><p n="p120.300"><s n="s1.1;p120.300">So, we can define <formula notation="" id="BAWE_3052f-form.037"/> such as, </s></p><p n="p121.300"><s n="s1.1;p121.300"><formula notation="" id="BAWE_3052f-form.038"/> </s></p><p n="p122.300"><s n="s1.1;p122.300">When <formula notation="" id="BAWE_3052f-form.039"/> will be smaller than a given value, then the vector <formula notation="" id="BAWE_3052f-form.040"/> will be the solution vector, because we consider that it is enough closed from the final and "true" solution. </s></p></div4><div4><head rend="italic">Condition to use the Jacobi method:</head><p n="p123.300"><s n="s1.4;p123.300">The Jacobi method uses a convergence state to find the solution. </s><s n="s2.4;p123.300">Thus, the system has to be convergent. </s><s n="s3.4;p123.300">If that is not the case, then such a method cannot be used. </s><s n="s4.4;p123.300">So, the system has to follow some rules which are define below, </s></p><p n="p124.300"><s n="s1.1;p124.300">For every element of A in AX=B, </s></p><p n="p125.300"><s n="s1.1;p125.300"><formula notation="" id="BAWE_3052f-form.041"/> </s></p><p n="p126.300"><s n="s1.2;p126.300">However, the matrix might not follow this rule and the system would converge anyway though it is not always the case. </s><s n="s2.2;p126.300">In any case, if this previous rule is not followed, it is required that the following condition has to be true, </s></p><p n="p127.300"><s n="s1.1;p127.300"><formula notation="" id="BAWE_3052f-form.042"/> </s></p><p n="p128.300"><s n="s1.2;p128.300">That means every term belonging to the diagonal must be greater than every other element in the matrix. </s><s n="s2.2;p128.300">Nonetheless, if this previous condition is true, the system won't necessary converge, but if this condition is true, that means the system might converge. </s></p></div4></div3></div2><div2><head rend="bold italic">Algorithms solving such mathematical issues:</head><p n="p129.300"><s n="s1.1;p129.300">In this part, we will present different algorithms in pseudo-code solving some mathematic operations dealing with matrices and executing the Jacobi method. </s></p><div3><head rend="italic">Matrices multiplication:</head><p n="p130.300"><s n="s1.7;p130.300">The matrix multiplication has been implemented in this project in two ways, the sequential one and the parallel one. </s><s n="s2.7;p130.300">It has been implemented in order to understand how works a parallel program, how manage to run a program on a cluster etc... </s><s n="s3.7;p130.300">It is very important because some problems and difficulties have been met and it is easier to control them with a simple mathematic problem. </s><s n="s4.7;p130.300">Indeed, knowing these problems, the implementation of a more difficult problem such as a system of linear equations can be done. </s><s n="s5.7;p130.300">In fact, if we first start with a difficult mathematical problem and we also have to manage the different computing problems, it could be complicated. </s><s n="s6.7;p130.300">So, it is really easier to start with a simple mathematical problem to understand the computing difficulties. </s><s n="s7.7;p130.300">Hence, that has been done for this project, and the matrices multiplication using distributed computing is an important part of it. </s></p><div4><head rend="italic">Sequential algorithm:</head><p n="p131.300"><s n="s1.2;p131.300">The sequential algorithm dealing with matrices multiplication is very simple and straightforward. </s><s n="s2.2;p131.300">This algorithm is shown above using a pseudo-code (Figure 2.2.), </s></p><figure id="BAWE_3052f-fig.004"><head rend="bold italic">Figure 2.2. Sequential algorithm implementing matrices multiplication</head></figure><p n="p132.300"><s n="s1.1;p132.300">In this algorithm, the multiplication A by B is implementing and the result is stored in the matrix C. Obviously, the number of line of the matrix B is equal to the number of column of the matrix A, otherwise the multiplication would be impossible<ref target="BAWE_3052f-ftnote.016"/>. </s></p><note place="foot" id="BAWE_3052f-ftnote.016"><p n="pn1.1"><s n="s1.1;pn1.1">See section 3.1.1. </s></p></note><p n="p133.300"><s n="s1.3;p133.300">This algorithm is straightforward and could be easily implemented. </s><s n="s2.3;p133.300">However, as we will see in the benchmarking<ref target="BAWE_3052f-ftnote.017"/> of this algorithm, the execution time increases very quickly, exponentially. </s><s n="s3.3;p133.300">This is from a cache memory problem. </s></p><note place="foot" id="BAWE_3052f-ftnote.017"><p n="pn1.1"><s n="s1.2;pn1.1">A benchmarking has been done using this algorithm on a monoprocessor machine. </s><s n="s2.2;pn1.1">This benchmarking is explained in section 4.5. </s></p></note><p rend="ordered" n="p134.300"><s rend="italic" n="s1.1;p134.300">The cache memory problem: </s></p><p n="p135.300"><s n="s1.3;p135.300">In order to obtain one element of the result matrix (C), one line of the A matrix is read when one column of the B matrix is read. </s><s n="s2.3;p135.300">Hence, to obtain one line of the C matrix, only one line of A is read whereas the whole B matrix is read. </s><s n="s3.3;p135.300">The schema below<ref target="BAWE_3052f-ftnote.018"/> (Figure 2.3.) shown that, </s></p><note place="foot" id="BAWE_3052f-ftnote.018"><p n="pn1.1"><s n="s1.1;pn1.1">Schema adapted from [Bib B] </s></p></note><figure id="BAWE_3052f-fig.005"><head rend="bold italic">Figure 2.3. Matrix multiplication: data needed to obtain one line of the result matrix</head></figure><p n="p136.300"><s n="s1.8;p136.300">Therefore, the matrix B is entirely read to obtain one line of the C matrix. </s><s n="s2.8;p136.300">However, when a program is running on a computer, the data needed (variable owned by the program, array, program instructions etc...) are stored in the main memory (RAM) and in the cache memory. </s><s n="s3.8;p136.300">An access to the main memory is longer than an access to the cache memory; that is why the cache memory contains the last data used. </s><s n="s4.8;p136.300">Nevertheless, the cache memory capacity is not infinite and is often very smaller than the main memory. </s><s n="s5.8;p136.300">Hence, when it is full and the program, which is running, needs a data which is not available in the cache memory, then this data is inserted in the cache instead of another one. </s><s n="s6.8;p136.300">And if the program needs this other data (which has been removed from the cache) another time, this data will have to be loaded again in the cache instead of another one. </s><s n="s7.8;p136.300">That is called a <hi rend="italic">cache miss</hi><ref target="BAWE_3052f-ftnote.019"/>. </s><s n="s8.8;p136.300">And these operations between the two kind of memory need "a lot of time". </s></p><note place="foot" id="BAWE_3052f-ftnote.019"><p n="pn1.1"><s n="s1.1;pn1.1">More informations about cache miss in Bib[B] </s></p></note><p n="p137.300"><s n="s1.5;p137.300">So, if the B matrix is to large, then it won't fit in the cache. </s><s n="s2.5;p137.300">Hence, there will be a cache miss for each new row of C calculated (because B is totally read for each line of C). </s><s n="s3.5;p137.300">And a cache miss means obviously a time loss. </s><s n="s4.5;p137.300">So, depending on the cache memory size, the execution time of the sequential algorithm increases considerably after a matrix size limit. </s><s n="s5.5;p137.300">It is important to be aware of this "problem"; it helps to understand the results obtained and then why the execution time is not linear. </s></p><p n="p138.300"><s n="s1.4;p138.300">For example, if the cache memory is 128 kilobytes. </s><s n="s2.4;p138.300">Let B a matrix of the size <hi rend="italic">n</hi> containing integer number. </s><s n="s3.4;p138.300">An integer number fills 4 bytes in the memory (in general computer architecture). </s><s n="s4.4;p138.300">Hence<ref target="BAWE_3052f-ftnote.020"/>, the matrix size can't be greater than 180 x 180, otherwise, the matrix won't fit in the cache memory and a long time will be needed because of the cache miss. </s></p><note place="foot" id="BAWE_3052f-ftnote.020"><p n="pn1.1"><s n="s1.1;pn1.1">The details of the calculation are given in the <hi rend="italic">appendix 8</hi> </s></p></note><p n="p139.300"><s n="s1.4;p139.300">To solve the cache memory problem, it is possible to use a recursive algorithm which divides the matrix B in several parts to enable it to fit in the cache memory. </s><s n="s2.4;p139.300">Then, the recursive algorithm would be faster than the basic sequential one. </s><s n="s3.4;p139.300">However, this algorithm is not detailed in this report because it is not the main purpose of the project. </s><s n="s4.4;p139.300">Bib[B] gives more details concerning this recursive algorithm. </s></p><p n="p140.300"><s n="s1.1;p140.300">Using a parallel solution to solve the matrix multiplication issue (explained in the next part) would increase the performances and reduce the cache miss (because several processors and so several cache memories will be used). </s></p></div4><div4><head rend="italic">The parallel algorithm:</head><p n="p141.300"><s n="s1.2;p141.300">To parallelise a program, we first need to see which part of the program is parallelisable. </s><s n="s2.2;p141.300">When the way to parallelise it is found, we need to check if there are some dependencies which would make the parallelism a bit more complicated. </s></p><p n="p142.300"><s n="s1.3;p142.300">To parallelise the matrices multiplication algorithm, several ways are available. </s><s n="s2.3;p142.300">Some famous methods have been invented such as the Cannon's algorithm<ref target="BAWE_3052f-ftnote.021"/>. </s><s n="s3.3;p142.300">Here is explained only the algorithm which has been developed in this project. </s></p><note place="foot" id="BAWE_3052f-ftnote.021"><p n="pn1.1"><s n="s1.1;pn1.1">See [Bib B] </s></p></note><p n="p143.300"><s n="s1.4;p143.300">In this part, we are going to computer the matrices product <hi rend="italic">A x B = C</hi>. </s><s n="s2.4;p143.300">Hence, only these letters will be used. </s><s n="s3.4;p143.300">As we saw in the previous section, the whole matrix B has to be read to obtain one line of the matrix C, when only one line of the matrix A has to be known. </s><s n="s4.4;p143.300">To share the multiplication among several processors, we can then think to partition the matrix A and distribute each part to every processor which would be enabled to compute the product of their "allocated" part knowing the whole matrix B. The schema below (figure 2.4.) explains how this distribution is theoretically managed, </s></p><figure id="BAWE_3052f-fig.006"><head rend="bold italic">Figure 2.4. Partition and distribution among several processors of the matrix A</head></figure><p n="p144.300"><s n="s1.4;p144.300">This schema shows that the matrix A is partitioned in several parts which are distributed among the processors. </s><s n="s2.4;p144.300">It is not necessary but it is better to partition the matrix in an equal number of rows. </s><s n="s3.4;p144.300">Hence, each processor would have the same number of row to compute. </s><s n="s4.4;p144.300">But that involves the number of row of the matrix has to be a multiple of the number of processor, that means, <hi rend="italic">number of row mod number of processors = 0</hi>. </s></p><p n="p145.300"><s n="s1.2;p145.300">So, each processor has a fixed number of rows from the matrix A and they also know the whole matrix B (as we saw before, this is required to compute a part of the product). </s><s n="s2.2;p145.300">Then they would be able to compute a part of the result matrix C. The range of the row computed by each processor in the matrix C is the same as the range known from the matrix A. The following representation (Figure 2.5.) shows how the computation of the product works when the matrix A is distributed, </s></p><figure id="BAWE_3052f-fig.007"><head rend="bold italic">Figure 2.5. Product computation with matrix A distributed</head></figure><p n="p146.300"><s n="s1.3;p146.300">Now, we can write the algorithm in pseudo-code which computes it<ref target="BAWE_3052f-ftnote.022"/>. </s><s n="s2.3;p146.300">Several ways can be taken to compute this algorithm. </s><s n="s3.3;p146.300">The following (figure 2.6.) is the one which has been chosen and implemented in this project. </s></p><note place="foot" id="BAWE_3052f-ftnote.022"><p n="pn1.1"><s n="s1.1;pn1.1">The algorithm is executed by EVERY process. </s></p></note><figure id="BAWE_3052f-fig.008"><head rend="bold italic">Figure 2.6. Algorithm implementing a parallel execution of a matrices product</head></figure><p n="p147.300"><s n="s1.2;p147.300">As we see, this algorithm is not so difficult theoretically, but some difficulties and problem can be met when it is implemented. </s><s n="s2.2;p147.300">The third part of this report explains the C implementation of this algorithm. </s></p></div4></div3><div3><head rend="italic">The Jacobi method:</head><p n="p148.300"><s n="s1.3;p148.300">As we explained<ref target="BAWE_3052f-ftnote.023"/>, the Jacobi method has been implemented to solve a system of linear equations. </s><s n="s2.3;p148.300">The main goal of this implementation has been to find, using the Jacobi method, the inverse of a matrix. </s><s n="s3.3;p148.300">Actually, compute the inverse of a matrix is simply solving <hi rend="italic">n</hi> systems of linear equations with <hi rend="italic">n</hi> unknown variable (in fact, only the square matrix can be inversed). </s></p><note place="foot" id="BAWE_3052f-ftnote.023"><p n="pn1.1"><s n="s1.1;pn1.1">See section 3.3.2 </s></p></note><p n="p149.300"><s n="s1.1;p149.300">So, in this part is explained the algorithm solving a system of linear equation and then the algorithm computing the inverse of a matrix. </s></p><div4><head rend="italic">Sequential algorithms:</head><p n="p150.300"><s n="s1.1;p150.300">First of all, the algorithm shown here (Figure 2.7.) solves simply a system of linear equations<ref target="BAWE_3052f-ftnote.024"/>. </s></p><note place="foot" id="BAWE_3052f-ftnote.024"><p n="pn1.1"><s n="s1.1;pn1.1">This algorithm is adapted from Bib[F] and Bib[B] </s></p></note><p n="p151.300"><s rend="italic" n="s1.1;p151.300">(This algorithm is shown on the next page.) </s></p><figure id="BAWE_3052f-fig.009"><head rend="bold italic">Figure 2.7. Sequential Jacobi method algorithm solving system of linear equations</head></figure><p n="p152.300"><s n="s1.3;p152.300">Using the previous algorithm, the equation <hi rend="italic">AX=B</hi> can be solved, with X and B two vectors. </s><s n="s2.3;p152.300">However, we want to find the inverse of a matrix. </s><s n="s3.3;p152.300">That means we want to solve <hi rend="italic">n</hi> equations containing <hi rend="italic">n x n</hi> unknown variables. </s></p><p n="p153.300"><s n="s1.1;p153.300">Indeed, solving the inverse of a matrix A of dimension <hi rend="italic">n</hi> means finding the matrix <hi rend="italic">A-1</hi> such as, </s></p><p n="p154.300"><s n="s1.1;p154.300"><formula notation="" id="BAWE_3052f-form.043"/> </s></p><p n="p155.300"><s n="s1.2;p155.300">Where <hi rend="italic">In</hi> is the identity matrix of dimension <hi rend="italic">n</hi>. </s><s n="s2.2;p155.300">An identity matrix can simply be defined as follows, </s></p><p n="p156.300"><s n="s1.1;p156.300"><formula notation="" id="BAWE_3052f-form.044"/> </s></p><p n="p157.300"><s n="s1.1;p157.300">Obviously, <hi rend="italic">A</hi> has to be a square matrix; otherwise it doesn't have an inverse. </s></p><p n="p158.300"><s n="s1.2;p158.300">So, we now want to find the inverse of a matrix using the Jacobi method. </s><s n="s2.2;p158.300">That involved we want to solve the following equation, </s></p><p n="p159.300"><s n="s1.1;p159.300"><formula notation="" id="BAWE_3052f-form.045"/> </s></p><p n="p160.300"><s n="s1.4;p160.300">Where X and B are now two bi-dimensional matrices (not a vector like in the previous algorithm) and B is equal to the identity matrix. </s><s n="s2.4;p160.300">Each column of the matrix X is actually one column of the inverse matrix and so each column of the matrix X contains <hi rend="italic">n</hi> unknown variables. </s><s n="s3.4;p160.300">There are <hi rend="italic">n</hi> column so there are <hi rend="italic">n x n</hi> unknown variables. </s><s n="s4.4;p160.300">Hence, to find the solution, we will actually solve several single systems and gather the result of all of them. </s></p><p n="p161.300"><s n="s1.2;p161.300">To understand how the inverse is found, here is a mathematical illustration. </s><s n="s2.2;p161.300">The first system which have to be solve to find the inverse matrix is, </s></p><p n="p162.300"><s n="s1.1;p162.300"><formula notation="" id="BAWE_3052f-form.046"/> </s></p><p n="p163.300"><s n="s1.1;p163.300">Where <hi rend="italic">Xi</hi> means the column <hi rend="italic">i</hi> of the matrix X. </s></p><p n="p164.300"><s n="s1.1;p164.300">The second system is then, </s></p><p n="p165.300"><s n="s1.1;p165.300"><formula notation="" id="BAWE_3052f-form.047"/> </s></p><p n="p166.300"><s n="s1.1;p166.300">And so on. </s></p><p n="p167.300"><s n="s1.1;p167.300">Hence, the algorithm solving the inverse of a matrix using the Jacobi method is simply exactly the same algorithm as the previous one but a bit adapted in order to solve several systems at the same time. </s></p><p n="p168.300"><s n="s1.1;p168.300">The algorithm in pseudo-code follows (Figure 2.8.), </s></p><figure id="BAWE_3052f-fig.010"><head rend="bold italic">Figure 2.8. Jacobi algorithm computing the inverse of a matrix</head></figure><p n="p169.300"><s n="s1.1;p169.300">As we see, this algorithm is only an extension from the previous one. </s></p></div4><div4><head rend="italic">Parallel algorithms:</head><p n="p170.300"><s n="s1.1;p170.300">We are going to see now the main goal of this project: parallelise an algorithm solving a system of linear equations, using here the Jacobi method. </s></p><p n="p171.300"><s n="s1.6;p171.300">After studying mathematically the Jacobi method, we can admit that the calculation of each row of the matrix X can be done independently. </s><s n="s2.6;p171.300">Hence, the "natural way of parallelisation" can be used. </s><s n="s3.6;p171.300">This way is to separate the rows of the matrix A and to distribute them to every process. </s><s n="s4.6;p171.300">Then, each process would have a range of the matrix A and then they would solve the matrix X in the same range. </s><s n="s5.6;p171.300">The distribution of the matrix A and B is done exactly in the same condition as the parallel matrix multiplication (see Figure 2.4.). </s><s n="s6.6;p171.300">Each process would have the same number of row, which means, in this algorithm, in order to make the computation easier, the number of row <hi rend="italic">n</hi> has to be a multiple of the number of processes in the system. </s></p><p n="p172.300"><s n="s1.5;p172.300">However, the vector <hi rend="italic">XPrev</hi> (see in above algorithm) which stores the last value of the X vector is not independent. </s><s n="s2.5;p172.300">Indeed, this vector has to be known entirely whatever the row. </s><s n="s3.5;p172.300">So, each processor has to know the whole vector <hi rend="italic">XPrev</hi>. </s><s n="s4.5;p172.300">However, this vector changes after a <hi rend="italic">k-loop</hi>. </s><s n="s5.5;p172.300">So, it has to be synchronised by each process after each execution of the <hi rend="italic">k-loop</hi>. </s></p><p n="p173.300"><s n="s1.1;p173.300">This parallel algorithm is then composed of several steps: </s></p><p rend="bulleted" n="p174.300"><s n="s1.2;p174.300">First of all, each processor receive (or read in a file) the part of the matrix A which is assigned to it. </s><s n="s2.2;p174.300">Each process has a different part of the matrix A. </s></p><p rend="bulleted" n="p175.300"><s n="s1.1;p175.300">Then, each process executes the Jacobi algorithm using its own part of the matrix A. So, the Jacobi method is executed ONLY in the range which has been assigned to this process. </s></p><p rend="bulleted" n="p176.300"><s n="s1.2;p176.300">After that, each process sends to a root process, which has been designed before the execution, the matrix X which has been modified (each process has modified a range in the matrix X). </s><s n="s2.2;p176.300">When the root process has gathered all the parts of the result matrix X, it has to decide whether the convergence "is finished" or not; calculating the absolute value if each row of the matrix <hi rend="italic">X - XPrev</hi> (see previous algorithm). </s></p><p n="p177.300"><s n="s1.2;p177.300">If the algorithm is not finished (the system doesn't converge yet) then the matrix of the result found is sent to every processes. </s><s n="s2.2;p177.300">Indeed, they will need it to compute the next step of the Jacobi method. </s></p><p n="p178.300"><s n="s1.1;p178.300">And so on until the solution is enough closed than the user expect (the system converge). </s></p><p n="p179.300"><s n="s1.4;p179.300">The following schema (Figure 2.9.) shows, using an easier way, these different steps. </s><s n="s2.4;p179.300">In this schema, there are two sides: root process side and single process side. </s><s n="s3.4;p179.300">The root process side shows only the steps done by the root process, knowing that the root process has to do also the different steps done by a single process. </s><s n="s4.4;p179.300">So, the root process is concerned by the both sides. </s></p><figure id="BAWE_3052f-fig.011"><head rend="bold italic">Figure 2.9. Steps needed by the parallel Jacobi Algorithm</head></figure><p n="p180.300"><s n="s1.1;p180.300">It is not included directly in the report because the above schema is better to understand how it works rather than an algorithm in pseudo-code. </s></p><p n="p181.300"><s n="s1.5;p181.300">We have seen the parallel algorithm solving the system <hi rend="italic">AX=B</hi>. </s><s n="s2.5;p181.300">However we can also obtain the inverse of a matrix using the Jacobi method. </s><s n="s3.5;p181.300">The algorithm doing that would be very similar than the previous one. </s><s n="s4.5;p181.300">Indeed, as we saw, the algorithm computing the inverse of a matrix is a bit similar than the one solving a simple system <hi rend="italic">AX=B</hi>, it is just an extension. </s><s n="s5.5;p181.300">Thus, we just need to modify the previous parallel algorithm to obtain the new one which computes the inverse of a matrix. </s></p><p n="p182.300"><s n="s1.3;p182.300">So, every process would have the same number of rows from the matrix A, but would compute and solve more than one system. </s><s n="s2.3;p182.300">They will solve <hi rend="italic">n</hi> systems with <hi rend="italic">n</hi> equal to the size of the matrix A and the matrix B would be equal to the identity matrix with the size <hi rend="italic">n</hi>. </s><s n="s3.3;p182.300">In such an algorithm, the matrix solution X would be a bi-dimensional matrix rather than a vector. </s></p></div4></div3></div2><div2><head rend="bold italic">Conclusion:</head><p n="p183.300"><s n="s1.2;p183.300">In this part, the mathematical theory used in this project and the different algorithms solving some mathematic problems have been explained. </s><s n="s2.2;p183.300">In the project progress, this research part has been a long part and is one of the most important because it is essential to understand all these concepts before implementing them. </s></p></div2></div1><div1 type="section"><head rend="bold italic">The implemented solution in C using MPI:</head><p n="p184.300"><s n="s1.5;p184.300">In this part, the implemented solution using C and MPI is detailed. </s><s n="s2.5;p184.300">Actually we won't explain again how these algorithms work (which is already done in the part 2) but we will explain how they have been translated and computed using the C language. </s><s n="s3.5;p184.300">In a first time, the main structure representing the matrix in the dynamic memory is explained, afterwards the "library" implemented in this project called <hi rend="italic">"Matrix Tools"</hi> is introduced. </s><s n="s4.5;p184.300">After that the way used to implement the sequential algorithms in C, introduced in the previous part in pseudo-code, solving the mathematical issues are explained. </s><s n="s5.5;p184.300">Then the main concepts of the MPI library will be explained in order to introduce the parallel algorithms implemented. </s></p><p n="p185.300"><s n="s1.3;p185.300">After that, the benchmarking will be analysed explaining how the matrices generators have been created and how the benchmark programs work. </s><s n="s2.3;p185.300">The results obtained on Beowulf will then be presented. </s><s n="s3.3;p185.300">Eventually, some details about the way to compile and execute these algorithms and benchmark programs on Beowulf will be given. </s></p><div2><head rend="bold italic">Main matrix structure:</head><p n="p186.300"><s n="s1.5;p186.300">In order to represent a matrix in the dynamic memory, a structure has been created. </s><s n="s2.5;p186.300">This structure allocates to a matrix a number of row, a number of column and a contents. </s><s n="s3.5;p186.300">This structure called <hi rend="italic">MatrixType</hi> enables all the matrices to have the same form. </s><s n="s4.5;p186.300">In C, the contents is represented by an array bi-dimensional of integer. </s><s n="s5.5;p186.300">However, two kinds of matrix structures have been created: the first enables a matrix to have only integer elements and the second one enables a matrix to have float elements (useful for the Jacobi method). </s></p><p n="p187.300"><s n="s1.1;p187.300">The schema below (Figure 3.1.) shows how this structure is implemented in C, </s></p><figure id="BAWE_3052f-fig.012"><head rend="bold italic">Figure 3.1. Matrix structure in the dynamic memory</head></figure><p n="p188.300"><s n="s1.3;p188.300">The contents can be of the type <hi rend="italic">Integer**</hi> or <hi rend="italic">Float**</hi> which simply means an array bi-dimensional, an array of array of <hi rend="italic">Integer</hi> or <hi rend="italic"> Float</hi>. </s><s n="s2.3;p188.300">This structure has been implemented in the file <hi rend="italic">structure.h</hi><ref target="BAWE_3052f-ftnote.025"/>. </s><s n="s3.3;p188.300">So when a matrix is needed in a program, we just have to include this file as an header in the program file and we are then able to declare a new matrix using this following line: "<hi rend="italic">MatrixType M;".</hi> </s></p><note place="foot" id="BAWE_3052f-ftnote.025"><p n="pn1.1"><s n="s1.1;pn1.1">The implementation of this file is given in the <hi rend="italic">appendix 7</hi> </s></p></note></div2><div2><head rend="bold italic">Matrix tools file:</head><p n="p189.300"><s n="s1.2;p189.300">In order to make easier some matrix handling and operations, a "library" has been created in this project. </s><s n="s2.2;p189.300">This library is contained in the files <hi rend="italic">MatrixTools.c</hi> and <hi rend="italic">MatrixTools.h</hi>. </s></p><p n="p190.300"><s n="s1.2;p190.300">Thanks to this library, it is easy to manipulate a matrix structured as it is explained in the above part. </s><s n="s2.2;p190.300">Indeed this library enables the creation of a new matrix, enables to print on the screen in a format way a matrix, to read a matrix in a file etc... </s></p><p n="p191.300"><s n="s1.3;p191.300">Here are explained only the main functions of this library, knowing that some others functions are available but not presented here. </s><s n="s2.3;p191.300">For more details about them, see the <hi rend="italic">appendix 3</hi>. </s><s n="s3.3;p191.300">In this <hi rend="italic">appendix 3</hi> are also available all the signature of all these functions presented below. </s></p><p rend="bulleted" n="p192.300"><s n="s1.1;p192.300"><hi rend="italic">printStructMatrixOnScreen</hi>: Print a matrix, which is structured as explained in the previous part, on the screen in a formatted way. </s></p><p rend="bulleted" n="p193.300"><s n="s1.4;p193.300"><hi rend="italic">AllocNewMatrix</hi>: This function returns a bi-dimensional array of integers (<hi rend="italic">int**</hi>). </s><s n="s2.4;p193.300">It creates an array of array of integer with a number of row/column given as a parameter. </s><s n="s3.4;p193.300">This is often used to allocate the necessary memory place for the contents of a matrix. </s><s n="s4.4;p193.300">This could be use as follows, </s></p><p n="p194.300"><s rend="italic" n="s1.1;p194.300">MatrixType M; </s></p><p n="p195.300"><s rend="italic" n="s1.1;p195.300">M.Contents = AllocNewMatrix(M.NbRow, M.NbColumn); </s></p><p rend="bulleted" n="p196.300"><s n="s1.1;p196.300"><hi rend="italic">MatrixCpy</hi>: This function copies the contents of a matrix in another one. </s></p><p rend="bulleted" n="p197.300"><s n="s1.3;p197.300"><hi rend="italic">readMatrixInFile</hi>: This function reads a matrix contained in a file and then creates a matrix structured (a matrix of type <hi rend="italic">MatrixType</hi>). </s><s n="s2.3;p197.300">A file containing a matrix has to use a formal: the first line contains two integer numbers which are the size of the matrix and the lines following it contain the matrix (one line is one row of the matrix). </s><s n="s3.3;p197.300">Below is an example, </s></p><p n="p198.300"><s n="s1.1;p198.300"><formula notation="" id="BAWE_3052f-form.048"/> </s></p><p n="p199.300"><s n="s1.1;p199.300">Of course, several matrices can be contained in the same file, this function has to be called several time and the matrices will be created. </s></p><p rend="bulleted" n="p200.300"><s n="s1.2;p200.300"><hi rend="italic">writeMatrixInFile</hi>: This function is does exactly the contrary as the previous one. </s><s n="s2.2;p200.300">It writes a matrix, using the syntax shown above, in a file from a matrix structured using the <hi rend="italic">MatrixType</hi>. </s></p><p n="p201.300"><s n="s1.2;p201.300">All these functions are also available when the matrix contains float elements. </s><s n="s2.2;p201.300">The name of the function is then the name + float: "<hi rend="italic">functionNameFloat</hi>". </s></p><p rend="bulleted" n="p202.300"><s n="s1.4;p202.300"><hi rend="italic">MatrixProductMono</hi>: This function computes a product of two matrices using a sequential algorithm. </s><s n="s2.4;p202.300">It creates a result matrix structured using the <hi rend="italic">MatrixType</hi> and returns it. </s><s n="s3.4;p202.300">It can also return only the contents of the matrix as an array of array of integers, which can be useful some times. </s><s n="s4.4;p202.300">The return type has to be indicated by a parameter. </s></p><p rend="bulleted" n="p203.300"><s n="s1.2;p203.300"><hi rend="italic">IsConvergent</hi>: This function returns whether or not a matrix is convergent. </s><s n="s2.2;p203.300">This is useful before using the Jacobi method because if the matrix is not convergent, the Jacobi method won't work<ref target="BAWE_3052f-ftnote.026"/>. </s></p><note place="foot" id="BAWE_3052f-ftnote.026"><p n="pn1.1"><s n="s1.1;pn1.1">The conditions which are to be followed by a convergent matrix are detailed in the part 2.2.2. c. </s></p></note><p rend="bulleted" n="p204.300"><s n="s1.2;p204.300"><hi rend="italic">JacobiSeq</hi>: This function is the sequential algorithm in C which computes the Jacobi method. </s><s n="s2.2;p204.300">The matrix <hi rend="italic">A, B</hi> and the tolerance <hi rend="italic">Epsy</hi> (<hi rend="italic">AX=B</hi>) are given to this function which returns a new matrix X which is the solution of the system of linear equations. </s></p><p rend="bulleted" n="p205.300"><s n="s1.3;p205.300"><hi rend="italic">JacobiSeqSolveInverse:</hi> This function computes, by a sequential way, the inverse of a matrix <hi rend="italic">A</hi> given as a parameter. </s><s n="s2.3;p205.300">A convergence tolerance <hi rend="italic">Epsy</hi> is also given as a parameters. </s><s n="s3.3;p205.300">It returns a new matrix which is the inverse of A (A <hi rend="sup">-1</hi>). </s></p><p n="p206.300"><s n="s1.3;p206.300">This library has to be included in every file which needs it. </s><s n="s2.3;p206.300">Actually, this library must be used to handle matrices. </s><s n="s3.3;p206.300">So, each program which computes the different algorithms in this project uses this library. </s></p></div2><div2><head rend="bold italic">The sequential programs:</head><div3><head rend="italic">Matrices multiplication:</head><p n="p207.300"><s n="s1.5;p207.300">This algorithm is implemented in the file <hi rend="italic">testMulti.c</hi>. </s><s n="s2.5;p207.300">The main function of this program is very simple. </s><s n="s3.5;p207.300">Actually, the main function just opens a file, reads the matrices in it (using the function <hi rend="italic">ReadMatrixInFile</hi> from the matrix tools file), and closes the file and computes the multiplication (using the function <hi rend="italic">MatrixProductMono</hi> from the matrix tools file). </s><s n="s4.5;p207.300">It get a new matrix from which is returned by the function <hi rend="italic">MatrixProductMono</hi> and shows this new matrix. </s><s n="s5.5;p207.300">As we can see all the main functions used in this program are contained in the matrix tools file. </s></p><p n="p208.300"><s n="s1.2;p208.300">We won't explain more this program here because it is easy to understand it without more explanations<ref target="BAWE_3052f-ftnote.027"/>. </s><s n="s2.2;p208.300">We could just give some details about the creation of a new matrix (which is done in the function <hi rend="italic">MatrixProductMono</hi>). </s></p><note place="foot" id="BAWE_3052f-ftnote.027"><p n="pn1.1"><s n="s1.1;pn1.1">The whole code of this program is given in the <hi rend="italic">appendix 4</hi>. </s></p></note><p n="p209.300"><s n="s1.1;p209.300">The dynamic creation of a new matrix is done as follows, </s></p><p n="p210.300"><s n="s1.1;p210.300"><formula notation="" id="BAWE_3052f-form.049"/> </s></p><p n="p211.300"><s n="s1.2;p211.300">The word <hi rend="italic">static</hi> means "this memory space is not only for this function, so do not delete it when the function is done". </s><s n="s2.2;p211.300">Hence, we can return this pointer and the main program can use it. </s></p><p n="p212.300"><s n="s1.1;p212.300">For more details, see the code of this program in <hi rend="italic">appendix 4</hi>. </s></p></div3><div3><head rend="italic">Jacobi method:</head><p n="p213.300"><s n="s1.4;p213.300">This program is also composed by two file. </s><s n="s2.4;p213.300">The first one contains the main function when the second (which is the matrix tools file) contains the functions computing the Jacobi method. </s><s n="s3.4;p213.300">So, the main function (in the file called <hi rend="italic">JacobiSeq.c</hi>) reads the matrix A and B in a file (<hi rend="italic">AX=B</hi>), then check if this matrix is convergent (otherwise the Jacobi method cannot be used) using the function <hi rend="italic">IsConvergent</hi> (in the matrix tools file), and in that case calls the function <hi rend="italic">JacobiSeq</hi> (in the matrix tools file) sending by a parameter A, B and <hi rend="italic">Epsy</hi> (tolerance). </s><s n="s4.4;p213.300">Then, <hi rend="italic">JacobiSeq</hi> would return another matrix X which is the result of the system of linear equations. </s></p><p n="p214.300"><s n="s1.1;p214.300">In the case where we want to compute the inverse of a matrix, it is exactly the same program but the function <hi rend="italic">JacobiSeqSolveInverse</hi> (only two parameters are needed, <hi rend="italic">A</hi> and <hi rend="italic">Epsy)</hi> is called instead of <hi rend="italic">JacobiSeq</hi> and the matrix returned would be the inverse of the matrix. </s></p><p n="p215.300"><s n="s1.1;p215.300">We won't explain more the sequential algorithm, however the code of this function is given in the appendix 5. </s></p></div3></div2><div2><head rend="bold italic">The parallel algorithms:</head><p n="p216.300"><s n="s1.3;p216.300">The parallel algorithms have been presented in the part 2.3. </s><s n="s2.3;p216.300">We will explain here how it is possible to implement them in C using MPI on a cluster. </s><s n="s3.3;p216.300">Hence, the main MPI concepts are introduced in a first time then the specific MPI methods used in each algorithm are explained. </s></p><div3><head rend="italic">Main MPI concepts:</head><p n="p217.300"><s n="s1.4;p217.300">As we saw, MPI enables the processes in a distributed system to communicate each others. </s><s n="s2.4;p217.300">This is done via messages sent through the network. </s><s n="s3.4;p217.300">In an implemented way, MPI has to be initialized before calling any MPI method in the program. </s><s n="s4.4;p217.300">In the same idea, MPI has to be finalized at the end. </s></p><div4><head rend="italic">MPI Initialisation/Finalisation:</head><p n="p218.300"><s n="s1.1;p218.300">First of all, the library MPI has to be included in the source file, </s></p><p n="p219.300"><s n="s1.1;p219.300"><formula notation="" id="BAWE_3052f-form.050"/> </s></p><p n="p220.300"><s n="s1.1;p220.300">This initialisation/finalisation are done as follows, </s></p><p n="p221.300"><s n="s1.1;p221.300"><formula notation="" id="BAWE_3052f-form.051"/> </s></p><p n="p222.300"><s n="s1.3;p222.300">To understand how a parallel program works, it is important to know that every process involved in the program (if 4 processors then 4 processes) executes this initialisation. </s><s n="s2.3;p222.300">When we launch a program using the command <hi rend="italic">mpirun - np 4 ./Prog</hi> (see section 2.1.3.), that means 4 processes are going to execute the program <hi rend="italic">./Prog</hi>. </s><s n="s3.3;p222.300">So, it is essential to be able to identify each of them. </s></p><p n="p223.300"><s n="s1.3;p223.300">Hence, <hi rend="italic">MyId</hi> contains the identifier of the process. </s><s n="s2.3;p223.300">This number is in the range 0..n-1 with n the number of processes. </s><s n="s3.3;p223.300"><hi rend="italic">MachineName</hi> contains the name of the node (or machine) on which the process is executed and <hi rend="italic">NbProc</hi> is the total number of processes. </s></p><p n="p224.300"><s n="s1.2;p224.300">Several groups of processes may be created. </s><s n="s2.2;p224.300">However, we will use only one group which is called MPI_COMM_WORLD. This name is useful to identify all the process in our group. </s></p></div4><div4><head rend="italic">Concept of root process:</head><p n="p225.300"><s n="s1.3;p225.300">It is sometimes helpful to define a root process. </s><s n="s2.3;p225.300">In general this root process is the number 0, but it's not compulsory. </s><s n="s3.3;p225.300">Hence, when the root process has to do something alone, which means only this process will do it, this condition has to be added, </s></p><p n="p226.300"><s n="s1.1;p226.300"><formula notation="" id="BAWE_3052f-form.052"/> </s></p><p n="p227.300"><s n="s1.1;p227.300">For example, if the parallel application is a client/server application where the server would be the root process and the clients all the others process, then we could implement such an application as follows in the main function, </s></p><p n="p228.300"><s n="s1.1;p228.300"><formula notation="" id="BAWE_3052f-form.053"/> </s></p><p n="p229.300"><s n="s1.1;p229.300">Client/server behaviour would be then obtained. </s></p><p n="p230.300"><s n="s1.2;p230.300">We will use this concept of root process to implement our mathematic parallel algorithms. </s><s n="s2.2;p230.300">Indeed, this root process will have to gather the different result from all the processes etc... </s></p></div4></div3><div3><head rend="italic">Matrices multiplication:</head><div4><head rend="italic">General:</head><p n="p231.300"><s n="s1.2;p231.300">The parallel algorithm used to compute the matrix multiplication is detailed in the part 2.3.1.b. </s><s n="s2.2;p231.300">We are just going to explain how this has been done using MPI. </s></p><p n="p232.300"><s n="s1.1;p232.300">In the algorithm<ref target="BAWE_3052f-ftnote.028"/> the following array declaration is done, </s></p><note place="foot" id="BAWE_3052f-ftnote.028"><p n="pn1.1"><s n="s1.1;pn1.1">In section 2.3.1. </s></p></note><p n="p233.300"><s n="s1.1;p233.300"><formula notation="" id="BAWE_3052f-form.054"/> </s></p><p n="p234.300"><s n="s1.1;p234.300">Using MPI, this declaration is easily implemented thanks to the variable <hi rend="italic">NbProc</hi> (initialized during the MPI initialisation). </s></p><p n="p235.300"><s n="s1.1;p235.300">Then the following function has to be done, </s></p><p n="p236.300"><s n="s1.1;p236.300"><formula notation="" id="BAWE_3052f-form.055"/> </s></p><p n="p237.300"><s n="s1.3;p237.300">It fills the matrix <hi rend="italic">MyPartOfA</hi> with elements contained in A with the suitable range (depending on the process number). </s><s n="s2.3;p237.300">This function is implemented using some variables defined during the MPI initialisation such as <hi rend="italic">NbProc</hi> and <hi rend="italic">MyId</hi>. </s><s n="s3.3;p237.300">To understand how it works, the implementation follows, </s></p><p n="p238.300"><s n="s1.1;p238.300"><formula notation="" id="BAWE_3052f-form.056"/> </s></p><p n="p239.300"><s n="s1.1;p239.300">The bold expression <hi rend="bold italic">(MyId*( RowA / NbProc))+i</hi> defines the range. </s></p><p n="p240.300"><s n="s1.4;p240.300">When each process has its own part of the matrix <hi rend="italic">A</hi>, it can compute the product of <hi rend="italic">MyPartOfA</hi> and <hi rend="italic">B</hi><ref target="BAWE_3052f-ftnote.029"/>. </s><s n="s2.4;p240.300">However, at the end, we notice that the data have to be gathered. </s><s n="s3.4;p240.300">Indeed, each process has a part of the result matrix C, but the root process must to gather all these parts in one matrix C. In this program, only the root process will know the whole result matrix, so only it has declared the matrix <hi rend="italic">C</hi>. </s><s n="s4.4;p240.300">All the others have only the matrix <hi rend="italic">MyPartOfC</hi>. </s></p><note place="foot" id="BAWE_3052f-ftnote.029"><p n="pn1.1"><s n="s1.1;pn1.1">As it is explained in the part 2.3.1.b. </s></p></note><p n="p241.300"><s n="s1.1;p241.300">In pseudo code, </s></p><p n="p242.300"><s n="s1.1;p242.300"><formula notation="" id="BAWE_3052f-form.057"/> </s></p><p n="p243.300"><s n="s1.1;p243.300">With MPI, this operation is done using the function <hi rend="italic">MPI_Gather()</hi>. </s></p></div4><div4><head rend="italic">MPI_Gather():</head><p n="p244.300"><s n="s1.3;p244.300">This function gathers together values from a group of processes. </s><s n="s2.3;p244.300">The root process (which can be any process) receives all the parts of a variable (an array/matrix) from the processes (including it) and gathers them. </s><s n="s3.3;p244.300">The schema below (Figure 3.2.) explains this process, </s></p><figure id="BAWE_3052f-fig.013"><head rend="bold italic">Figure 3.2. MPI_Gather schema</head></figure><p n="p245.300"><s n="s1.1;p245.300">The signature<ref target="BAWE_3052f-ftnote.030"/> of this method is as follows, </s></p><note place="foot" id="BAWE_3052f-ftnote.030"><p n="pn1.1"><s n="s1.1;pn1.1">More details about this function are available at Bib[C] </s></p></note><p n="p246.300"><s rend="italic" n="s1.1;p246.300"><formula notation="" id="BAWE_3052f-form.058"/> </s></p><p n="p247.300"><s n="s1.1;p247.300">Where <hi rend="italic">sendarray</hi> is the buffer containing the value sent by the process, <hi rend="italic">NbElemSent</hi> is the number of elements sent, <hi rend="italic">Type</hi> is the type of the sent elements, <hi rend="italic">rbuf</hi> is the beginning of the buffer in which the result will be stored, <hi rend="italic">root</hi> is the number of the root process (the one which will gather) and <hi rend="italic">comm</hi> is the group of the communication (MPI_COMM_WORLD in our case) </s></p><p n="p248.300"><s n="s1.1;p248.300">In our program, this is used to gather the result matrix C. Each process sends its own <hi rend="italic">PartOfC</hi> and the root process 0 receives it and gathers it in another new matrix C which contains the whole result matrix. </s></p><p n="p249.300"><s n="s1.1;p249.300">The following instruction is so used, </s></p><p n="p250.300"><s n="s1.1;p250.300"><formula notation="" id="BAWE_3052f-form.059"/> </s></p><p n="p251.300"><s n="s1.4;p251.300">Hence the parallel multiplication is implemented in C using MPI. Actually two ways, both implementing this algorithm, have been done in this project. </s><s n="s2.4;p251.300">The first one was less optimised because it didn't use the function <hi rend="italic">MPI_Gather</hi>. </s><s n="s3.4;p251.300">It could gather the value using the simple function <hi rend="italic">MPI_Send</hi> and <hi rend="italic"> MPI_Recv</hi><ref target="BAWE_3052f-ftnote.031"/> which are the default communication functions. </s><s n="s4.4;p251.300">But it is really better to use <hi rend="italic">MPI_Gather</hi> to gather the value because this function has been implemented only in this purpose, so is optimised to do that. </s></p><note place="foot" id="BAWE_3052f-ftnote.031"><p n="pn1.1"><s n="s1.1;pn1.1">See Bib[C] </s></p></note></div4></div3><div3><head rend="italic">Jacobi method:</head><div4><head rend="italic">General:</head><p n="p252.300"><s n="s1.3;p252.300">The principle of the parallel Jacobi algorithm is explained in the part 2.3.2.b. </s><s n="s2.3;p252.300">(Figure 2.9.). </s><s n="s3.3;p252.300">We are then going to explain how it has been implemented using C and MPI explaining some particular MPI methods. </s></p><p n="p253.300"><s n="s1.2;p253.300">Unlike the matrices multiplication algorithm, each processes read the whole matrix A. Indeed, it is not necessary to separate the range for each process in another matrix My <hi rend="italic">PartOfA</hi>. </s><s n="s2.2;p253.300">However, in order to compute the Jacobi method only in the range which concern it, a process has to define the Jacobi loop range as follows, </s></p><p n="p254.300"><s n="s1.1;p254.300"><formula notation="" id="BAWE_3052f-form.060"/> </s></p><p n="p255.300"><s n="s1.2;p255.300">Hence each process knows from which until which row in the matrix A it has to compute it. </s><s n="s2.2;p255.300">The loops declaration will then be similar as follows, </s></p><p n="p256.300"><s n="s1.1;p256.300"><formula notation="" id="BAWE_3052f-form.061"/> </s></p><p n="p257.300"><s n="s1.7;p257.300">The complex part of this algorithm follows. </s><s n="s2.7;p257.300">Indeed, as we saw, the Jacobi method calculates iteratively a solution. </s><s n="s3.7;p257.300">So, each process will have to calculate the solution of its own range for the k <hi rend="sup">th</hi> iteration and then, send whether the solution is enough close from the last one (convergence sufficient or not, depending on the tolerance) or another iteration is needed. </s><s n="s4.7;p257.300">Then, each process sends the result to the root process. </s><s n="s5.7;p257.300">Hence, the root process will have to gather all these results and decide whether or not a new iteration (<hi rend="italic">k+1</hi>) is needed. </s><s n="s6.7;p257.300">At the same time, each process sends the solution found for its own range to <hi rend="bold">every</hi> others process. </s><s n="s7.7;p257.300">So, two kinds of data are sent: a variable which tell whether or not the convergence is enough and a matrix XPrev <hi rend="sub">Range</hi> which contains the solution for the process range. </s></p><p n="p258.300"><s n="s1.1;p258.300">Briefly, the method looks like as follows (this code has been simplified) </s></p><p n="p259.300"><s n="s1.1;p259.300"><formula notation="" id="BAWE_3052f-form.062"/> </s></p><p n="p260.300"><s n="s1.2;p260.300">To decide whether or not each process has reached the convergence, the root process has just to check if every Stop variable is equal to 1. </s><s n="s2.2;p260.300">If it is not the case then, a new iteration is needed. </s></p><p n="p261.300"><s n="s1.2;p261.300">Has we can see in the above algorithm, each process sends the result matrix XPrev to every others process<ref target="BAWE_3052f-ftnote.032"/>. </s><s n="s2.2;p261.300">The function <hi rend="italic">MPI_Allgather()</hi> does it. </s></p><note place="foot" id="BAWE_3052f-ftnote.032"><p n="pn1.1"><s n="s1.2;pn1.1">It is sent to every others processes because they all need to know the whole matrix X. Indeed, this matrix X will become XPrevious in the next iteration (<hi rend="italic">k+1</hi>). </s><s n="s2.2;pn1.1">For more details, see section 2.3.2. </s></p></note></div4><div4><head rend="italic">MPI_Allgather():</head><p n="p262.300"><s n="s1.2;p262.300">This function gathers data from all processes and sends it to them. </s><s n="s2.2;p262.300">So, each process sends its own part of the array and each process gathers the parts from every other process. </s></p><p n="p263.300"><s n="s1.1;p263.300">The schema below (Figure 3.3.) explains this process, </s></p><figure id="BAWE_3052f-fig.014"><head rend="bold italic">Figure 3.3. MPI_Allgather schema</head></figure><p n="p264.300"><s n="s1.1;p264.300">More details concerning this function (signature, attributes etc...) are given in Bib[C]. </s></p><p n="p265.300"><s n="s1.1;p265.300">In our application, the data type used is MPI_FLOAT, because the Jacobi method works with float numbers. </s></p></div4><div4><head rend="italic">MPI_Gatherv() and MPI_Allgatherv():</head><p n="p266.300"><s n="s1.5;p266.300">The two last function we saw (<hi rend="italic">MPI_Gather</hi> and <hi rend="italic"> MPI_Allgather</hi>) work only if each process wants to gather the same number of elements. </s><s n="s2.5;p266.300">That means the number of elements has to be a multiple of the number of processes. </s><s n="s3.5;p266.300">However, the number of elements might not be a multiple of the number of processes. </s><s n="s4.5;p266.300">In this case, the function MPI_Gatherv and MPI_Allgatherv have to be used. </s><s n="s5.5;p266.300">They have exactly the same role than the previous but with a number of elements not restricted as a multiple of the number of processes. </s></p></div4></div3></div2><div2><head rend="bold italic">Benchmarking:</head><p n="p267.300"><s n="s1.2;p267.300">Some benchmarks have been done in this project in order to evaluate the parallel solutions. </s><s n="s2.2;p267.300">In this part, we present this benchmark (how it has been done) and we analyse the result. </s></p><div3><head rend="italic">Matrices generators:</head><p n="p268.300"><s n="s1.4;p268.300">In order to be able to test the previous algorithm, some matrices samples are needed. </s><s n="s2.4;p268.300">However for very large matrices, it's impossible to create it by ourselves. </s><s n="s3.4;p268.300">So two matrices generator have been created. </s><s n="s4.4;p268.300">The first one is a simple random matrices generator and the second one is a random matrices generator which produces only convergent matrices (for the Jacobi method). </s></p><div4><head rend="italic">Simple random matrices generator:</head><p n="p269.300"><s n="s1.2;p269.300">This program creates a file which contains a matrices generated randomly. </s><s n="s2.2;p269.300">To use this program, the following syntax is used, </s></p><p n="p270.300"><s n="s1.1;p270.300"><formula notation="" id="BAWE_3052f-form.063"/> </s></p><p n="p271.300"><s n="s1.3;p271.300">This command creates the file <hi rend="italic">FileName</hi> containing a matrix with 400 rows and 400 columns. </s><s n="s2.3;p271.300">The last parameter (<hi rend="italic">new</hi> or <hi rend="italic"> save</hi>) determines whether the matrix is added in the file or the file is emptied before writing the matrix. </s><s n="s3.3;p271.300">So, if we want to write two matrices in the same file, then the parameters <hi rend="italic">save</hi> has to be used. </s></p></div4><div4><head rend="italic">Random convergent matrices generator:</head><p n="p272.300"><s n="s1.2;p272.300">This program generates also a random matrix but only a convergent matrix<ref target="BAWE_3052f-ftnote.033"/>. </s><s n="s2.2;p272.300">To be sure that the matrix generated randomly will be convergent (so be able to used with the Jacobi method), on each row generated, the diagonal element (<hi rend="italic">aii</hi>) will be always greater than the sum of the others elements in the same row. </s></p><note place="foot" id="BAWE_3052f-ftnote.033"><p n="pn1.1"><s n="s1.1;pn1.1">The rules determining whether or not a matrix is convergent are defined in the section 2.2.2.c </s></p></note><p n="p273.300"><s n="s1.1;p273.300">To use this program, the syntax is as follows, </s></p><p n="p274.300"><s n="s1.1;p274.300"><formula notation="" id="BAWE_3052f-form.064"/> </s></p><p n="p275.300"><s n="s1.1;p275.300">And the parameters are the same as the previous generator. </s></p></div4></div3><div3><head rend="italic">Matrix multiplication benchmarks:</head><div4><head rend="italic"> The benchmarks programs:</head><p n="p276.300"><s n="s1.2;p276.300">A program which performs a lot of tests, using the program explained above, has been created in order to compare the sequential algorithm performance and the parallel algorithm performance. </s><s n="s2.2;p276.300">Here is explained how this benchmark program works and one execution will be presented and the results detailed. </s></p><p n="p277.300"><s n="s1.3;p277.300">Two different benchmarks programs work: one for the sequential algorithm and the other for the parallel algorithm. </s><s n="s2.3;p277.300">Both execute the mathematical program <hi rend="italic">n</hi> times, every time with a different matrix size and the execution time is tracked in a file. </s><s n="s3.3;p277.300">When the executions are finished, we just have to copy the result file (which contains all the execution time) and create some graph to analyse the performances. </s></p><p n="p278.300"><s n="s1.1;p278.300">To execute one of both benchmarks programs, the following command is used, </s></p><p n="p279.300"><s n="s1.1;p279.300"><formula notation="" id="BAWE_3052f-form.065"/> </s></p><p n="p280.300"><s n="s1.2;p280.300">This command would execute the benchmark program which will execute the multiplication matrices algorithm: <hi rend="italic">from 10 to 1000 step 10</hi> means: the benchmark will start with two matrices of size 10 x 10 (multiplication), then will execute another test with a matrix 20 x 20 (because 10 + step = 10 + 10), and so one until 1000 x 1000. </s><s n="s2.2;p280.300">So, this command would execute exactly 99 tests, tracking each time the execution time. </s></p><p n="p281.300"><s n="s1.2;p281.300">Of course, each time the benchmark program run a next test, it will create two new matrices using the matrix generator before computing the product of these matrices, and of course, the execution time of the matrix generator is not tracked because it's not the purpose of the project. </s><s n="s2.2;p281.300">Only the mathematical algorithms are timed. </s></p><p n="p282.300"><s n="s1.3;p282.300">These benchmarks programs use the system function <hi rend="italic">fork()</hi> to execute one test and to execute the matrix generator. </s><s n="s2.3;p282.300">Some shell scripts have been created to ease the benchmark execution, but they are not detailed here. </s><s n="s3.3;p282.300">They would be explained during the demonstration. </s></p></div4><div4><head rend="italic">The results:</head><p n="p283.300"><s n="s1.4;p283.300">Some different tests have been executed. </s><s n="s2.4;p283.300">Here is presented the result of one of them. </s><s n="s3.4;p283.300">This benchmark has been executed once using the sequential algorithm and another time using the parallel way. </s><s n="s4.4;p283.300">Hence, we will be able to calculate the speedup and efficiency of this parallel algorithm. </s></p><p n="p284.300"><s n="s1.3;p284.300">This benchmark executed the algorithm with matrices size from 12 to 972 with a step of 60. </s><s n="s2.3;p284.300">So, 17 tests have been done. </s><s n="s3.3;p284.300">The sequential program has been executed one 4 processors on Beowulf. </s></p><p n="p285.300"><s n="s1.1;p285.300">The execution results are in the <hi rend="italic">appendix 6</hi>. </s></p><p n="p286.300"><s n="s1.2;p286.300">We will analyse the result drawing some representative of the performance graphs. </s><s n="s2.2;p286.300">On the first graph (Figure 3.4.) are two curves: the first one is the execution time depending on the matrix size using the sequential algorithm and the second one is the execution time using the parallel algorithm on 4 processors. </s></p><figure id="BAWE_3052f-fig.015"><head rend="bold italic">Figure 3.4. Graph showing the execution time of the matrices multiplication algorithms</head></figure><p n="p287.300"><s n="s1.2;p287.300">As we can see on this graph, the parallel algorithm is faster than the sequential one, especially when the matrices are very large. </s><s n="s2.2;p287.300">Indeed, the parallel algorithm becomes very quicker when the matrices are larger than 400 x 400. </s></p><p n="p288.300"><s n="s1.6;p288.300">However, we can notice that for matrices smaller than 400 x 400, the parallel algorithm is less efficiency than the sequential one. </s><s n="s2.6;p288.300">This is because some factors (network, communication inter processes, processes synchronisation, etc...) are added in the parallel way. </s><s n="s3.6;p288.300">So, to be efficiency, the parallel algorithm needs large matrices. </s><s n="s4.6;p288.300">We can also notice that the multiprocessors curve is less linear than the other one (there is some waves). </s><s n="s5.6;p288.300">This can be explained because the network (between the nodes) is not always in the same state. </s><s n="s6.6;p288.300">That means, at one time it can be more overloaded than another time. </s></p><p n="p289.300"><s n="s1.1;p289.300">Is it very gainful to use four processors instead of only one for such an application? </s></p><p n="p290.300"><s n="s1.1;p290.300">To answer, we can draw the speedup graph (Figure 3.5.) and the efficient graph (Figure 3.6.), </s></p><figure id="BAWE_3052f-fig.016"><head rend="bold italic">Figure 3.5. Speedup graph using 4 processors on Beowulf</head></figure><figure id="BAWE_3052f-fig.017"><head rend="bold italic">Figure 3.6. Efficiency graph using 4 processors on Beowulf</head></figure><p n="p291.300"><s n="s1.3;p291.300">We can see on these graphs that the results are not bad at all. </s><s n="s2.3;p291.300">The maximum speedup is 2.6 for an efficiency of nearly 65%. </s><s n="s3.3;p291.300">Furthermore, the general aspect of these two curves is good because they grow up considerably. </s></p></div4></div3><div3><head rend="italic">Jacobi method benchmark:</head><p n="p292.300"><s n="s1.2;p292.300">The Jacobi method works using a sequential way as well as a parallel way. </s><s n="s2.2;p292.300">Some tests have been done, with several matrices sizes, using either a simple or multiple (to compute the inverse of a matrix A) system of linear equations. </s></p><p n="p293.300"><s n="s1.2;p293.300">However, such benchmarks (as the matrices multiplication) have unfortunately not been done using the Jacobi method because some difficulties and some problems concerning MPI on Beowulf came and took some times to be solved. </s><s n="s2.2;p293.300">So, no graph can be drawn but we can analyse the results found. </s></p><p n="p294.300"><s n="s1.3;p294.300">The result matrix found using the sequential way and the parallel way are exactly the same which means both works well (the result have been checked). </s><s n="s2.3;p294.300">However, the execution time using the parallel way was always longer than the one using the sequential way; at least with for all the tests which have been done (the matrices were not larger than 500 x 500). </s><s n="s3.3;p294.300">The parallel algorithm has been tested using 4, then 6 processors. </s></p><p n="p295.300"><s n="s1.2;p295.300">We could explain these results because as we saw in the algorithm, a data gather is often done. </s><s n="s2.2;p295.300">After discussing about these unsatisfactory results with my supervisor, Chris Cox, I have understood that the Jacobi method is efficient with very large matrices (larger than the one I used) and with very specifics matrices (matrices which are dense around the diagonal). </s></p><p n="p296.300"><s n="s1.3;p296.300">After this discussion, I checked again if I could obtain better result using the parallel algorithm, but I didn't manage to obtain them. </s><s n="s2.3;p296.300">I have also checked again whether the matrix result obtained with the parallel algorithm was correct in order to be sure there was no mistake in the code and I realized that the matrix result was correct. </s><s n="s3.3;p296.300">So, the algorithm works but to be efficient using several processors the matrices representing the system of linear equations has to be very large. </s></p></div3></div2></div1><div1 type="section"><head rend="bold italic">Conclusion:</head><p n="p297.300"><s n="s1.4;p297.300">We have seen in this project several technologies and concepts concerning parallel mathematical applications, distributed computing etc... </s><s n="s2.4;p297.300">A lot of these technologies are still developed and are the subject of a lot of research projects throughout the world. </s><s n="s3.4;p297.300">As we saw, distributed computing is a very large and interesting subject, but complex. </s><s n="s4.4;p297.300">That is why this project involved a lot of researches dealing with these technologies and concepts which I nearly didn't know before. </s></p><p n="p298.300"><s n="s1.4;p298.300">The most difficult part has been the understanding part. </s><s n="s2.4;p298.300">Indeed, I had to understand the concepts dealing with the computing part (distributed computing, parallel algorithm, Message Passing Interface, etc...) and the mathematical part (different properties of matrices, Jacobi method, etc...). </s><s n="s3.4;p298.300">But after this part, came the practical part which involved the development of my own mathematical applications. </s><s n="s4.4;p298.300">This part was also very interesting because I could try concretely all the concepts I had read before. </s></p><p n="p299.300"><s n="s1.6;p299.300">Work using a cluster such as the Brookes Beowulf was totally new for. </s><s n="s2.6;p299.300">It was a real great opportunity to be able to use it because we can't use such a cluster "every day" at home. </s><s n="s3.6;p299.300">Applications which have been created in this project gave some interesting results, such as the results obtained by the matrices multiplication program with a speedup not bad at all and promising. </s><s n="s4.6;p299.300">The results obtained, with the Jacobi method application, were good because we could obtain the inverse of a matrix very quickly and using either a sequential or a parallel way. </s><s n="s5.6;p299.300">However, the performances obtained with the Jacobi method application were not good in a first time because the parallel way was longer than the sequential one. </s><s n="s6.6;p299.300">But after a discussion with my supervisor, I have understood that the Jacobi parallel method needs very large matrices to be efficient. </s></p><p n="p300.300"><s n="s1.1;p300.300">To conclude, this project was very interesting and I really learnt a lot about new concepts (for me), thanks to it. </s></p></div1></body><back><div1 type="bibliography"><head rend="bold italic">Bibliography</head><p><hi rend="bold italic">[Bib A] </hi><seg type="URL" n="http://mathworld.wolfram.com/JacobiMethod.html"/> and </p><p><seg type="URL" n="http://mathworld.wolfram.com/StrassenFormulas.html"/></p><p><hi rend="bold italic">[Bib B] "Parallel programming in C with MPI and OpenMP", </hi>Micheal J. Quin</p><p><hi rend="bold italic">[Bib C]</hi> MPI Forum, web site containing the specification of all the MPI methods and a lot of others details about it: <seg type="URL" n="www.mpi-forum.org"/></p><p><hi rend="bold italic">[Bib D]</hi> OPALE project: <seg type="URL" n="www.inrialpes.fr/opale"/> which is a project managed by the National Research Institute in Computing and Robotic in France (INRIA)</p><p><hi rend="bold italic">"Interface d'une plate-forme sur grille de calcul avec le middleware Unicore", </hi>Rapport de stage/Internship report, in French, Vincent Bel</p><p><hi rend="bold italic">[Bib E]</hi> Documentation about the Gaussian method: </p><p><seg type="URL" n="http://www-groups.dcs.st-and.ac.uk/~history/HistTopics/Matrices_and_determinants.html"/></p><p><seg type="URL" n="http://www.ee.oulu.fi/~mpa/matreng/ematr1_4.htm"/></p><p><seg type="URL" n="http://math.fullerton.edu/mathews/n2003/Web/GaussianJordanMod/GaussianJordanMod.html"/></p><p><hi rend="bold italic">[Bib F] </hi><hi rend="italic">"Distributed solution to Linear Systems using a Finite Element Method (ref. Wilkinson and Allen)",</hi> Chris Cox</p><p><hi rend="bold italic">[Bib G]</hi> Web site which lists the 500 most powerful supercomputer: <seg type="URL" n="www.top500.org"/></p></div1><div1 type="appendix"><head rend="bold italic">Appendix</head><p/></div1></back></text></TEI.2>
