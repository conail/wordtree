<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE TEI.2 SYSTEM "tei_bawe.dtd"><TEI.2 id="_3094b" n="version 1.0"><teiHeader><fileDesc><titleStmt><title>Analysis Documentation, Choice 4: hash table using a Quadratic Probing Scheme Vs. AVL tree</title></titleStmt><extent/><publicationStmt><distributor>British Academic Written English (BAWE) corpus</distributor><availability><p>The British Academic Written English (BAWE) corpus was developed at the Universities of Warwick, Reading and Oxford Brookes, under the directorship of Hilary Nesi and Sheena Gardner (formerly of the Centre for Applied Linguistics [previously called CELTE], Warwick), Paul Thompson (Department of Applied Linguistics, Reading) and Paul Wickens (Westminster Institute of Education, Oxford Brookes), with funding from the ESRC. Subject to the rights of the these institutions in the BAWE corpus, and pursuant to the ESRC agreement, the BAWE corpus is available to researchers for research purposes PROVIDED THAT the following conditions are met:</p><p>1. The corpus files are not distributed in either their original form or in modified form.</p><p>2. The texts are used for research purposes only; they should not be reproduced in teaching materials.</p><p>3. The texts are not reproduced in full for a wider audience/readership, although researchers are free to quote short passages of text (up to 200 running words from any given text).</p><p>4. The BAWE corpus developers (contact: Hilary Nesi) are informed of all projects, dissertations, theses, presentations or publications arising from analysis of the corpus.</p><p>5. Researchers acknowledge their use of the corpus using the following form of words: "The data in this study come from the British Academic Written English (BAWE) corpus, which was developed at the Universities of Warwick, Reading and Oxford Brookes under the directorship of Hilary Nesi and Sheena Gardner (formerly of the Centre for Applied Linguistics [previously called CELTE], Warwick), Paul Thompson (Department of Applied Linguistics, Reading) and Paul Wickens (Westminster Institute of Education, Oxford Brookes), with funding from the ESRC (RES-000-23-0800)."</p></availability></publicationStmt><notesStmt><note resp="British Academic Written English (BAWE) corpus project">Language used in quote: <foreign id="English">English</foreign></note></notesStmt><sourceDesc><p n="level">3</p><p n="date">2005-12</p><p n="module title">Data Structures</p><p n="module code">U08020</p><p n="genre family">Design specification</p><p n="discipline">Computer Science</p><p n="disciplinary group">PS</p><p n="grade">D</p><p n="number of authors">1</p><p n="number of words">1325</p><p n="number of s-units">84</p><p n="number of p">64</p><p n="number of tables">1</p><p n="number of figures">0</p><p n="number of block quotes">1</p><p n="number of formulae">2</p><p n="number of lists">4</p><p n="number of paragraphs formatted like lists">23</p><p n="abstract present">no abstract</p><p n="average words per s-unit">15.8</p><p n="average s-units per p">1.3</p><p n="macrotype of assignment">simple assignment</p></sourceDesc></fileDesc><encodingDesc><p>TEI P4 (documented in: BAWE.documentation.pdf)</p></encodingDesc><profileDesc><particDesc><person><p n="gender">m</p><p n="year of birth">1979</p><p n="first language">Sinhala</p><p n="education">OSa</p><p n="course">BSc Computing and Information Systems</p><p n="student ID">3094</p></person></particDesc></profileDesc></teiHeader><text><front><titlePage><titlePart rend="underlined bold">Analysis Documentation</titlePart><docTitle><titlePart rend="bold">Choice 4: Hash table using a Quadratic Probing scheme Vs. AVL tree </titlePart></docTitle></titlePage></front><body><div1 type="section"><list type="simple"><head rend="bold">Database characteristics</head><item>Record size = 5 bytes + 30bytes + 6 bytes + 60 bytes = 101 bytes</item><item>Max number of records = 200</item><item>Max database size = 101 x 200 = 20200 bytes = 20kb</item></list><list type="simple"><head rend="bold">Data operations characteristics</head><item>Insertion: Occasional</item><item>Deletion: Occasional</item><item>Successful searching: Frequent</item><item>Unsuccessful searches: Occasional</item></list><list type="simple"><head rend="bold">Primary Key</head><item>Catalogue code ID: a unique ID which consists of a alphabetic character as the 1 <hi rend="sup">st</hi> character and rest of the five characters as numeric.</item></list><p n="p1.64"/></div1><div1 type="section"><head rend="underlined bold">1. Table size analysis</head><p n="p2.64"><s n="s1.1;p2.64">The maximum number of predicted records is 200 with occasional insersertions and deletions. </s></p><p n="p3.64"><s n="s1.1;p3.64">It is expected that the database size remains fairly constant. </s></p><p n="p4.64"><s n="s1.1;p4.64">Assuming 200 records is 80% of the total table size so the efficiency is not degraded, the table size is: </s></p><list type="simple"><item>80/100 x Tablesize = 200</item><item>Tablesize = 200x100/80</item><item>Tablesize = 250</item></list><p n="p5.64"><s n="s1.1;p5.64">Initially the closest prime number to 250 was selected as the table size. </s></p><p n="p6.64"><s n="s1.1;p6.64">(prime numbers near 250 are {.., 223, 227, 229, 233, 239, 241, <hi rend="bold">251,</hi> 257, 263, ..}) </s></p><p n="p7.64"><s n="s1.1;p7.64">This is makes sure the records are evenly distributed throughout the hash table so possibility of collisions is greatly reduced. </s></p><p n="p8.64"><s n="s1.1;p8.64">However there's the power of 2 close by which is 256. </s></p><p n="p9.64"><s n="s1.1;p9.64">(Powers of two near 250 are {..,128, 256, 512,..}) </s></p><quote lang="English">" Formally stated: <hi rend="italic"> h (k</hi><hi rend="italic">k</hi> mod <hi rend="italic">m</hi> Typically, we should avoid values for <hi rend="italic">m</hi> that are powers of 2. This is because if <hi rend="italic">m</hi> = 2 <hi rend="italic">p</hi>, <hi rend="italic">h</hi> becomes just the <hi rend="italic">p</hi> lowest-order bits of <hi rend="italic">k</hi>. Usually we choose <hi rend="italic">m</hi> to be a prime number not too close to a power of 2, while considering storage constraints and load factor." Source: page 144, Mastering Algorithms with C, By Kyle Loudon</quote><p n="p10.64"><s n="s1.1;p10.64">191 is the ideal prime number since it is best positioned between 128 and 256, however is it too low if load factor is to be less than 80% in the worst case scenario of 200 records in the catalogue. </s></p><p n="p11.64"><s n="s1.1;p11.64">So the table size was reduced to a prime number few positions below 256. (we can afford compromise the table size a little since we took the maximum database size of 200 to be 80%). </s></p><p n="p12.64"><s n="s1.1;p12.64">After all the adjustments the final <hi rend="bold">table size is 223</hi>. </s></p><p n="p13.64"><s n="s1.2;p13.64">Coincidently this value closely matches with the concept of "Size should be about 10% larger than the maximum number of entries". </s><s n="s2.2;p13.64">Source: lecture notes: Slide 33, U08020 Lecture Notes - week 6 </s></p><p n="p14.64"><s n="s1.2;p14.64">However it is just a coincident as the table size calculation is a bit more bias to performance issues than resources issues. </s><s n="s2.2;p14.64">The table size is specifically tuned to this specific plant online catalogue. </s></p></div1><div1 type="section"><head rend="underlined bold">2. Hash function analysis</head><p n="p15.64"><s n="s1.4;p15.64">"Division method" using the modular function is used as the hash function. </s><s n="s2.4;p15.64">This is so the hash function is made simplest as possible and minimum amount from statements so it does to steal too many CPU cycles. </s><s n="s3.4;p15.64">The even distribution of values and collision reduction of the index is dependant on choosing the right of the values themselves. </s><s n="s4.4;p15.64">Collisions are inevitable but selection of the quadratic probing and keeping the maximum load factor around 80% makes sure the collisions are kept at a minimum. </s></p><p n="p16.64"><s n="s1.1;p16.64">"Division method" source: page 145, Mastering Algorithms with C, By Kyle Loudon </s></p><p n="p17.64"><s n="s1.2;p17.64">"Modula arithmetic provides a simple and effective hash function." </s><s n="s2.2;p17.64">Basis of the division method is: </s></p><p n="p18.64"><s n="s1.1;p18.64"><formula notation="" id="BAWE_3094b-form.001"/> </s></p><p n="p19.64"><s n="s1.1;p19.64">Pseudocode of the hash new function is: </s></p><p n="p20.64"><s n="s1.1;p20.64"><formula notation="" id="BAWE_3094b-form.002"/> </s></p></div1><div1 type="section"><head rend="underlined bold">3. Efficiency of hash table operations</head><p n="p21.64"><s n="s1.1;p21.64">This particular design should be extremely efficient. </s></p><p rend="bulleted" n="p22.64"><s n="s1.1;p22.64">Pros of this design: </s></p><p rend="bulleted" n="p23.64"><s n="s1.1;p23.64">maximum predicted database size assumed to 200 records </s></p><p rend="bulleted" n="p24.64"><s n="s1.1;p24.64">table size is made 223 making it 89.69% full even if all 200 records are occupied </s></p><p rend="bulleted" n="p25.64"><s n="s1.1;p25.64">selection of a best possible optimum value to table size (with regard to prime numbers and powers of 2) which makes sure clustering and collisions are kept at a minimum) </s></p><p rend="bulleted" n="p26.64"><s n="s1.1;p26.64">usage of a simple hashing function which doesn't take too much CPU cycles </s></p><p rend="bulleted" n="p27.64"><s n="s1.1;p27.64">usage of quadratic probing which eliminates primary clustering </s></p><p rend="bulleted" n="p28.64"><s n="s1.1;p28.64">Cons of this design: </s></p><p rend="bulleted" n="p29.64"><s n="s1.1;p29.64">uses more memory than actually required to maintain efficiency. </s></p><p rend="bulleted" n="p30.64"><s n="s1.1;p30.64">usage of a complex (multiplication method) would have produce more random distribution values in the index (at the expense of CPU cycles of course) </s></p><p n="p31.64"><s rend="underlined" n="s1.1;p31.64">Efficiency of operations </s></p><p n="p32.64"><s n="s1.1;p32.64">Hash tables take the constant time close to O(1) for all operations. </s></p><p n="p33.64"><s rend="bold" n="s1.1;p33.64">Insertion: O(1) </s></p><p n="p34.64"><s rend="bold" n="s1.1;p34.64">Searching: O(1) </s></p><p n="p35.64"><s n="s1.2;p35.64">This is the theoretical minimum access time. </s><s n="s2.2;p35.64">Actual time is the (constant) time of the hashing function and the length of the probe. </s></p><p n="p36.64"><s n="s1.1;p36.64">As the probe length depends on the ratio of number of items on the table and the size of the table (i.e. load factor), during design every step has been taken to reduce performance degrading due to collisions considering the resource vs. efficiency factors (see Pros design above). </s></p><p n="p37.64"><s n="s1.2;p37.64">Unsuccessful search will take more time than a successful search since the whole index has to be probed. </s><s n="s2.2;p37.64">But this isn't a problem since unsuccessful searches are occasional. </s></p></div1><div1 type="section"><head rend="underlined bold">4. Efficiency of AVL tree operations</head><p n="p38.64"><s n="s1.2;p38.64">AVL trees have logarithmic complexity. </s><s n="s2.2;p38.64">Searching, insertion, and deletion are all O(log n) in both the average and worst cases. </s></p><p n="p39.64"><s rend="bold" n="s1.1;p39.64">Insertion: O(log n) </s></p><p n="p40.64"><s rend="bold" n="s1.1;p40.64">Searching: O(log n) </s></p><p n="p41.64"><s n="s1.1;p41.64">(where n is number of nodes in the tree) </s></p><p n="p42.64"><s n="s1.3;p42.64">The AVL tree algorithms of the operations are more complex than the hash tables. </s><s n="s2.3;p42.64">The main algorithmic complexity comes due to the rebalancing requirement of the AVL tree. </s><s n="s3.3;p42.64">This makes understanding and codeing bit difficult. </s></p></div1><div1 type="section"><head rend="underlined bold">5. Critical comparison of hash tables and AVL trees</head><p n="p43.64"><s n="s1.3;p43.64">For this online catalogue hash table should performs better of the two data structures. </s><s n="s2.3;p43.64">It is because of the way hash table was designed, hash table has better performance in this case at the expense of memory. </s><s n="s3.3;p43.64">But the table size can further be optimized by experimenting by changing the table size at run time. </s></p><p rend="bulleted" n="p44.64"><s rend="bold" n="s1.1;p44.64">Following are circumstances where hash tables are of better choice than AVL trees </s></p><p rend="bulleted" n="p45.64"><s n="s1.1;p45.64">Less frequent insertions and deletions [X] </s></p><p rend="bulleted" n="p46.64"><s n="s1.1;p46.64">Unsuccessful searches generally require more time than successful searches [X] </s></p><p rend="bulleted" n="p47.64"><s n="s1.2;p47.64">Inefficient utilization of memory. </s><s n="s2.2;p47.64">Hash tables (with quadratic probing) require additional memory. [X] </s></p><p rend="bulleted" n="p48.64"><s n="s1.2;p48.64">Number of elements to be stored must be known accurately in advance. </s><s n="s2.2;p48.64">Hash cannot be expanded once implemented. [X] </s></p><p rend="bulleted" n="p49.64"><s n="s1.2;p49.64">Rehashing into a larger table is expensive. </s><s n="s2.2;p49.64">(If the number of elements is not known in advance separate chaining can still be used) [X] </s></p><p rend="bulleted" n="p50.64"><s n="s1.1;p50.64">Simple and easy to understand data structure is needed (hash tables are very simple data structures) </s></p><p rend="bulleted" n="p51.64"><s n="s1.1;p51.64">Application need to be developed very quickly without regard any efficient memory utilisation </s></p><p rend="bulleted" n="p52.64"><s n="s1.2;p52.64">Database does not occupy more than say 80%. </s><s n="s2.2;p52.64">Performance degrades dramatically when the table becomes near full. </s></p><p rend="bulleted" n="p53.64"><s n="s1.1;p53.64">Need high efficiency (more specifically constant time random access to data elements) </s></p><p n="p54.64"><s n="s1.1;p54.64">[X] Plants online catalogue requirement match </s></p><p rend="bulleted" n="p55.64"><s rend="bold" n="s1.1;p55.64">Circumstances where AVL trees are of better choice than hash tables </s></p><p rend="bulleted" n="p56.64"><s n="s1.1;p56.64">If table traversals (i.e. inorder, preorder, postorder) are needed </s></p><p rend="bulleted" n="p57.64"><s n="s1.1;p57.64">Need access to the minimum or maximum items or finding items in a certain range. </s></p><p rend="bulleted" n="p58.64"><s n="s1.1;p58.64">Number of elements in the database changes frequently and unpredictably </s></p><p n="p59.64"><s n="s1.1;p59.64">Data structure that self maintains its efficiency without user/programmer intervention </s></p><p n="p60.64"><s n="s1.1;p60.64">If memory is of expensive/limited resource </s></p><p n="p61.64"><s n="s1.1;p61.64">Developer has good undersanding and codeing experience with AVL trees </s></p><p n="p62.64"><s n="s1.1;p62.64">Developer has good understanding of complex data structures (in which case red black trees would be used instead of AVL trees in the first place) </s></p></div1><div1 type="section"><head rend="bold">Test results summary</head><table id="BAWE_3094b-tab.001"><row><cell/></row></table><p n="p63.64"><s n="s1.3;p63.64">Unfortunately I was unable to find a match between theory and practice (through the program). </s><s n="s2.3;p63.64">The reason for the different results (steps/elements inspected), in my opinion, depends on where the element actually gets placed in the index. </s><s n="s3.3;p63.64">Since this is random I was unable to find the link (between the theory and the program) and prove my claims without committing much more time to testing and test result presentation. </s></p><p n="p64.64"><s n="s1.3;p64.64">However it should be noted that with hash tables, the practical performance (which depends on the load factor) need to be fine tuned by experimenting with the table size in the real world environment with contrast to the specific performance requirements of the application. </s><s n="s2.3;p64.64">Such data is unavailable in the assignment. </s><s n="s3.3;p64.64">So I calculated the best possible table size (in my opinion). </s></p></div1></body><back><div1 type="bibliography"><head rend="bold">References</head><p><formula notation="" id="BAWE_3094b-form.003"/></p><p rend="bold">Mastering Algorithms with C</p><p>By Kyle Loudon</p><p>First Edition August 1999 </p><p>ISBN: 1-56592-453-3</p><p rend="bold">Data Structures &amp; Algorithms in Java</p><p>by Robert Lafore </p><p>ISBN: 1571690956</p><p>U8020 Lecture Notes:</p><p> Week 5, Balanced trees</p><p> Week 6, Hash tables</p></div1></back></text></TEI.2>
