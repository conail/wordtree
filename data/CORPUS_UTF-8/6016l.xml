<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE TEI.2 SYSTEM "tei_bawe.dtd"><TEI.2 id="_6016l" n="version 1.0"><teiHeader><fileDesc><titleStmt><title>Report on data set</title></titleStmt><extent/><publicationStmt><distributor>British Academic Written English (BAWE) corpus</distributor><availability><p>The British Academic Written English (BAWE) corpus was developed at the Universities of Warwick, Reading and Oxford Brookes, under the directorship of Hilary Nesi and Sheena Gardner (formerly of the Centre for Applied Linguistics [previously called CELTE], Warwick), Paul Thompson (Department of Applied Linguistics, Reading) and Paul Wickens (Westminster Institute of Education, Oxford Brookes), with funding from the ESRC. Subject to the rights of the these institutions in the BAWE corpus, and pursuant to the ESRC agreement, the BAWE corpus is available to researchers for research purposes PROVIDED THAT the following conditions are met:</p><p>1. The corpus files are not distributed in either their original form or in modified form.</p><p>2. The texts are used for research purposes only; they should not be reproduced in teaching materials.</p><p>3. The texts are not reproduced in full for a wider audience/readership, although researchers are free to quote short passages of text (up to 200 running words from any given text).</p><p>4. The BAWE corpus developers (contact: Hilary Nesi) are informed of all projects, dissertations, theses, presentations or publications arising from analysis of the corpus.</p><p>5. Researchers acknowledge their use of the corpus using the following form of words: "The data in this study come from the British Academic Written English (BAWE) corpus, which was developed at the Universities of Warwick, Reading and Oxford Brookes under the directorship of Hilary Nesi and Sheena Gardner (formerly of the Centre for Applied Linguistics [previously called CELTE], Warwick), Paul Thompson (Department of Applied Linguistics, Reading) and Paul Wickens (Westminster Institute of Education, Oxford Brookes), with funding from the ESRC (RES-000-23-0800)."</p></availability></publicationStmt><notesStmt><note resp="British Academic Written English (BAWE) corpus project">Evaluated as candidate compound assignment. Assigned to S1b: collection of reports: compound.</note><note resp="British Academic Written English (BAWE) corpus project">Page footer contains: page numbers. 
</note></notesStmt><sourceDesc><p n="level">4</p><p n="date">2006-04</p><p n="module title">Econometrics</p><p n="module code">APME51</p><p n="genre family">Exercise</p><p n="discipline">Agriculture</p><p n="disciplinary group">LS</p><p n="grade">M</p><p n="number of authors">1</p><p n="number of words">1706</p><p n="number of s-units">86</p><p n="number of p">53</p><p n="number of tables">10</p><p n="number of figures">0</p><p n="number of block quotes">0</p><p n="number of formulae">15</p><p n="number of lists">0</p><p n="number of paragraphs formatted like lists">0</p><p n="abstract present">no abstract</p><p n="average words per s-unit">19.8</p><p n="average s-units per p">1.6</p><p n="macrotype of assignment">compound assignment consisting of 3 parts (see notesStmt for details)</p></sourceDesc></fileDesc><encodingDesc><p>TEI P4 (documented in: BAWE.documentation.pdf)</p></encodingDesc><profileDesc><particDesc><person><p n="gender">f</p><p n="year of birth">1979</p><p n="first language">Greek</p><p n="education">OSa</p><p n="course">MSc in Horticulture</p><p n="student ID">6016</p></person></particDesc></profileDesc></teiHeader><text><front/><body><div1 type="text"><head rend="bold">QUESTION 1</head><div2><head rend="bold">Introduction and methodology</head><p n="p1.53"><s n="s1.2;p1.53">We want to test the hypothesis that the regression coefficients for the first half of the sample (20 observations) are significantly different from that for the second half (20 observations). </s><s n="s2.2;p1.53">For this purpose we will conduct a Chow test, which is based on an F-test. </s></p><p n="p2.53"><s n="s1.1;p2.53">Initially, we have the regression model: </s></p><p n="p3.53"><s n="s1.1;p3.53"><formula notation="" id="BAWE_6016l-form.001"/> </s></p><p n="p4.53"><s n="s1.2;p4.53">We introduce a dummy variable (D <hi rend="sub">t</hi>), which we assume it affects both the intercept and the slope. </s><s n="s2.2;p4.53">The resulting regression model is now given by: </s></p><p n="p5.53"><s n="s1.1;p5.53"><formula notation="" id="BAWE_6016l-form.002"/> </s></p><p n="p6.53"><s n="s1.2;p6.53">The dummy variable takes the value 1 for the first half of the sample (20 observations) and the value 0 for the second half of the sample (20 observations). </s><s n="s2.2;p6.53">So, the regression functions for the dependent variable in the two half of the sample as resulting from (2) are given by: </s></p><p n="p7.53"><s n="s1.1;p7.53"><formula notation="" id="BAWE_6016l-form.003"/> </s></p><p n="p8.53"><s n="s1.1;p8.53"><formula notation="" id="BAWE_6016l-form.004"/> </s></p><p n="p9.53"><s n="s1.1;p9.53">The assumption that we have made here is that the regressions for the two half of the sample are completely different, since we have allowed the intercept and slope to differ. </s></p><p n="p10.53"><s n="s1.2;p10.53">We now want to test whether the regression coefficients for the first half of the sample (20 observations) are significantly different from that for the second half (20 observations). </s><s n="s2.2;p10.53">In other words, we test the equivalence of the two regressions and therefore we specify the null hypothesis (H <hi rend="sub">0</hi>: δ=0, γ=0), which states that the two regressions are equivalent and the alternative hypothesis (H <hi rend="sub">1</hi>: δ≠0 or γ≠0), which states that the two regressions are not equivalent. </s></p><p n="p11.53"><s n="s1.1;p11.53">To test the H <hi rend="sub">0</hi> against the H <hi rend="sub">1</hi>, we use a Chow test. </s></p><p n="p12.53"><s n="s1.1;p12.53">The restricted model is represented by equation (1) and it assumes no difference in the intercept and slope coefficients across the two half of the sample. </s></p><p n="p13.53"><s n="s1.3;p13.53">The unrestricted model is represented by equation (2) and it allows the intercept and slope coefficients to differ for the two half of the sample. </s><s n="s2.3;p13.53">By running two regressions, one for each model we find the SSE for each one, which we then input in Table 1 together with the values for J, T, K and alpha (number of restrictions, number of observations, number of coefficients and significance level respectively). </s><s n="s3.3;p13.53">By replacing these values in Table 1 we get the computed values required to reach a decision. </s></p></div2><div2><head rend="bold">Results and conclusions</head><p n="p14.53"><s n="s1.2;p14.53">Since F=5.638488309>F <hi rend="sub">c</hi> =3.259446306, we reject the null hypothesis and therefore we reach the conclusion that the two regressions are not equivalent. </s><s n="s2.2;p14.53">Thus, there is difference in the intercept and slope coefficients of the two regressions which means that we cannot pool the data into one sample and describe them with one common regression model that has the form (Table 1): </s></p><p n="p15.53"><s n="s1.1;p15.53"><formula notation="" id="BAWE_6016l-form.005"/> </s></p><p n="p16.53"><s n="s1.1;p16.53"><formula notation="" id="BAWE_6016l-form.006"/> </s></p><p n="p17.53"><s n="s1.1;p17.53">Instead of that, the equation describing the regression for the first half of the sample is given by (Table 2): </s></p><p n="p18.53"><s n="s1.1;p18.53"><formula notation="" id="BAWE_6016l-form.007"/> </s></p><p n="p19.53"><s n="s1.1;p19.53"><formula notation="" id="BAWE_6016l-form.008"/> </s></p><p n="p20.53"><s n="s1.1;p20.53">This regression model implies that if x increases by 1 unit, y will be increased by about 0.41 units. </s></p><p n="p21.53"><s n="s1.1;p21.53">The equation describing the regression for the second half of the sample is given by (Table 2): </s></p><p n="p22.53"><s n="s1.1;p22.53"><formula notation="" id="BAWE_6016l-form.009"/> </s></p><p n="p23.53"><s n="s1.1;p23.53">This regression model implies that if x increases by 1 unit, y will be increased by about 4.8 units. </s></p><p n="p24.53"><s n="s1.2;p24.53">The same decision can be reached if we use the p-value. </s><s n="s2.2;p24.53">Thus, we reject the H <hi rend="sub">0</hi> because p=0.007408845&lt;0.05 (Table 3). </s></p><table id="BAWE_6016l-tab.001"><head><hi rend="bold">Table 1:</hi> Restricted model.</head><row><cell/></row></table><table id="BAWE_6016l-tab.002"><head><hi rend="bold">Table 2:</hi> Unrestricted model.</head><row><cell/></row></table><table id="BAWE_6016l-tab.003"><head><hi rend="bold">Table 3</hi> : F-test template.</head><row><cell/></row></table></div2></div1><div1 type="text"><head rend="bold">QUESTION 2</head><div2><head rend="bold">Introduction and methodology</head><p n="p25.53"><s n="s1.2;p25.53">We want to examine whether heteroskedasticity is present. </s><s n="s2.2;p25.53">One way for doing that is by plotting the residuals, however, a formal way of testing for heteroskedasticity is the Goldfeld-Quandt test, which is the method applied in the current problem. </s></p><p n="p26.53"><s n="s1.1;p26.53">We specify the null hypothesis (H <hi rend="sub">0</hi>: σ<hi rend="sub">1</hi><hi rend="sup">2</hi>=σ<hi rend="sub">2</hi><hi rend="sup">2</hi>), which states that the two subsamples have equal variances and therefore heteroskedasticity does not exist and the alternative hypothesis (H <hi rend="sub">1</hi>: σ<hi rend="sub">1</hi><hi rend="sup">2</hi>≠σ<hi rend="sub">2</hi><hi rend="sup">2</hi>), which states that the two subsamples do not have equal variances and therefore heteroskedasticity exists. </s></p><p n="p27.53"><s n="s1.10;p27.53">The data were split in half and ordered based on variance. </s><s n="s2.10;p27.53">Then, two regressions were run, using the first half of the data (20 observations) in the first one and the second half of the data (20 observations) in the second regression. </s><s n="s3.10;p27.53">The two regressions provided the estimated variances, which were input in Table 2 together with the values for T <hi rend="sub">1</hi>, T <hi rend="sub">2</hi>, K, alpha (number of observations in the first subsample, number of observations in the second subsample, number of coefficients and significance level respectively). </s><s n="s4.10;p27.53">By replacing these values in Table 2 we get the computed values required to reach a decision. </s><s n="s5.10;p27.53">The GQ value was then compared with F <hi rend="sub">c,</hi> the critical value from the F-distribution with (T <hi rend="sub">1</hi>-K) and (T <hi rend="sub">2</hi>-K) degrees of freedom. </s><s n="s6.10;p27.53">In case that heteroskedasticity is present in the data we assume that we have proportional heteroskedasticity and perform generalized least squares to correct for this. </s><s n="s7.10;p27.53">To do this, we need to change or transform our statistical model into one with homoskedastic errors. </s><s n="s8.10;p27.53">The initial statistical model y <hi rend="sub">t</hi>=β<hi rend="sub">1</hi>+β<hi rend="sub">2</hi>x <hi rend="sub">t</hi>+e <hi rend="sub">t</hi> is divided by the square root of x <hi rend="sub">t</hi> and we get the transformed model y <hi rend="sub">t</hi>*=β<hi rend="sub">1</hi>x <hi rend="sub">t1</hi>*+β<hi rend="sub">2</hi>x <hi rend="sub">t2</hi>*+e <hi rend="sub">t</hi>*. </s><s n="s9.10;p27.53">The new transformed error term e <hi rend="sub">t</hi>* is now homoskedastic. </s><s n="s10.10;p27.53">Finally, we run a regression on the transformed model. </s></p></div2><div2><head rend="bold">Results and conclusions</head><p n="p28.53"><s n="s1.3;p28.53">Since GQ=5.54153314>F <hi rend="sub">c</hi>=2.217197134 (Table 4), we reject the null hypothesis of equal variances between the two subsamples and therefore we conclude that heteroskedasticity is present because the variances for all observations are not the same. </s><s n="s2.3;p28.53">The same decision can be reached if we use the p-value. </s><s n="s3.3;p28.53">Thus, we reject the H <hi rend="sub">0</hi> because p=0.00034173&lt;0.05 (Table 4). </s></p><p n="p29.53"><s n="s1.3;p29.53">The fact that heteroskedasticity is present in the data means that the least squares estimator is still a linear and unbiased estimator but it is no longer the best linear unbiased estimator (B.L.U.E.). </s><s n="s2.3;p29.53">Moreover, the standard errors usually computed for the least squares estimator are incorrect. </s><s n="s3.3;p29.53">As a result, confidence intervals and hypothesis tests that use these standard errors may be misleading. </s></p><p n="p30.53"><s n="s1.1;p30.53">The equation that describes the regression of the new transformed model is given by (Table 5): </s></p><p n="p31.53"><s n="s1.1;p31.53"><formula notation="" id="BAWE_6016l-form.010"/> </s></p><p n="p32.53"><s n="s1.3;p32.53">To sum up, by transforming the variables we have converted a heteroskadastic error model into a homoskedastic error model, but the meaning of the coefficients does not change. </s><s n="s2.3;p32.53">So, the transformed regression model implies that if x <hi rend="sub">t1</hi>* increases by 1 unit, y <hi rend="sub">t</hi>* will be decreased by about 9.14 units. </s><s n="s3.3;p32.53">Similarly, if x <hi rend="sub">t2</hi>* increases by 1 unit, y <hi rend="sub">t</hi>* will be increased by about 11.59 units. </s></p><table id="BAWE_6016l-tab.004"><head><hi rend="bold">Table 4</hi> : Goldfeld-Quandt test template.</head><row><cell/></row></table><table id="BAWE_6016l-tab.005"><head><hi rend="bold">Table 5</hi> : Output from the generalized least squares estimation procedure.</head><row><cell/></row></table></div2></div1><div1 type="text"><head rend="bold">QUESTION 3</head><div2><head rend="bold">Introduction and methodology</head><p n="p33.53"><s n="s1.5;p33.53">We want to test both series of data for the presence of a unit root. </s><s n="s2.5;p33.53">For this purpose we will conduct a Dickey-Fuller test for each series of data. </s><s n="s3.5;p33.53">Initially, we have the y <hi rend="sub">t</hi> variable and we create five more, namely Δy <hi rend="sub">t</hi>, t, y <hi rend="sub">t-1</hi>, Δy <hi rend="sub">t-1</hi> and Δy <hi rend="sub">t-2</hi>. </s><s n="s4.5;p33.53">We then run three regressions, which they have the same dependent variable y <hi rend="sub">t</hi> but different independent variables. </s><s n="s5.5;p33.53">For the first regression we use y <hi rend="sub">t-1</hi> as the independent variable, for the second t and y <hi rend="sub">t-1</hi> and for the last y <hi rend="sub">t-1</hi>, Δy <hi rend="sub">t-1</hi> and </s></p><p n="p34.53"><s n="s1.2;p34.53">Δy <hi rend="sub">t-2</hi>. </s><s n="s2.2;p34.53">The three regressions have the form: </s></p><p n="p35.53"><s n="s1.1;p35.53"><formula notation="" id="BAWE_6016l-form.011"/> </s></p><p n="p36.53"><s n="s1.1;p36.53">The hypothesis that we want to test are specified as follows: </s></p><p n="p37.53"><s n="s1.1;p37.53">The null hypothesis (H <hi rend="sub">0</hi>: γ=0), which states that the series has a unit root, thus it is nonstationary against the alternative (H <hi rend="sub">1</hi>: γ≠0), which states that the series does not have a unit root, thus it is stationary. </s></p><p n="p38.53"><s n="s1.1;p38.53">The tau statistics that result from the three regressions are compared with the critical values for the Dickey-Fuller test in order to reach a decision. </s></p><p n="p39.53"><s n="s1.2;p39.53">We then want to test whether the first difference of the series is stationary. </s><s n="s2.2;p39.53">We test this by running a regression using the first difference of the series as the y variable and Δy <hi rend="sub">t</hi> as the x variable using the same type of t-statistic. </s></p><p n="p40.53"><s n="s1.4;p40.53">In case of the presence of the unit root we want to test whether these two series are cointegrated, thus we need to test whether the errors are stationary. </s><s n="s2.4;p40.53">For this purpose, we conduct a Dickey-Fuller test on the residuals of the regression between the two series. </s><s n="s3.4;p40.53">From the regression output we get the residuals and then create two new variables, delta_e and e_ <hi rend="sub">t-1</hi>. </s><s n="s4.4;p40.53">We then run a new regression using delta_e as y variable and e_ <hi rend="sub">t-1</hi> as x variable, which has the form: </s></p><p n="p41.53"><s n="s1.1;p41.53"><formula notation="" id="BAWE_6016l-form.012"/> </s></p><p n="p42.53"><s n="s1.1;p42.53">The tau statistic that results from this regression is compared with the critical value for the cointegration test in order to reach a decision. </s></p></div2><div2><head rend="bold">Results and conclusions</head><p n="p43.53"><s n="s1.1;p43.53">The regressions (3), (4) and (5) for the first series have the form (Table 6): </s></p><p n="p44.53"><s n="s1.1;p44.53"><formula notation="" id="BAWE_6016l-form.013"/> </s></p><p n="p45.53"><s n="s1.1;p45.53">The tau statistics (4.846487991, 2.934464877, 1.424061824) are all positive (Table 6) and when compared to the critical values for the Dickey-Fuller test (which are negative), we fail to reject the null hypothesis and conclude that the series have a unit root (nonstationary). </s></p><p n="p46.53"><s n="s1.1;p46.53">Moreover, we do not reject the null hypothesis and conclude that the first difference is nonstationary, because the tau statistic (2.141160617) (Table 7) is positive. </s></p><p n="p47.53"><s n="s1.1;p47.53">The regressions (3), (4) and (5) for the second series have the form (Table 8): </s></p><p n="p48.53"><s n="s1.1;p48.53"><formula notation="" id="BAWE_6016l-form.014"/> </s></p><p n="p49.53"><s n="s1.1;p49.53">The tau statistics (5.519240913, 3.265707032, 1.890947711) are all positive (Table 8) and when compared to the critical values for the Dickey-Fuller test (which are negative), we fail to reject the null hypothesis and conclude that the series have a unit root (nonstationary). </s></p><p n="p50.53"><s n="s1.1;p50.53">Moreover, we do not reject the null hypothesis and conclude that the first difference is nonstationary, because the tau statistic (2.597557656) (Table 9) is positive. </s></p><p n="p51.53"><s n="s1.1;p51.53">Since the tau statistic (-8.657508067) is greater than the critical value for the 1% significance level (-3.90) (Table 10), we reject the null hypothesis that the least squares residuals are nonstationary and conclude that they are stationary and therefore the two series are cointegrated. </s></p><p n="p52.53"><s n="s1.1;p52.53">The regression (6) has the form (Table 10): </s></p><p n="p53.53"><s n="s1.1;p53.53"><formula notation="" id="BAWE_6016l-form.015"/> </s></p><table id="BAWE_6016l-tab.006"><head><hi rend="bold">Table 6</hi> : First series Dickey-Fuller test output.</head><row><cell/></row></table><table id="BAWE_6016l-tab.007"><head><hi rend="bold">Table 7</hi> : Test for the first difference of the first series.</head><row><cell/></row></table><table id="BAWE_6016l-tab.008"><head><hi rend="bold">Table 8</hi> : Second series Dickey - Fuller test output.</head><row><cell/></row></table><table id="BAWE_6016l-tab.009"><head><hi rend="bold">Table 9</hi> : Test for the first difference of the second series.</head><row><cell/></row></table><table id="BAWE_6016l-tab.010"><head><hi rend="bold">Table 10</hi> : Cointegration test output.</head><row><cell/></row></table></div2></div1></body><back/></text></TEI.2>
