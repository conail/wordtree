<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE TEI.2 SYSTEM "tei_bawe.dtd"><TEI.2 id="_6100a" n="version 1.0"><teiHeader><fileDesc><titleStmt><title>Simulation of a Theoretical Neuroprocessor Board Based on the Izhikevich Neuron Model</title></titleStmt><extent/><publicationStmt><distributor>British Academic Written English (BAWE) corpus</distributor><availability><p>The British Academic Written English (BAWE) corpus was developed at the Universities of Warwick, Reading and Oxford Brookes, under the directorship of Hilary Nesi and Sheena Gardner (formerly of the Centre for Applied Linguistics [previously called CELTE], Warwick), Paul Thompson (Department of Applied Linguistics, Reading) and Paul Wickens (Westminster Institute of Education, Oxford Brookes), with funding from the ESRC. Subject to the rights of the these institutions in the BAWE corpus, and pursuant to the ESRC agreement, the BAWE corpus is available to researchers for research purposes PROVIDED THAT the following conditions are met:</p><p>1. The corpus files are not distributed in either their original form or in modified form.</p><p>2. The texts are used for research purposes only; they should not be reproduced in teaching materials.</p><p>3. The texts are not reproduced in full for a wider audience/readership, although researchers are free to quote short passages of text (up to 200 running words from any given text).</p><p>4. The BAWE corpus developers (contact: Hilary Nesi) are informed of all projects, dissertations, theses, presentations or publications arising from analysis of the corpus.</p><p>5. Researchers acknowledge their use of the corpus using the following form of words: "The data in this study come from the British Academic Written English (BAWE) corpus, which was developed at the Universities of Warwick, Reading and Oxford Brookes under the directorship of Hilary Nesi and Sheena Gardner (formerly of the Centre for Applied Linguistics [previously called CELTE], Warwick), Paul Thompson (Department of Applied Linguistics, Reading) and Paul Wickens (Westminster Institute of Education, Oxford Brookes), with funding from the ESRC (RES-000-23-0800)."</p></availability></publicationStmt><notesStmt><note resp="British Academic Written English (BAWE) corpus project">The document has a title page which contains:
document title
title part</note><note resp="British Academic Written English (BAWE) corpus project">Appendix contains: 
5.1.	 Pseudo-Code version of Original spNET
5.2.	Code-Listing of Implementation v1 - Control Module
5.3.	Code-Listing of Implementation v1 - Neuron Module
5.4.	 Code-Listing of Implementation v2 - Main Func
5.5.	Code-Listing of Implementation v2 - Host Computer
5.6.	 Code-Listing of Implementation v2 - PCI Controller
5.7.	 Code-Listing of Implementation v2 - Neuron Board</note><note resp="British Academic Written English (BAWE) corpus project">Page header contains: student name; student number; date. 
Page footer contains: module code; page number; module title. 
</note></notesStmt><sourceDesc><p n="level">1</p><p n="date">2006-04</p><p n="module title">Programming</p><p n="module code">SE1SA5</p><p n="genre family">Design specification</p><p n="discipline">Cybernetics &amp; Electronic Engineering</p><p n="disciplinary group">PS</p><p n="grade">D</p><p n="number of authors">1</p><p n="number of words">8047</p><p n="number of s-units">306</p><p n="number of p">133</p><p n="number of tables">0</p><p n="number of figures">1</p><p n="number of block quotes">0</p><p n="number of formulae">27</p><p n="number of lists">5</p><p n="number of paragraphs formatted like lists">33</p><p n="abstract present">abstract present</p><p n="average words per s-unit">26.3</p><p n="average s-units per p">2.3</p><p n="macrotype of assignment">simple assignment</p></sourceDesc></fileDesc><encodingDesc><p>TEI P4 (documented in: BAWE.documentation.pdf)</p></encodingDesc><profileDesc><particDesc><person><p n="gender">m</p><p n="year of birth">1986</p><p n="first language">English</p><p n="education">UK6</p><p n="course">AI &amp; Cybernetics</p><p n="student ID">6100</p></person></particDesc></profileDesc></teiHeader><text><front><titlePage><docTitle><titlePart rend="bold">Simulation of a Theoretical Neuroprocessor Board Based on the Izhikevich Neuron Model</titlePart></docTitle><titlePart>A first year programming project By<name type="student name"/></titlePart></titlePage><div1 type="toc" n="3"><head rend="underlined bold">Contents</head><p/></div1></front><body><div1 type="abstract"><head rend="underlined bold">1. Introduction1.1. Abstract</head><p n="p1.133"><s n="s1.8;p1.133">The Izhikevich Neuron model is currently the forefront spiking neural net model. </s><s n="s2.8;p1.133">It can emulate all the key modes of firing, and does so with only 13 floating point operations per second of emulation time <hi rend="sup">1</hi>. </s><s n="s3.8;p1.133">The Hodgkin's and Huxley model - the only other model capable of displaying so many firing modes - requires 1200. </s><s n="s4.8;p1.133">In an experiment done by Izhikevich, where he ran a network of these neurons with 10 <hi rend="sup">10</hi> synapses on a Beowulf supercomputer, it took 50 days to run what could be considered one second of brain activity. </s><s n="s5.8;p1.133">The integrate-and-fire model, the fastest spiking neuron model, which has drastically lower functionality, still requires 5 FLOPS and thus would have still taken around 20 days to run the simulation. </s><s n="s6.8;p1.133">This highlights a problem with such neurons - running them on conventional computers - even supercomputers, is not efficient. </s><s n="s7.8;p1.133">The only way to run them, at any kind of reasonable speed is on massively parallel hardware dedicated to running them. </s><s n="s8.8;p1.133">This report charts the development of a simulator for a theoretical board which could run Izhikevich type neurons, in a parallel fashion with easily expandable processing capabilities. </s></p></div1><div1 type="section"><div2><head rend="underlined bold">1.2. The Task</head><p n="p2.133"><s n="s1.4;p2.133">The task in its simplest terms is to develop a simulation of some type, or part, of a computer system. </s><s n="s2.4;p2.133">Having selected what we are going to simulate we are to examine other systems, and develop a design for our own version, with highlight on certain differences. </s><s n="s3.4;p2.133">For this particular project, 'computer system' will be taken to be a 'Spiking Neural Network Board' - as permitted by the project coordinator. </s><s n="s4.4;p2.133">Due to its relative complexity, it will be developed using an iterative methodology, initially only being a simulation in an abstract sense, which will then be refined to a greater depth. </s></p></div2><div2><head rend="underlined bold">1.3. Spiking Neurons</head><p n="p3.133"><s n="s1.6;p3.133">The development of Neural Networks has gone through three main generations <hi rend="sup">2</hi>. </s><s n="s2.6;p3.133">The first was focused around the McCulloch-Pitts neuron. </s><s n="s3.6;p3.133">A conceptually simple model, consisting of neurons which multiplied their inputs by a weighting, summed them and fired a binary high signal if its input went above a set threshold. </s><s n="s4.6;p3.133">These neurons are quite powerful, have many uses, and do not use much computational power however they do have limitations, such as only being about to handle digital values. </s><s n="s5.6;p3.133">The second generation solved this problem, and begun using continuous activation functions and used <hi rend="italic">rate coding</hi> (a higher rate of firing implies a higher output) to give their output. </s><s n="s6.6;p3.133">This generation was more biologically realistic and computationally powerful than the first - it did require more processing time though. </s></p><p n="p4.133"><s n="s1.4;p4.133">It was then discovered though that the cortex was able to perform at far greater speeds than possible with rate coding, when Thorpe et Al found that it only took 100ms for a human to analyse and clarify visual input (for instance, in facial recognition). </s><s n="s2.4;p4.133">The third generation thus had to find a new way to transmit information, and this was found in the form of pulse coding, with each spike being important on its own basis. </s><s n="s3.4;p4.133">This generation begun using features which take into account the exact time that a spike is triggered and arrives at the postsynaptic synapse such as axonal conductance delays and spike-timing dependant plasticity,. </s><s n="s4.4;p4.133">This was the birth of the Spiking Neuron. </s></p><figure id="BAWE_6100a-fig.001"><head>[<hi rend="italic">Fig 1</hi> ] These diagrams show a selection of the various different <hi rend="italic">spike trains</hi> that a single spiking (or biological) neuron can send out across a period of time, in response to only a single step input. There are many more behaviours which can be demonstrated by biological neurons - but the majority of which are not possible with most spiking neuron models.</head></figure></div2><div2><head rend="underlined bold">1.4. The Izhikevich Neuron</head><p n="p5.133"><s n="s1.7;p5.133">The first neuronal model that could be considered a spiking neural network was the Hodgkin's &amp; Huxley model. </s><s n="s2.7;p5.133">It was created after several years of electrical analysis of squids neurons. </s><s n="s3.7;p5.133">It was a highly detailed model, correctly known as a <hi rend="italic">bio--physiological</hi> model which took into account such things as the transfer of molecules across the cell membrane along the axon. </s><s n="s4.7;p5.133">It was able to demonstrate the vast array of different neuro-computational properties that are out there, some of which are shown in figure one. </s><s n="s5.7;p5.133">The majority of spiking neuron models have since sacrificed these though in the name of simplicity and computational efficiency. </s><s n="s6.7;p5.133">The integrate-and-fire model, probably the most popular - and most computationally simple, has only three different behaviours <hi rend="sup">1</hi>. </s><s n="s7.7;p5.133">The Izhikevich model on the other hand has the full range of neuro-computational properties, at the same time it remains quite simple, and computationally light - only requiring 13 floating point operations during each ms of simulating time. </s></p><figure id="BAWE_6100a-fig.002"><head>[<hi rend="italic">Fig 2</hi> ] The biological plausibility of neurons tends to veer off sharply with decreased implementation cost. The Izhikevich model is however, the exception to the rule. </head></figure><p n="p6.133"><s n="s1.2;p6.133">The model was created using bifurcation methodologies, which allowed the Hodgkin's &amp; Huxley model to be reduced to, and represented as standard differential equations. </s><s n="s2.2;p6.133">These are <hi rend="sup">4</hi>: </s></p><p n="p7.133"><s n="s1.1;p7.133"><formula notation="" id="BAWE_6100a-form.001"/> </s></p><p n="p8.133"><s n="s1.1;p8.133"><formula notation="" id="BAWE_6100a-form.002"/> </s></p><p n="p9.133"><s n="s1.1;p9.133"><formula notation="" id="BAWE_6100a-form.003"/> </s></p><p n="p10.133"><s n="s1.1;p10.133">Where <formula notation="" id="BAWE_6100a-form.004"/> is membrane potential and <formula notation="" id="BAWE_6100a-form.005"/> is membrane recovery </s></p><p n="p11.133"><s n="s1.1;p11.133">Synaptic &amp; injected currents are inserted via variable <formula notation="" id="BAWE_6100a-form.006"/> </s></p><p n="p12.133"><s n="s1.1;p12.133"><formula notation="" id="BAWE_6100a-form.007"/> and <formula notation="" id="BAWE_6100a-form.008"/> are dimensionless parameters </s></p><p n="p13.133"><s n="s1.1;p13.133"><formula notation="" id="BAWE_6100a-form.009"/> describes the time scale of the recovery of <formula notation="" id="BAWE_6100a-form.010"/> </s></p><p n="p14.133"><s n="s1.1;p14.133"><formula notation="" id="BAWE_6100a-form.011"/> describes the sensitivity of <formula notation="" id="BAWE_6100a-form.012"/> to subtle changes in <formula notation="" id="BAWE_6100a-form.013"/> </s></p><p n="p15.133"><s n="s1.1;p15.133"><formula notation="" id="BAWE_6100a-form.014"/> is the after spike reset value of <formula notation="" id="BAWE_6100a-form.015"/> </s></p><p n="p16.133"><s n="s1.1;p16.133"><formula notation="" id="BAWE_6100a-form.016"/> affects the after spike recovery of <formula notation="" id="BAWE_6100a-form.017"/> </s></p><p n="p17.133"><s n="s1.1;p17.133"><formula notation="" id="BAWE_6100a-form.018"/> represents <formula notation="" id="BAWE_6100a-form.019"/> where <formula notation="" id="BAWE_6100a-form.020"/> is the time </s></p><p n="p18.133"><s n="s1.1;p18.133">These formulas create a spike, which affect the two key variables, as such: </s></p><figure id="BAWE_6100a-fig.003"><head>[<hi rend="italic">Fig 3</hi> ] The effect of a spike on u and v</head></figure><p n="p19.133"><s n="s1.3;p19.133">By altering the values of a, b, c and d the different types of behaviour discussed above can be created. </s><s n="s2.3;p19.133">For instance, to create a Regular Spiking neuron you would use the values, a = 0.02, b = 0.2, c = -65 and d = 2. </s><s n="s3.3;p19.133">You could get the other types in accordance with these charts: </s></p><figure id="BAWE_6100a-fig.004"><head>[<hi rend="italic">Fig 4</hi> ] The different values of parameters needed to get different neuron behaviours</head></figure></div2><div2><head rend="underlined bold">1.5. Axonal Conduction Delays</head><p n="p20.133"><s n="s1.4;p20.133">In its simplest terms the axonal conductance delay is the length of time required for a postsynaptic neuron to become aware that a spike occurred. </s><s n="s2.4;p20.133">In the neocortex it can vary from 0.1ms to 44ms, and can be precisely reproduced between two neurons. </s><s n="s3.4;p20.133">The majority of the computational neuroscience community has typically ignored its existence in the cortex, where it has typically been treated as a complicating nuisance. </s><s n="s4.4;p20.133">But in recent times the argument has been repeatedly driven in that the brain would not maintain different delays with such precision if spike timings were not important. </s></p><p n="p21.133"><s n="s1.3;p21.133">Izhikevich proved in 2005 <hi rend="sup">5</hi> that delays could lead to an unprecedented information capacity in neural networks and allows stable firing patterns that are not possible without the delay. </s><s n="s2.3;p21.133">Because of this, it is now increasingly considered an important feature of spiking neural networks, and will be included in this simulation. </s><s n="s3.3;p21.133">The idea can be illustrated simply as follows: </s></p><figure id="BAWE_6100a-fig.005"><head>[<hi rend="italic">Fig 5</hi> ] A) Axonal connections between neurons have different delays (B - D) Firings of neurons are denoted by the vertical bars. Each arrow points to the spike arrival time to the postsynaptic neuron. (B) Synchronous firing is not effective in eliciting a potent postsynaptic response since the spikes arrive at the postsynaptic neurons at different times. (C) The spiking pattern with neuron d firing at 0 ms, neuron c firing at 4 ms, and neuron b firing at 8 ms is optimal to excite neuron a because the spikes arrive at a simultaneously. (D) The reverse order of firing is optimal to excite neuron e.</head></figure></div2><div2><head rend="underlined bold">1.6. Spike-Timing Dependant Plasticity</head><p n="p22.133"><s n="s1.3;p22.133">STDP, or as it's also known, Hebbian Temporally Asymmetric Synaptic Plasticity, is a form of neural learning which has developed in response to the growth of Spiking Neural Networks. </s><s n="s2.3;p22.133">It is essentially half Hebbian learning and half Anti-Hebbian - Hebbian learning being the formally accepted idea that 'those who fire together, wire together'. </s><s n="s3.3;p22.133">It relies on that idea that synaptic plasticity (the strength - or weight - or a synaptic connection) depends on the relative timing of pre and post synaptic spikes.<hi rend="sup">6</hi> It was formalized into a mathematical system by Gal Chechik in 2003 and will be the learning system employed in this simulation. </s></p><p n="p23.133"><s n="s1.5;p23.133">It can be summarized simply as follows. </s><s n="s2.5;p23.133">If the presynaptic spike arrives at the postsynaptic neuron before the postsynaptic neuron fires - for example, it causes the firing - the synapse is potentiated. </s><s n="s3.5;p23.133">Its weight is increased according to the positive part of the STDP curve in Figure 6 but does not allow growth beyond a cut-off value, which is a parameter in the model. </s><s n="s4.5;p23.133">If the presynaptic spike arrives at the postsynaptic neuron after it fired, that is, it brings the news late, the synapse is depressed. </s><s n="s5.5;p23.133">Its weight is decreased according to the negative part of the STDP curve. </s></p><figure id="BAWE_6100a-fig.006"><head>[<hi rend="italic">Fig 6</hi> ] The STDP curve: The weight of synaptic connection from pre- to postsynaptic neuron is increased if the postneuron fired after the presynaptic spike, that is, the interspike interval <hi rend="italic">t ></hi> 0. The magnitude of change decreases as <hi rend="italic">A</hi> +<hi rend="italic">e</hi>−<hi rend="italic">t/τ</hi> + . Reverse order results in a decrease of the synaptic weight with magnitude <hi rend="italic">A</hi>−<hi rend="italic">et/τ</hi>− .</head></figure></div2><div2><head rend="underlined bold">1.7. Why Implement as Hardware? </head><p n="p24.133"><s n="s1.5;p24.133">Spiking Neural Networks require much greater amounts of processing than their earlier cousins, such as the McCulloch-Pitts neuron. </s><s n="s2.5;p24.133">Because of this, when we create large networks of them, it becomes impossible to run them in real time on a serial processing computer. </s><s n="s3.5;p24.133">As many of the purposes we wish to assign neural networks to, rely on being able to process real time data - such as analyzing continuous visual and audio streams, which require networks of around the magnitude of 10 <hi rend="sup bold">6</hi>. </s><s n="s4.5;p24.133">They become significantly less useful, even useless, when they are unable to do this. </s><s n="s5.5;p24.133">Some tasks may not <hi rend="italic">require</hi> real time simulation, but some, such as whole brain simulations, certainly require speeds higher than those currently available. </s></p><p n="p25.133"><s n="s1.5;p25.133">Having realized this, there are a few options - to either make the serial computer more powerful, to develop a specialised serial computer built for the task, or, develop the network in parallel hardware. </s><s n="s2.5;p25.133">Due to the very nature of neural networks, this is the most efficient, and logical way to perform the task. </s><s n="s3.5;p25.133">There are other reasons beyond the ability to process real time as to why we want dedicated SNN hardware. </s><s n="s4.5;p25.133">If they are to be commercialized and integrated into appliances and working robots then it will be necessary to produce them <hi rend="italic">en</hi> mass, in an easily adaptable form, small enough, quick enough and cheaply enough to justify using them. </s><s n="s5.5;p25.133">They will also have to drain limited amounts of power in the majority of circumstances - something certainly not done by a modern workstation. </s></p></div2><div2><head rend="underlined bold">1.8. Existing Hardware Implementations</head><p n="p26.133"><s n="s1.5;p26.133">Neural networks based on the first two generations never really begun to be implemented in hardware to any great extent - it was never necessary due to the limited amount of calculation required. </s><s n="s2.5;p26.133">However, with the move to spiking neural networks it has become more important, and thus more heavily researched. </s><s n="s3.5;p26.133">The majority of work done at converting the networks to hardware has been with FPGA's (Field Programmable Gate Arrays)<hi rend="sup"> 7</hi>, although recently eyes have begun being cast towards the emerging technology of FPAA (Field Programmable Analogue Arrays). </s><s n="s4.5;p26.133">This has been due to the flexibility of implementing such a solution without commiting to a costly silicon ASIC fabrication. </s><s n="s5.5;p26.133">Using them it is effectively possible to simply develop the network as a program then directly translate it into silicon. </s></p><p n="p27.133"><s n="s1.5;p27.133">There have also been two key sides from which the problem has been approached, in whether it is implemented in an analogue form with continuous values in continuous time, or digital form with discrete values valid at specific instants. </s><s n="s2.5;p27.133">Analogue has drawn a lot of attention from some sides but there are many disadvantages. </s><s n="s3.5;p27.133">There are serious restrictions due to pin-count, limiting the number of neurons per chip, and consequently the size of the network. </s><s n="s4.5;p27.133">Values are prone to inaccuracy and signals are slow to change. </s><s n="s5.5;p27.133">Along with this, most VLSI design and fabrication systems are designed for digital and so the analogue designer is often not using the best systems. </s></p><figure id="BAWE_6100a-fig.007"><head rend="italic">[Fig 7] A p-Channel analogue synapse which incorporates the key properties of a neuronal synapse into a single transistor</head></figure><p n="p28.133"><s n="s1.3;p28.133">Digital systems have the advantages of high speed adders, multipliers and functions, but are disadvantaged in that the same things are quite large, consume large amounts of power and on top of that - the input and output need to be digitized. </s><s n="s2.3;p28.133">Digital systems usually share certain components, and are thus limited in speed by the number of simulated neurons times the speed of the signal. </s><s n="s3.3;p28.133">Digital memory is also far more advanced than its analogue counterparts, which is important due to constant and alterable values of the system such as delay lengths and synaptic weights. </s></p><p n="p29.133"><s n="s1.3;p29.133">As this technology is on the whole new, and still well within the fields of research there are no existing standards as to how things are done, even once it has been decided whether a project will be done in analogue or digital form. </s><s n="s2.3;p29.133">There are however, some broad overall trends - such as there tends to be a controller, which routes signals, and a neuron chip, which performs essential calculations. </s><s n="s3.3;p29.133">Beyond this it isn't really possible to make broad sweeping statements, and thus I shall just discuss two existing systems about which information is available. </s></p><p n="p30.133"><s n="s1.1;p30.133">There are some techniques used broadly across development of hardware, for example, it was found by Roth et al in 1995, that there was no significant degradation of network performance - in terms of image processing - by limiting weights to eight bit and internal potentials to 16 bits - because of this, most projects used fast fixed point arithmetic instead of floating point arithmetic, which results in fairly significant speeds increases. </s></p><p n="p31.133"><s n="s1.5;p31.133">The first project we shall look at was developed at the California Institute of Technology, with the aim of using spiking neural nets for automotive applications, such as analysing data from numerous subsystems and determining when something is wrong <hi rend="sup">8</hi>. </s><s n="s2.5;p31.133">The existing pressure on car processing systems prompted efforts to development a VLSI design which would facilitate the implementation of SNN in high volume products. </s><s n="s3.5;p31.133">The design constraints for this project called for the development of an inexpensive, fully autonomous, and commercially viable electronic chip. </s><s n="s4.5;p31.133">This single chip implementation was required to be extremely compact in size and accurate. </s><s n="s5.5;p31.133">It was also desired that the chip should be useable for numerous applications beyond the current scope. </s></p><figure id="BAWE_6100a-fig.008"><head rend="italic">[Fig 8] The overall design of the VLSI Automotive SNN</head></figure><p n="p32.133"><s n="s1.6;p32.133">The design consists of a global controller, a pool of 16 neurons (three layers - 7 inputs, 1 output and 8 hidden), a ROM-based look-up table, neuron state registers and a synaptic weight ram. </s><s n="s2.6;p32.133">Inputs and outputs are stored in the state registers. </s><s n="s3.6;p32.133">When triggered by the controller, each of the <hi rend="italic">neurons</hi> takes in their inputs and synaptic weights, and performs the necessary functions. </s><s n="s4.6;p32.133">As the system uses layers which calculate their data one level at a time, it is possible for neurons on different layers to share mathematical processors. </s><s n="s5.6;p32.133">It is setup such that the chip is able to perform fully parallel calculations, under the supervision of the global controller. </s><s n="s6.6;p32.133">The task of the controller is essentially to avoid memory access issues and orchestrate data movements on-chip and off chip. </s></p><p n="p33.133"><s n="s1.6;p33.133">The second project is known as MASPINN (Memory optimized Accelerator for Spiking Neural Networks)<hi rend="sup">9</hi>. </s><s n="s2.6;p33.133">It is the latest of a series of network accelerators based on two previous configurations called SPIKE128k and NESPINN. SPIKE 128k was developed in the university of Padeborn. </s><s n="s3.6;p33.133">It is a FPGA based structure which leads to a limited speed. </s><s n="s4.6;p33.133">The next step is the NESPINN-system, which takes advantage of the VLSI-technology using Application Specific Integrated Circuits (ASICs). </s><s n="s5.6;p33.133">MASPINN has the purpose of allowing a sufficient resolution of processed images. </s><s n="s6.6;p33.133">In comparison to the NESPINN-architecture, the memory organization and dataflow of MASPINN is optimized in order to meet this challenge. </s></p><p n="p34.133"><s n="s1.2;p34.133">It is important to note the MASPINN is an accerator designed for connection of a potentially very large number of the boards up to a PC. The board itself is split into three main sections, the neuron unit, the connection unit and the spike event list. </s><s n="s2.2;p34.133">The actions of the individual sections is fairly self explanatory in a broad sense but, being designed for acceleration of large networks uses a large number of tricks to allow it to run fast - because of this is gets exceedingly complex and instead of trying to explain how each part works I will give an overview of its techniques used to achieve high speeds: </s></p><p rend="bulleted" n="p35.133"><s n="s1.1;p35.133">The event list is local to each board but directly spoken to by the host computer; it concerns which neuron has fired and those to be hit. </s></p><p rend="bulleted" n="p36.133"><s n="s1.2;p36.133">Several mathematical shortcuts are used, such as, due to decay, many action potential are effectively zero. </s><s n="s2.2;p36.133">In cases where this is true, instead of performing equations involving the action potential (Known as IP in MASPINN documentation), established values are inserted into them. </s></p><p rend="bulleted" n="p37.133"><s n="s1.2;p37.133">The board is capable of simulating neurons with different sets of parameters (allowing for instance, regular spiking and fast spiking neurons). </s><s n="s2.2;p37.133">It has been setup so that this is optimized, and similar neurons work together. </s></p><figure id="BAWE_6100a-fig.009"><head rend="italic">[Fig 9] An overview of the workings of the MASPINN board internals</head></figure></div2><div2><head rend="underlined bold">1.9. Existing Simulations of Hardware Implementations</head><p n="p38.133"><s n="s1.7;p38.133">The amount of information available about simulations of SNN hardware is limited. </s><s n="s2.7;p38.133">There was only information available about simulation of various parts of the MASPINN system created during and prior to its silicon implementation. </s><s n="s3.7;p38.133">One paper documented the simulation of the entire neuro-pipe chip - which is the backbone of the board <hi rend="sup">10</hi>, the other detailed the development of the decay module for the chip <hi rend="sup">9</hi>. </s><s n="s4.7;p38.133">They seem to have used to simulations to <hi rend="italic">sketch</hi> out implementation methods, and by profiling it, worked out how to make it run as quickly and efficiently as possible. </s><s n="s5.7;p38.133">Their simulations seemed to go to a great depth, with classes developed so as to allow realistic simulation of VLSI parts. </s><s n="s6.7;p38.133">They did such things as setup signal codes representing knowledge being passed around in the form of high/low signals through pins. </s><s n="s7.7;p38.133">It all seems to have been done with an eye of enhancing running speeds of the finished product. </s></p></div2></div1><div1 type="section"><head rend="underlined bold">2. Design and Development</head><div2><head rend="underlined bold">2.1. Overall plan</head><p n="p39.133"><s n="s1.4;p39.133">The design plan for this project is far from linear, and cannot simply be laid out straight from the start. </s><s n="s2.4;p39.133">The first step is to reverse engineer the original Izhikevich SNN program, and work out exactly how it works. </s><s n="s3.4;p39.133">With this done it will be possible to lay down the core designs for how the simulation will work overall, with emphasis on the modularization of subsystems. </s><s n="s4.4;p39.133">This first design will only be for a simulation in an abstract sense, with the basis laid down for further add-ons which will enhance it as a simulation. </s></p></div2><div2><head rend="underlined bold">2.2. Reverse Engineering</head><p n="p40.133"><s n="s1.4;p40.133">The original source code for a Izhikevich type neural network was available in two forms, as either MATLAB or C code <hi rend="sup">11</hi>. </s><s n="s2.4;p40.133">Reverse Engineering the C code was a necessary step to this project, as I had never had any experience with SNN's. </s><s n="s3.4;p40.133">By going through the code, examining each section and rewriting it as pseudo-code, I was able to tell what was simply an artefact of Izhikevich programming style, what was a core feature of SNN's and what was necessary to the Izhikevich model. </s><s n="s4.4;p40.133">The pseudo code I created is available in the appendix. </s></p></div2><div2><head rend="underlined bold">2.3. Requirements</head><p n="p41.133"><s n="s1.3;p41.133">Having now analysed what spiking neurons are and a large range of related systems - hardware, simulations and purely software, it is now possible to lay down a set of rules which the final system must obey. </s><s n="s2.3;p41.133">Matters such as how input should be gathered and entered, and what network topography should be used will be left to the next sections. </s><s n="s3.3;p41.133">Here are the initial requirements: </s></p><p rend="bulleted" n="p42.133"><s n="s1.1;p42.133">The hardware version should be able to work in parallel with the aim of allowing real time simulation - these parts should be based on dedicated hardware </s></p><p rend="bulleted" n="p43.133"><s n="s1.1;p43.133">It should be able to support any number of neurons </s></p><p rend="bulleted" n="p44.133"><s n="s1.1;p44.133">The hardware should be expandable to suit networks of different sizes </s></p><p rend="bulleted" n="p45.133"><s n="s1.1;p45.133">Different types of neurons, such as Regular Spiking and Resonators should be supported and the variety and number of which are in any system should be editable by the user </s></p><p rend="bulleted" n="p46.133"><s n="s1.1;p46.133">The ratio of excitory to inhibitory neurons should be variable </s></p><p rend="bulleted" n="p47.133"><s n="s1.1;p47.133">Inhibitory neurons should not be able to change to excitory and visa versa </s></p><p rend="bulleted" n="p48.133"><s n="s1.1;p48.133">Floating point inputs and outputs should be entered and taken from the system </s></p><p rend="bulleted" n="p49.133"><s n="s1.1;p49.133">The SNN model should used Axonal Delays, Spike-Timing Dependant Plasticity and the Izhikevich internal potential equations </s></p><p rend="bulleted" n="p50.133"><s n="s1.1;p50.133">There should be a user interface allowing selection of network choices, input of data and viewing of network output </s></p></div2><div2><head rend="underlined bold">2.4. Initial Design </head><div3><head rend="underlined">2.4.1. Issues:</head><p n="p51.133"><s n="s1.1;p51.133">Should each 'neuron' have its own piece of hardware? </s></p><p n="p52.133"><s rend="italic" n="s1.5;p52.133">The first of the hardware projects looked at had a separate chunk for each neuron, and had areas of memory dedicated to each. </s><s rend="italic" n="s2.5;p52.133">MASPINN on the other hand was simply an accelerator and only had one of each component and was able to run any number of neurons. </s><s rend="italic" n="s3.5;p52.133">As it is important that this hardware/simulator be able to simulate any number of neurons, it should not have a physical limitation - a low one anyway - on the number of neurons that can be run on it per board. </s><s rend="italic" n="s4.5;p52.133">As the simulations for one neuron can be run at a far greater speed than required for real time simulations anyway, the number of neurons run per board could instead be based on the speed of the operations and the amount of memory on the chip, and the extent to which these enable x number of neurons to run at real-time. </s><s rend="italic" n="s5.5;p52.133">Implementing this is however, not strictly necessary for the first version, and thus shall be left till later. </s></p><p n="p53.133"><s n="s1.1;p53.133">Should each 'neuron' be able to directly call each other neuron? </s></p><p n="p54.133"><s rend="italic" n="s1.3;p54.133">Neither of the projects looked at had direct connections between neurons. </s><s rend="italic" n="s2.3;p54.133">Having such connections would only really have any value or purpose in an analogue system or a digital system where each neuron was physically connected - something that could only happen in a system with very few neurons. </s><s rend="italic" n="s3.3;p54.133">In a digital system however, where multiple neurons are run from a single chip, it makes more sense to have a 'global controller' channelling communications between neurons. </s></p><p n="p55.133"><s n="s1.1;p55.133">Should it all be run on dedicated hardware? </s></p><p n="p56.133"><s rend="italic" n="s1.4;p56.133"> Having analysed the original code I feel that it is not necessary to run the entire system of dedicated hardware, in fact I feel it would unnecessarily complicate matters. </s><s rend="italic" n="s2.4;p56.133">As long as all the mathematics could be kept onto dedicated hardware, it is best to have a central computer which would route signals between neurons. </s><s rend="italic" n="s3.4;p56.133">This idea was used to an extent in the two hardware projects analysed above - in the first, in the form of the global controller, and in MASPINN in the form of the PCI Interface, taking spike data from the neurons and adding it to the event list. </s><s rend="italic" n="s4.4;p56.133">The PCI interface is not worth implementing in the first version though. </s></p><p n="p57.133"><s n="s1.1;p57.133">What network topographies should be used? </s></p><p n="p58.133"><s rend="italic" n="s1.5;p58.133">In neural networks, there has always been the concept of network topography. </s><s rend="italic" n="s2.5;p58.133">Networks normally take the form of several layers, the first taking in input then feeding this into one or more 'hidden layers', these will then feed this information on into an output layer. </s><s rend="italic" n="s3.5;p58.133">The first hardware project looked at used one hidden layer, and MASPINN was designed to merge the two types together - focusing on a general layered topology but with many random connections, of a primarily inhibitory nature. </s><s rend="italic" n="s4.5;p58.133">I feel though, that as the original spNET code was developed to work in random topography, with no enforced order, I should do the same with the first version to aid testing. </s><s rend="italic" n="s5.5;p58.133">Adding a layered topography should be considered an important next step though. </s></p><p n="p59.133"><s n="s1.1;p59.133">Should axonal connections and delays etc be explicitly stated by the user? </s></p><p n="p60.133"><s rend="italic" n="s1.3;p60.133"> While this would be a useful and interesting feature for proof of theory in very small networks of neurons, the complexity of building a network by explicitly stating connections would be an implausible waste of time, and effectively impossible to comprehend. </s><s rend="italic" n="s2.3;p60.133">Although it might be a useful tool in testing I do not feel it is currently worth the time required to implement it. </s><s rend="italic" n="s3.3;p60.133">Instead they should be randomly generated when the network is initialized. </s></p><p n="p61.133"><s n="s1.1;p61.133">Should it be able to handle input &amp; output? </s></p><p n="p62.133"><s rend="italic" n="s1.2;p62.133">Complicated input and output are beyond the scope of getting an initial version to work, although again, it may be useful for testing on some levels, the added complexity of the user interface that would be required would haul back testing time significantly. </s><s rend="italic" n="s2.2;p62.133">This should therefore be left till after. </s></p></div3><div3><head rend="underlined">2.4.2. Initial Specification:</head><p n="p63.133"><s n="s1.1;p63.133">Having now considered the requirements and associated issues an initial specification for the first version can be put together. </s></p><p rend="bulleted" n="p64.133"><s n="s1.3;p64.133">The hardware system would consist of a piece of software run on a host computer. </s><s n="s2.3;p64.133">Connected to the host, would be a number of boards - each capable of simulating any number of neurons - working in unison to host the network. </s><s n="s3.3;p64.133">The simulator will consider this as a control module and a neuron module - they should communicate directly. </s></p><p rend="bulleted" n="p65.133"><s n="s1.1;p65.133">All mathematical processing should be done in the neural module, and the work done in the control module constrained to simple data channelling tasks </s></p><p rend="bulleted" n="p66.133"><s n="s1.2;p66.133">The learning model used by the system should be Spike-Timing Dependant Plasticity, and the activation model, the Izhikevich type neuron. </s><s n="s2.2;p66.133">These should be hard coded into the system. </s></p><p rend="bulleted" n="p67.133"><s n="s1.1;p67.133">The system should be able to accommodate for user specified numbers of neurons, and the ratio of their types. </s></p><p rend="bulleted" n="p68.133"><s n="s1.2;p68.133">The system should generate random inputs, and feed them into the network at random locations. </s><s n="s2.2;p68.133">No specific output beyond firing rate should be given to the user. </s></p><p rend="bulleted" n="p69.133"><s n="s1.1;p69.133">A random topology should be used by the network, and the connections between the neurons within the network should be randomly generated </s></p></div3><div3><head rend="underlined">2.4.3. Initial Design:</head><p n="p70.133"><s n="s1.2;p70.133">The nature of this program is such that it could not be usefully represented by a Jason Structure Diagram, and so I have used a basic structure diagram to represent it. </s><s n="s2.2;p70.133">The diagram shows the modules as containers, core functions as squares, memory stores as rounded squares and data movements between them as directed arrows: </s></p><figure id="BAWE_6100a-fig.010"><head rend="italic">[Fig 10] The proposed software program as a network of functions connected by data flows </head></figure><p n="p71.133"><s n="s1.1;p71.133">The action of each function is as follows: </s></p><list type="simple"><item><hi rend="underlined">Control Loop:-</hi> Loops through each second and millisecond thereof, effectively acting as a clocking device, sending a signal to each neuron every millisecond, noting if it spikes, organising who is hit by it, and what neurons will be affected by the learning algorithm. </item><item><hi rend="underlined">Initialization Function:-</hi> Sends initialization calls to each neuron, setting up their types. Then sets up lists of the post synapses of each neuron. </item><item><hi rend="underlined">Initialization Functions:-</hi> Sets up initial values, neuronal constants and synapses </item><item><hi rend="underlined">STDP Functions:-</hi> Uses the STDP rules (<hi rend="italic">see 1.6</hi>) to adjust synaptic weights. Even though differences are slight between functions, there will be one of decay and another for potentiation. </item><item><hi rend="underlined">Activation Functions:-</hi> Uses Izhikevich equations governing neuronal activation (<hi rend="italic">see 1.4</hi>) to adjust the activation potential of each neuron every millisecond. Determines if the neuron has fired or not and makes required adjustments. </item><item><hi rend="underlined">Hit Manager:-</hi> Due to Axonal Conductance Delays (<hi rend="italic">see 1.5</hi>) it is not possible to simply input the weighted values to all the post neurons of a spiking neuron. Instead they have to be slowly added on as and when the set delay is reached. This deals with the intricacies of doing so.</item></list></div3><div3><head rend="underlined">2.4.4. Initial Pseudo-Code:</head><p n="p72.133"><s n="s1.1;p72.133">CONTROL LOOP: </s></p><p n="p73.133"><s n="s1.1;p73.133"><formula notation="" id="BAWE_6100a-form.021"/> </s></p><p n="p74.133"><s n="s1.1;p74.133">HIT MANAGEMENT: </s></p><p n="p75.133"><s n="s1.1;p75.133"><formula notation="" id="BAWE_6100a-form.022"/> </s></p><p n="p76.133"><s n="s1.1;p76.133">STDP FUNCTIONS: </s></p><p n="p77.133"><s n="s1.1;p77.133"><formula notation="" id="BAWE_6100a-form.023"/> </s></p><p n="p78.133"><s n="s1.1;p78.133">ACTIVATION FUNCTION: </s></p><p n="p79.133"><s n="s1.1;p79.133"><formula notation="" id="BAWE_6100a-form.024"/> </s></p><p n="p80.133"><s n="s1.1;p80.133">INITIALIZATION FUNCTION: </s></p><p n="p81.133"><s n="s1.1;p81.133"><formula notation="" id="BAWE_6100a-form.025"/> </s></p></div3><div3><head><hi rend="underlined">2.4.5. Code Listing:</hi> <hi rend="italic">See Appendix 2 &amp; 3</hi></head><p n="p82.133"/></div3><div3><head rend="underlined">2.4.6. Implementation Issues</head><p n="p83.133"><s n="s1.3;p83.133">The key problems with implementing this program lay with the validation testing, and ensuring that it was functioning as required. </s><s n="s2.3;p83.133">This was a great problem because it was essentially a learning process - I only knew how the network and its dynamic parts should behave, once I had examined it in numerous different situations, with different variables and configurations. </s><s n="s3.3;p83.133">Comparing output between the original program and that which I had developed was an important method, but was complicated when I discovered a flaw in the original program - it then became a case of guessing which was right, based on assumptions of desired values (i.e. firings rates should ideally be below 1% of network capacity). </s></p><p n="p84.133"><s n="s1.5;p84.133">Even had I known exactly what trends and patterns I should have been looking for in the data it would still have been an immense task. </s><s n="s2.5;p84.133">The problem was that numerous features only really emerged when the program was configured for large networks - in the order of 500-1000 neurons. </s><s n="s3.5;p84.133">When you run a network of this size, with time splits of millisecond size, the amount of data generated is colossal. </s><s n="s4.5;p84.133">As the standard debuggers were no real use for examining the multidimensional, dynamic arrays used to store most the data, all testing had to be hard coded, and data sent to the display. </s><s n="s5.5;p84.133">As most the time, many variables for each neuron had to be considered side by side, millisecond after millisecond, and cross referenced, backwards and forward; it frequently got frustrating and confusing. </s></p><p n="p85.133"><s n="s1.5;p85.133">Beyond the above issues, implementation was standard - a few bugs as ever, and a few cases of having to learn new facets of the language. </s><s n="s2.5;p85.133">One thing that I found quite interesting was that I had never had to use the class libraries for dynamic arrays (i.e. </s><s n="s3.5;p85.133"><hi rend="italic">vector</hi> or <hi rend="italic">deque</hi>) before as I had always written my own. </s><s n="s4.5;p85.133">However, it was simply not possible to use my classes with this, as they slowed the program down immensely. </s><s n="s5.5;p85.133">Instead I had to select the appropriate type of array for each task, based on whether they could perform their core functions required of them in constant or linear time. </s></p></div3><div3><head rend="underlined">2.4.7. Testing</head><p n="p86.133"><s n="s1.3;p86.133">As noted above, the testing caused great problems, and was in all, far harder than writing the program in the first place. </s><s n="s2.3;p86.133">It was not possible to use a standard testing framework, and thus, I cannot simply what my test plan was, along with expected results, actual results and required changes - it was instead necessary that I concocted a new test every time I uncovered a new lead, on the path to tracing problems back to their source. </s><s n="s3.3;p86.133">Here is an overview of how this worked: </s></p><div4><head rend="underlined">Initialization Testing:</head><p n="p87.133"><s n="s1.1;p87.133">Problem: - No explicit problem </s></p><p n="p88.133"><s n="s1.2;p88.133">Plan: - Set up 10 neurons - 8 excitory, 2 inhibitory. </s><s n="s2.2;p88.133">Before control loop starts display: </s></p><list type="bulleted"><item>Preneuron index, Neuron Type, Synapse Number, Target, Weight and Delay for all synapses</item><item>Post synapse &amp; Preneuron list for each neuron</item><item>Presynaptic delay data</item></list><p n="p89.133"><s n="s1.2;p89.133">The test should then be repeated with larger numbers of neurons, and checked that it roughly still works. </s><s n="s2.2;p89.133">Here is what data I expected to get from this: </s></p><p rend="bulleted" n="p90.133"><s n="s1.1;p90.133">There would be one or two synapses per neuron </s></p><p rend="bulleted" n="p91.133"><s n="s1.1;p91.133">The synapse numbers would be 0 and/or 1 </s></p><p rend="bulleted" n="p92.133"><s n="s1.1;p92.133">The synaptic targets would be between 0 &amp; 9 and extensive </s></p><p rend="bulleted" n="p93.133"><s n="s1.1;p93.133">Weights should be 6 for excitory neurons and -5 for inhibitory </s></p><p rend="bulleted" n="p94.133"><s n="s1.1;p94.133">Axonal conductance delays should be between 1 &amp; 20 and not necessarily extensive </s></p><p rend="bulleted" n="p95.133"><s n="s1.1;p95.133">All preneurons should have synapses leading to all post neurons </s></p><p rend="bulleted" n="p96.133"><s n="s1.1;p96.133">All post synapse will correspond to a preneuron </s></p><p rend="bulleted" n="p97.133"><s n="s1.1;p97.133">Predelay data corresponds with post-delay data </s></p><p n="p98.133"><s n="s1.5;p98.133">The first thing I noticed upon running it was that neurons had either zero or one synapse - this I fixed by making the line of code which selects the number of neurons, add one onto the number it generates. </s><s n="s2.5;p98.133">The next thing I noticed was that not all neurons had post neurons. </s><s n="s3.5;p98.133">To fix this I added a check into the main initialization function to ensure all neurons had post synapses - if they are found lacking, then a post synapse is added. </s><s n="s4.5;p98.133">It was also immediately apparent that predelays were being added ten times over - this was because the code section where they were added to the array was inside an external loop. </s><s n="s5.5;p98.133">By removing the section to outside that loop, the bug was fixed. </s></p></div4><div4><head rend="underlined">Control Loop Testing:</head><list type="simple"><item>Problem: Firing fell of soon after the start</item><item> Plan: Run sequential tests, uncovering the path to the problem and fix it along the way. The following mini-tests were performed:</item><item> Description: Set a "second" as 5ms long and see if no fire error still occurs</item><item> Result: Error still occurs</item><item> Description: Comment out SEC_Pulse code and see if no fire error still occurs</item><item> Result: Still occurs</item><item> Description: Comment out nFirings=0 and see if no fire error still occurs</item><item> Result: Still occurs</item><item> Description: Output text if a neuron fires and confirm neurons don't fire past the first second</item><item> Result: Neurons fire during first second but not after</item><item> Description: Confirm its still check during later seconds if neurons have fired</item><item> Result: Its still checked</item><item> Description: Print output of MS_Pulse</item><item> Result: Always false past first second</item><item> Description: Print <hi rend="italic">input</hi> from with MS_Pulse</item><item> Result: Input values are immediately exponentially larger than expected</item><item> Solution: Set hits[] values to 0 in init function</item><item> Description: Ran test 7 again</item><item> Result: Low firing rates achieved</item></list><p n="p99.133"><s n="s1.3;p99.133">Although the original problem was now solved, I was still faced with appalling speeds (two minutes and over to simulate a single second). </s><s n="s2.3;p99.133">The firing rates were also up in the hundreds. </s><s n="s3.3;p99.133">I performed the next batch of tests: </s></p><list type="simple"><item> Description: Ran program normally with 1000 neurons still with small seconds</item><item> Result: Firing rates of between 1 &amp; 2 achieved for first 250 seconds, then shrinks to around 0.06</item><item> Description: Ran program normally with 1000 neurons still with 1000ms seconds</item><item> Result: Program is very slow (over 2 minutes per second)</item><item> Eventually reveals an firing rate in the first second of 999.89</item><item> Tried Solution: Reduced potential number of synapses by factor of two</item><item> Description: Repeat of 10</item><item> Result: Around 45 seconds per simulated second</item><item> Firing rates still around 1000 p/sec</item></list><p n="p100.133"><s n="s1.2;p100.133">The program had by now stopped running for networks smaller than 100 neurons. </s><s n="s2.2;p100.133">I continued on, planning to lose this problem on the way: </s></p><list type="simple"><item> Description: Outputted synapse data at end of each millisecond</item><item> Result: Synapse weights seemed to change quite reasonably</item><item> Description: Outputted synapse data at end of each second</item><item> Result: Weights of numerous synapses was infinity (1.#INF), others had very large exponential numbers. </item><item> Solution: Introduced limits to the adjustments of weights</item><item> Description: Outputted synapse data at end of each second again</item><item> Result: All synaptic weights hit their lower limits</item><item> Solution: Reversed Polarity of Adjustments for Inhibitory neurons</item><item> Problem: Weights able to overcome limits again</item><item> Solution: Add limit checks to each adjust function</item><item> Result: Firing rate reduced by many hundreds, but weights still frequently at upper &amp; lower limits</item><item> Description: A <hi rend="italic">m</hi> and A <hi rend="italic">p</hi> reduced by factor of 10 and 13 repeated</item><item> Result: Firing rate returned to around 1000, then collapsed to 200 for the next second as weights approached 0.</item><item> Description: Monitor Voltage values with less than 500 neurons</item><item> Result: All seems quite kosher - Inhibitory neurons fire a lot more</item><item> Description: Monitor number of hits per MS with 100 neurons</item><item> Result: Starts low - grows exponentially</item><item> Description: Monitor average inputs to neurons</item><item> Result: Starts of occasional neuron getting hit - grows exponentially. Eventually reaches a steady state, whereby, I assume, each neuron is firing constantly, and so, a steady(ish) value of input to each neuron is attained</item></list><p n="p101.133"><s n="s1.2;p101.133">At this point I decided to simply try outputting all the information I could get about each neuron to the screen, as best formatted as possible, and to just stare at it till patterns jumped out at me. </s><s n="s2.2;p101.133">Through doing this, I discovered firstly and most importantly, that inhibitory neurons were having no effect (there were no negative inputs), delays were two ms shorter than they should be and voltages and recovery time can hit negative infinity and remain there for ever, after the 19 <hi rend="sup">th</hi> millisecond. </s></p><p n="p102.133"><s n="s1.3;p102.133">I found the solutions to these were to adjust the line of code which reads from the hit list, and to ensure the hit list was properly initialized to zero (previously the 20 <hi rend="sup">th</hi> value had been random garbage). </s><s n="s2.3;p102.133">By fixing the problem with delays, the problem with inhibitory neurons not hitting was solved. </s><s n="s3.3;p102.133">Fixing this problem, in turn solved the problem of extremely high firing rates, which in turn, solved the problem of extremely high run times. </s></p><p n="p103.133"><s n="s1.3;p103.133">There was still a strange problem though - synaptic weights always seemed to reach their limits rather quickly and on average they tended to levitate around them. </s><s n="s2.3;p103.133">While I appreciate the erroneous nature of this, the network still functions reasonably with it - albeit causing a higher firing rate than strictly required. </s><s n="s3.3;p103.133">Due to time constraints I was forced to leave solving this to my own free time. </s></p></div4></div3></div2><div2><head rend="underlined bold">2.5. Further Design</head><div3><head rend="underlined">2.5.1. Potential Improvements</head><p n="p104.133"><s n="s1.4;p104.133">With the core part of the programme now working, improvements can now be made with two key aims: making the network more useful, and more of a simulation than a simple program. </s><s n="s2.4;p104.133">The three key ways I plan to do this are mentioned in section 2.4.1. </s><s n="s3.4;p104.133">The issues at hand are implementing data input and output, a layered topology such that the network becomes more useful, and building a PCI Controller class through which all communication between the host computer and the neuronal board must be channelled. </s><s n="s4.4;p104.133">There are many other changes that could be made, but time constrictions are such that implementing all of these changes are unlikely. </s></p></div3><div3><head rend="underlined">2.5.2. Final Specification</head><p n="p105.133"><s n="s1.1;p105.133">Here is the final specification: </s></p><p rend="bulleted" n="p106.133"><s n="s1.3;p106.133">The hardware system would consist of a piece of software run on a host computer. </s><s n="s2.3;p106.133">Connected to the host, would be a number of boards - each capable of simulating any number of neurons - working in unison to host the network. </s><s n="s3.3;p106.133">The simulator will consider this as a control module and a neuron module - they should communicate through a PCI bus. </s></p><p rend="bulleted" n="p107.133"><s n="s1.1;p107.133">All mathematical processing should be done in the neural module, and the work done in the control module constrained to simple data channelling tasks </s></p><p rend="bulleted" n="p108.133"><s n="s1.2;p108.133">The learning model used by the system should be Spike-Timing Dependant Plasticity, and the activation model, the Izhikevich type neuron. </s><s n="s2.2;p108.133">These should be hard coded into the system. </s></p><p rend="bulleted" n="p109.133"><s n="s1.1;p109.133">The system should be able to accommodate for user specified numbers of neurons, and the ratio of their types. </s></p><p rend="bulleted" n="p110.133"><s n="s1.1;p110.133">Inputs should be read from a file, and inserted into 'input' neurons in the form of hits, where the input is the weight. </s></p><p rend="bulleted" n="p111.133"><s n="s1.1;p111.133">Output, in the form of spike timings for each output neuron, should be written to file each second. </s></p><p rend="bulleted" n="p112.133"><s n="s1.4;p112.133">A layered topology should be used by the network, and the connections between the neurons within the network should be generated with respect to this. </s><s n="s2.4;p112.133">Input neurons should have no preneurons, output neurons should have no post neurons. </s><s n="s3.4;p112.133">The user should be able to specify a number of hidden layers, between which information will be passed mainly forward, but with some backwards transfers. </s><s n="s4.4;p112.133">Each neuron on one layer should have a connection to every neuron on the next layer. </s></p></div3><div3><head rend="underlined">2.5.3. Final Design</head><p n="p113.133"><s n="s1.1;p113.133">The structure diagram remains almost identical, but now with a PCI function separating the two main components, and two file stores for I/O: </s></p><figure id="BAWE_6100a-fig.011"><head rend="italic">[Fig 12] The updated system functional layout</head></figure><p n="p114.133"><s n="s1.1;p114.133">As a conceptual aid the following diagram can be used to think of the system as hardware: </s></p><figure id="BAWE_6100a-fig.012"><head rend="italic">[Fig 12] The proposed system hardware system</head></figure><p n="p115.133"><s n="s1.3;p115.133">I feel that the best way to implement input initially will be to have a separate program generate a file containing a list of floating point number, based on some function, the length of the main control loop. </s><s n="s2.3;p115.133">Output can simply be churned out as a list of neurons and every occasion on which they fired - I feel it is more useful to do so for every neuron, as this allows for easier neurocomputational studies on the system. </s><s n="s3.3;p115.133">Implementing layers can be done fairly simply, I believe, by just making a few small changes to the section of done which handles synapse selection, which limit where the synaptic target is drawn from - this assumes it is possible to setup layers as sets of neurons, whose indices are continuous - allowing for the divide in values between excitory and inhibitory indices. </s></p><p n="p116.133"><s n="s1.4;p116.133">By looking at all the function headers for transfer between functions in the host computer and on the board, I was able to determine that there was a generic structure used in most of the transfers - there was a mention of the neuron referred to, a sub-reference and then a piece of floating point data. </s><s n="s2.4;p116.133">Even if the exact pattern was not exactly met in all circumstances, the data could be manipulated to fit the form - Boolean represented as 0 &amp; 1 integer values, integer values represented as floats etc. </s><s n="s3.4;p116.133">Even requests for data from the other side can be made using this function, as I have imbued the PCI controller with a basic form of processing power, allowing it to direct and pull information. </s><s n="s4.4;p116.133">Having determined this, I plan to implement the PCI bus with one function being involved in all data transfers, it shall have the form: </s></p><p n="p117.133"><s n="s1.1;p117.133">PCI(<hi rend="italic">function,index,subref,data</hi>) </s></p></div3><div3><head rend="underlined">2.5.4. Updated Pseudo-Code</head><p n="p118.133"><s n="s1.1;p118.133"><formula notation="" id="BAWE_6100a-form.026"/> </s></p></div3><div3><head><hi rend="underlined">2.5.5. Updated Code Listings:-</hi> <hi rend="italic">See Appendix 4,5,6 and 7</hi></head><p n="p119.133"/></div3><div3><head rend="underlined">2.5.6. Example of Input/Output</head><p n="p120.133"><s n="s1.1;p120.133">Here is a screen shot of the displayed output: </s></p><figure id="BAWE_6100a-pic.001"><head rend="italic">[Fig 13] Example of Screen Output</head></figure><p n="p121.133"><s n="s1.3;p121.133">To show the input and output as text is realistically not possible, as it would fill a small library if the program was merely left to run for five minutes. </s><s n="s2.3;p121.133">I would display it as a sequence of graphs, but have not found time to write the relevant MATLAB code yet. </s><s n="s3.3;p121.133">Here is a brief look at what some of the output looks like: </s></p><p n="p122.133"><s n="s1.1;p122.133"><formula notation="" id="BAWE_6100a-form.027"/> </s></p><p n="p123.133"><s n="s1.1;p123.133">The values on the left are the number of the neuron, and those on the right are the time of each spike in milliseconds. </s></p></div3><div3><head rend="underlined">2.5.7. Implementation Issues</head><p n="p124.133"><s n="s1.4;p124.133">When I tried generated an input file the same length as the main control file, the file reached ridiculous sizes in the region of 50 megabytes. </s><s n="s2.4;p124.133">I therefore settled for a quick fix of simply reducing the number of seconds simulated. </s><s n="s3.4;p124.133">In many circumstances though, inputs would be generated either randomly, based on some function made internal to the program or fed in from some external sensor generated at real time. </s><s n="s4.4;p124.133">Because of this I don't consider this a great problem. </s></p><p n="p125.133"><s n="s1.4;p125.133">After adding the PCI calls, the code became a lot harder to read and understand, as the "English speak" function names were now replaced by a block standard function call, and while it is possible to differentiate between them by their function numbers - it's simply, a lot harder. </s><s n="s2.4;p125.133">This added even further to the complexity of debugging and testing this new version of the program. </s><s n="s3.4;p125.133">The result of this highly complex testing and a lack of time has been that I ran out of time to finish implementing this version. </s><s n="s4.4;p125.133">The neural network still has a bug in it, resulting in neurons not being fully affected by spikes hitting it, and also the layered topology has not been implemented at all. </s></p></div3><div3><head rend="underlined">2.5.8. Testing</head><p n="p126.133"><s n="s1.5;p126.133">Testing with this new version progressed in much the same way as the last, with each step leading onto the next possible step along the trail. </s><s n="s2.5;p126.133">As pieces of data were taking long journeys from being created through the neurons, up an axon, across the PCI board, into the hit list, into the activation function etc, finding out what part wasn't actually working was not approachable in terms of standard testing, and noting each individual test down would have slowed it down to an unworkably low speed. </s><s n="s3.5;p126.133">Suspicions just had to be clarified and leads traced back to their roots, as and when they popped up. </s><s n="s4.5;p126.133">While I did solve some errors - such as a mix-up in the PCI unit over whether a 0 or 1 signified an excitory neuron was causing the neurons to be setup wrong. </s><s n="s5.5;p126.133">In the end though, it was not possible to fix all the bugs before the time was up. </s></p></div3></div2></div1><div1 type="section"><head rend="underlined bold">3. Conclusion</head><p n="p127.133"><s n="s1.4;p127.133">When all is said and done, I am very proud of this project, even though it is not really complete and has much work left to go on it. </s><s n="s2.4;p127.133">It has been a fascinating task and I have learnt a huge amount from working on it. </s><s n="s3.4;p127.133">When I embarked upon it, my knowledge of spiking neural networks was next to non-existent and it was the perfect way to learn more about them. </s><s n="s4.4;p127.133">While battling through mountains of data trying to debug it did get extremely frustrating, it has given me a good, clear understanding of the processes that occur within the neurons. </s></p><p n="p128.133"><s n="s1.4;p128.133">Due to the enourmous amounts of data generated, and calculations done by the program it forced me to deal with certain issues I had never had to consider before in my time as a programmer. </s><s n="s2.4;p128.133">How to optimize the speeds of processes, selecting functions that can operate at the lowest order of time possible - never before had I had the need to really consider these things, and it brought into perspective all that I have learnt recently in the algorithms module. </s><s n="s3.4;p128.133">And while last programs of mine have generated multidimensional, dynamic arrays of regularly changing data - it has never quite been on this scale before. </s><s n="s4.4;p128.133">The methods I found and used to order my thoughts, and the data in such a way that I could handle debugging was an important lesson for me. </s></p><p n="p129.133"><s n="s1.2;p129.133">While a few of the requirements and parts of the specification have not quite been met - for instance, the interface is not quite up to the standard suggested, only four types of neurons are possible and a layered topology is not implemented, I consider this to be simply part of the nature of the program. </s><s n="s2.2;p129.133">There will always be more work that can be done with it - and as soon as I can find the time, I have every intention of continuing through with it, to add as many of them in as I can. </s></p><p n="p130.133"><s n="s1.2;p130.133">There are many more features that I wish to add to it yet. </s><s n="s2.2;p130.133">A full SDL interface, real time graph plotting of spikes, multithreading for the different neurons, more user control over network setup and types of neurons and different ways of entering input are just the start of what I want to do with them - and they can only really be done once I've filled in the holes within the current specification. </s></p><p n="p131.133"><s n="s1.3;p131.133">There are still two key bugs within the program, but these should be easily dealt with. </s><s n="s2.3;p131.133">I will be removing the PCI interface from my version, and returning it to a purely software oriented program - this will remove the 'no-secondary-fire' problem from the second version. </s><s n="s3.3;p131.133">The other problem - that synaptic weights quickly reach their limits, can be dealt with by making changes to them occur through the use of a derivative, which slows down the rate at which they change, stretching it slowly across a few seconds. </s></p><p n="p132.133"><s n="s1.1;p132.133">I do feel that the level and range of the actual programming demonstrated within the program was unfortunately fairly low, but due to the huge amount of research, testing and debugging required I did not find time to really push the limits, as this would have required a greater range of functionality to make room, and require a wider range of programming skills. </s></p><p n="p133.133"><s n="s1.4;p133.133">If I was to do it again I would make it easier to perform the testing. </s><s n="s2.4;p133.133">By adding in a separate function or so, which could adjust certain values, and output strings of values, which again and again I found myself having to adjust and code for, I think I could have cut a large amount of the development time. </s><s n="s3.4;p133.133">But as they say - we live and learn. </s><s n="s4.4;p133.133">Or more precisely - our neural nets do. </s></p></div1></body><back><div1 type="bibliography"><head rend="underlined bold">4. Bibliography</head><p><hi rend="sup">1</hi> "Which Model to Use for Cortical Spiking Neurons" - Eugene M Izhikevich [2004]</p><p><hi rend="sup">2</hi> "Spiking Neural Networks, an Introduction" - Jilles Kreeken</p><p><hi rend="sup">3</hi> "Spike Based Strategies for Rapid Processing" - Thorpe et Al [2001] </p><p><hi rend="sup">4</hi> "Simple Model of Spiking Neurons" - Eugene M Izhikevich [2003]</p><p><hi rend="sup">5</hi> "Polycronization: Computation with Spikes" - Eugene M Izhikevich [2005]</p><p><hi rend="sup">6</hi> "Spike-Timing Dependant Plasticity and Relevant Mutual Information Maximization" - Gal Chechik [2003]</p><p><hi rend="sup">7</hi> "Why implement a neural model in silicon?" - Leslie Smith [2004]</p><p><hi rend="sup">8</hi> "Custom VLSI ASIC for Automotive Applications with Recurrent Networks" - R. Tawel and N. Aranki</p><p><hi rend="sup">9</hi> "Design and Simulation of a Digital Neuro-processor Chip Module: Decay Module" - Rafael Lopez Garcia del Valle [1999]</p><p><hi rend="sup">10</hi> "Simulation of a Digital Neuro-Chip for Spiking Neural Networks" - T.Schoenauer</p><p><hi rend="sup">11</hi> Original spNET C code - <seg type="URL" n="www.nsi.edu/users/izhikevich/publications/spnet.cpp"/></p></div1><div1 type="appendix"><head rend="underlined bold">5. Appendix</head><p/></div1></back></text></TEI.2>
