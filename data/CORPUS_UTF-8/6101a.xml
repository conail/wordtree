<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE TEI.2 SYSTEM "tei_bawe.dtd"><TEI.2 id="_6101a" n="version 1.0"><teiHeader><fileDesc><titleStmt><title>Computer Vision Assignment: Image Compression</title></titleStmt><extent/><publicationStmt><distributor>British Academic Written English (BAWE) corpus</distributor><availability><p>The British Academic Written English (BAWE) corpus was developed at the Universities of Warwick, Reading and Oxford Brookes, under the directorship of Hilary Nesi and Sheena Gardner (formerly of the Centre for Applied Linguistics [previously called CELTE], Warwick), Paul Thompson (Department of Applied Linguistics, Reading) and Paul Wickens (Westminster Institute of Education, Oxford Brookes), with funding from the ESRC. Subject to the rights of the these institutions in the BAWE corpus, and pursuant to the ESRC agreement, the BAWE corpus is available to researchers for research purposes PROVIDED THAT the following conditions are met:</p><p>1. The corpus files are not distributed in either their original form or in modified form.</p><p>2. The texts are used for research purposes only; they should not be reproduced in teaching materials.</p><p>3. The texts are not reproduced in full for a wider audience/readership, although researchers are free to quote short passages of text (up to 200 running words from any given text).</p><p>4. The BAWE corpus developers (contact: Hilary Nesi) are informed of all projects, dissertations, theses, presentations or publications arising from analysis of the corpus.</p><p>5. Researchers acknowledge their use of the corpus using the following form of words: "The data in this study come from the British Academic Written English (BAWE) corpus, which was developed at the Universities of Warwick, Reading and Oxford Brookes under the directorship of Hilary Nesi and Sheena Gardner (formerly of the Centre for Applied Linguistics [previously called CELTE], Warwick), Paul Thompson (Department of Applied Linguistics, Reading) and Paul Wickens (Westminster Institute of Education, Oxford Brookes), with funding from the ESRC (RES-000-23-0800)."</p></availability></publicationStmt><notesStmt><note resp="British Academic Written English (BAWE) corpus project">Page header contains: student name; module code. 
Page footer contains: page number. </note><note resp="British Academic Written English (BAWE) corpus project">Appendix: Code Listing
- not available in corpus</note><note resp="British Academic Written English (BAWE) corpus project">Running text contains reference to external or missing appendix.
</note></notesStmt><sourceDesc><p n="level">3</p><p n="date">2006-03</p><p n="module title">Computer Vision</p><p n="module code">CS3G2</p><p n="genre family">Design specification</p><p n="discipline">Computer Science</p><p n="disciplinary group">PS</p><p n="grade">D</p><p n="number of authors">1</p><p n="number of words">2007</p><p n="number of s-units">92</p><p n="number of p">50</p><p n="number of tables">3</p><p n="number of figures">8</p><p n="number of block quotes">0</p><p n="number of formulae">0</p><p n="number of lists">2</p><p n="number of paragraphs formatted like lists">0</p><p n="abstract present">abstract present</p><p n="average words per s-unit">21.8</p><p n="average s-units per p">1.8</p><p n="macrotype of assignment">simple assignment</p></sourceDesc></fileDesc><encodingDesc><p>TEI P4 (documented in: BAWE.documentation.pdf)</p></encodingDesc><profileDesc><particDesc><person><p n="gender">m</p><p n="year of birth">1984</p><p n="first language">English</p><p n="education">UKa</p><p n="course">Computer Science</p><p n="student ID">6101</p></person></particDesc></profileDesc></teiHeader><text><front><titlePage><docTitle><titlePart rend="bold">Computer Vision Assignment: Image Compression</titlePart></docTitle></titlePage><div1 type="toc" n="2"><head rend="bold">Contents</head><p/></div1></front><body><div1 type="abstract"><head rend="bold">Abstract</head><p n="p1.50"><s rend="italic" n="s1.4;p1.50">This paper serves as a project report for an assignment into image compression. </s><s rend="italic" n="s2.4;p1.50">The task was to compress 256-pixel square greyscale images, with lossless techniques or not, into a smaller file. </s><s rend="italic" n="s3.4;p1.50">I achieved the aim using a combination of run length encoding, reducing the number of greys in the image, and applying the Huffman technique. </s><s rend="italic" n="s4.4;p1.50">This report documents this process, and contains the results. </s></p></div1><div1 type="section"><head rend="bold">Introduction</head><p n="p2.50"><s n="s1.2;p2.50">The brief was to take a 256-pixel square greyscale image, for example <hi rend="italic">Fig.</hi> </s><s n="s2.2;p2.50"><hi rend="italic">1</hi>, and compress them as much as possible without loosing too much information about the face. </s></p><figure id="BAWE_6101a-pic.001"><head rend="italic">Fig. 1 - an original image</head></figure></div1><div1 type="section"><head rend="bold">Method</head><p n="p3.50"><s n="s1.2;p3.50">I will split the method into the three different compression techniques. </s><s n="s2.2;p3.50">They are applied in the order they are given during compression, and, obviously, in reverse for decompression. </s></p><div2><head rend="bold">Grey Level Decrease</head><p n="p4.50"><s n="s1.2;p4.50">Because I was aiming to write a compression algorithm that would encode and decode pictures of faces, I decided that the background was of little importance relative to the face itself. </s><s n="s2.2;p4.50">By experimentation, I found that 32 levels of grey were all that were needed to represent the face in a reasonable amount of quality. </s></p><p n="p5.50"><s n="s1.2;p5.50">The main negative effect of reducing the colour depth is that gradients become stepped. </s><s n="s2.2;p5.50">As faces usually do not contain many gradients, the only place this should be noticeable is in the background, which shouldn't matter so much. </s></p><p n="p6.50"><s n="s1.2;p6.50">Please refer to <hi rend="italic">Fig.</hi> </s><s n="s2.2;p6.50"><hi rend="italic">2</hi> for the same image, but represented with 32 greys. </s></p><figure id="BAWE_6101a-pic.002"><head rend="italic">Fig. 2 - Image reduced to 32 greys</head></figure></div2><div2><head rend="bold">Run Length Encoding</head><p n="p7.50"><s rend="italic" n="s1.2;p7.50">Fig. </s><s n="s2.2;p7.50"><hi rend="italic">3</hi>, below, shows a detail of this image taken from the top left corner: </s></p><figure id="BAWE_6101a-pic.003"><head rend="italic">Fig 3. Detail of Fig. 2</head></figure><p n="p8.50"><s n="s1.2;p8.50">As you can see, there are very large patches of the same level of grey. </s><s n="s2.2;p8.50">In this case, it is very wasteful to represent every pixel with an 8-bit number (or 6-bit for 32 grey levels), when we can describe the patches themselves. </s></p><p n="p9.50"><s n="s1.1;p9.50">This idea gives several options, most notably run length and quad tree encoding. </s></p><p n="p10.50"><s n="s1.4;p10.50">The na√Øve view of quad tree encoding is that the square regions of the same colour get boxed together and labelled as one entity in the tree. </s><s n="s2.4;p10.50">In fact, of course, this is not the case. </s><s n="s3.4;p10.50">The boundaries of each region are set arbitrarily by cutting the image recursively into quadrants. </s><s n="s4.4;p10.50">Therefore, an area of constant colour could be described by many regions, if the cuttings do not happen to fall in the best places, and the solution will not be optimal. </s></p><p n="p11.50"><s rend="italic" n="s1.3;p11.50">Fig. </s><s n="s2.3;p11.50"><hi rend="italic">4</hi> is a small detail of <hi rend="italic">Fig.</hi> </s><s n="s3.3;p11.50"><hi rend="italic">2</hi> taken at random from the top right of the left eye. </s></p><figure id="BAWE_6101a-pic.004"><head><hi rend="italic">Fig. 4</hi> - A small detail of <hi rend="italic">Fig. 2</hi></head></figure><p n="p12.50"><s n="s1.2;p12.50">This is typical of the face part of the image in 32 greys (the background is generally more patchy, but more of the images are face than background). </s><s n="s2.2;p12.50">There are definite regions of constant shade, but none of them particularly large. </s></p><p n="p13.50"><s n="s1.1;p13.50">If you apply quad tree encoding to this, you get the following: </s></p><figure id="BAWE_6101a-pic.005"><head rend="italic"> Fig. 5 - The boundaries given by quad tree encoding</head></figure><p n="p14.50"><s n="s1.1;p14.50">As the regions of grey are not conveniently placed and sized, and as they are not particularly large, quad tree encoding, as can readily be seen, has little effect. </s></p><p n="p15.50"><s n="s1.2;p15.50">In the example of the region above, there would be 233 leaf nodes to the tree. </s><s n="s2.2;p15.50">This means that there will be 233 8-bit numbers to be stored, as well as the tokens in the non-leaf nodes. </s></p><figure id="BAWE_6101a-pic.006"><head rend="italic"> Fig. 6 - Boundaries given by run length encoding</head></figure><p n="p16.50"><s rend="italic" n="s1.4;p16.50">Fig. </s><s n="s2.4;p16.50"><hi rend="italic">6</hi> shows the boundaries given by run length encoding. </s><s n="s3.4;p16.50">There are 87 regions in this example, and for each of these two items need to be stored, the intensity and length of the region. </s><s n="s4.4;p16.50">Each of these can be stored in an 8-bit number; so 174 values will classify this image. </s></p><p n="p17.50"><s n="s1.1;p17.50">As this example shows, where the regions of colour are reasonably large but not very large, run length encoding is better than quad tree. </s></p></div2><div2><head rend="bold">Huffman Encoding </head><figure id="BAWE_6101a-fig.001"><head rend="italic">Fig. 7 - Histogram of Face Image</head></figure><p n="p18.50"><s n="s1.3;p18.50">The histogram of <hi rend="italic">Fig.</hi> </s><s n="s2.3;p18.50"><hi rend="italic">7</hi> shows that some levels of grey are dramatically more frequent than others. </s><s n="s3.3;p18.50">In fact, the standard deviation of the level frequencies is, in this example, 53.0. </s></p><p n="p19.50"><s n="s1.1;p19.50">If the five grey levels that make the central peak were all described with short codes, and the other grey levels with longer codes, the result will obviously be more efficient. </s></p><p n="p20.50"><s n="s1.3;p20.50">This fact applies even more to the lengths of the codes. </s><s n="s2.3;p20.50">The longest run in this particular image is 76 pixels, but the vast majority, as can be seen by the following histogram, are under 10. </s><s n="s3.3;p20.50">Even if there were an encoding method did nothing other than allowing one bit for a run length of 1, and an 8-bit code starting with a zero for everything else, there would still be 99,400 bits saved. </s></p><figure id="BAWE_6101a-fig.002"><head rend="italic">Fig. 8 - Histogram of run length frequencies</head></figure><p n="p21.50"><s n="s1.1;p21.50">To compress the image further, therefore, the Huffman technique was employed. </s></p><p n="p22.50"><s n="s1.1;p22.50">The pseudo-code for the implementation of Huffman encoding used to encode a string of binary values (in 8 bit sections) is as follows: </s></p><list type="ordered"><item>Split the input string into strings of 8 bit</item><item>Find the frequency of each string</item><item>Put a frequency table at the start of the output string. This is made of 256 16-bit binary numbers.</item><item>Initialise a forest of 256 trees. Each of them will have a value between 0 and 255, and the frequency of that value as a weight.</item><item>Repeat until there is only one tree:</item><item>Find the two trees with minimum weights</item><item>Create a new tree, and put these two trees as its children. </item><item>Assign a weight to this new parent of the sum of the children's weights.</item><item>Reduce the children's weights to zero</item><item>For every 8-bit string in the input:</item><item>Find the node in the tree with the string's value. Let's call it CurrentNode.</item><item>Create a temporary string</item><item>If CurrentNode is the left-hand child of its parent, add a 0 to the temporary stream, otherwise add a 1.</item><item>Repeat until we are at the root:</item><item>Reassign CurrentNode to its parent</item><item>If the new CurrentNode is the left-hand child of its parent, add a 0 to the temporary stream, otherwise add a 1.</item><item>Reverse the temporary stream</item><item>Append the temporary stream to the output stream</item></list><p n="p23.50"><s n="s1.2;p23.50">When it is decoded, the tree is built from the frequency table at the start of the string, using the same method as steps 4 and 5 above. </s><s n="s2.2;p23.50">As there is no random element to either process, the tree will always be the same. </s></p><p n="p24.50"><s n="s1.1;p24.50">After the tree is built, the string (without the frequency table) can be decoded with: </s></p><list type="ordered"><item>Repeat until the end of the input string has been reached:</item><item>Create a variable called CurrentNode. Let it point to the root.</item><item>Repeat until we reach a leaf node:</item><item>If the next character of the input string is 0 then reassign CurrentNode to its left child</item><item>If the next character of the input string is 1 then reassign CurrentNode to its right child</item><item>Advance our position in the input string by one character</item><item>Put the next number on the output stream as the value of the leaf.</item></list><p n="p25.50"><s n="s1.2;p25.50">One idea for Huffman encoding was to encode the run lengths and intensities separately, so that each would have its own frequency table. </s><s n="s2.2;p25.50">The reasoning for this was that the frequencies would be more accurate if they were separate. </s></p><p n="p26.50"><s n="s1.3;p26.50">After further thought, it can be seen that that idea is wrong. </s><s n="s2.3;p26.50">The frequencies of the data are absolute, no matter what they represent. </s><s n="s3.3;p26.50">That idea would have just resulted in an extra frequency table. </s></p></div2></div1><div1 type="section"><head rend="bold">Results</head><p n="p27.50"><s n="s1.2;p27.50">For the results of each technique, I shall test them on a single image. </s><s n="s2.2;p27.50">Obviously, exact compression ratios vary image to image, so I shall give a set of compression ratios for different images in the Overall Compression section. </s></p><p n="p28.50"><s n="s1.3;p28.50">For the purposes of examining the techniques individually, it is necessary to give their results individually. </s><s n="s2.3;p28.50">This, however, causes a problem as the compression achieved by each of the techniques is dependent on the compression that has already been achieved. </s><s n="s3.3;p28.50">I will try to break it down as much as possible, however. </s></p><div2><head rend="bold">Grey Levels</head><p n="p29.50"><s n="s1.3;p29.50">This is the simplest to evaluate. </s><s n="s2.3;p29.50">Given a 256 square image, there will be 65,536 pixels. </s><s n="s3.3;p29.50">If each of them were assigned a 6-bit binary number (0 - 31) then this would take 393,216 bits, or 49.152kb. </s></p><p n="p30.50"><s n="s1.2;p30.50">In this application, the reduction of grey levels does not have this effect quite though, as, for the convenience of Huffman, they are each encoded in 8 bits. </s><s n="s2.2;p30.50">Huffman then compensates for this inefficiency by assigning longer codes for the values over 32, which are never used, and short codes for values under 32. </s></p></div2><div2><head rend="bold">Run Length</head><p n="p31.50"><s n="s1.2;p31.50">When the image has 256 levels of grey, run length encoding "compresses" the image up to 77.484kb. </s><s n="s2.2;p31.50">This is because with so many levels of grey, there are very few runs. </s></p><p n="p32.50"><s n="s1.1;p32.50">When the grey levels have been reduced to 32, run length encoding (while still assigning 8 bits to both the intensity and length of each run) compresses the image down to 27.029kb. </s></p></div2><div2><head rend="bold">Huffman Encoding</head><p n="p33.50"><s n="s1.1;p33.50">When the image has 256 levels of grey with no run length encoding, Huffman compresses the image down to 54.708kb. </s></p><p n="p34.50"><s n="s1.1;p34.50">When the image has 32 levels of grey (each represented with 8 bits), and no run length encoding, Huffman compresses the image down to 31.941kb. </s></p></div2><div2><head rend="bold">Overall Compression</head><p n="p35.50"><s n="s1.1;p35.50">When all three techniques are applied together, the image is compressed down to 14.480kb. </s></p><p n="p36.50"><s n="s1.1;p36.50">Here is an overall comparison of the three techniques: </s></p><table id="BAWE_6101a-tab.001"><row><cell/></row></table><p n="p37.50"><s n="s1.2;p37.50">Actually, you can divide each of those percentages by three, as the original image had a red, green and blue stream, each of which stored the intensity of the grey. </s><s n="s2.2;p37.50">This brought the original file size up to 196.608kb. </s></p><p n="p38.50"><s n="s1.2;p38.50">This means that the final compression ratio of [Original Image : Compressed Image] is 0.0736523 : 1. </s><s n="s2.2;p38.50">Alternatively, that could be expressed as 1 : 13.5773085. </s></p><p n="p39.50"><s n="s1.2;p39.50">The compression time is 0.125 seconds. </s><s n="s2.2;p39.50">The decompression time is 0.101 seconds. </s></p><p n="p40.50"><s n="s1.2;p40.50">All the statistics above are based on a single image, however. </s><s n="s2.2;p40.50">Here are the compression ratios (as a percentage of the original 196.608kb file) for the 8 test images: </s></p><table id="BAWE_6101a-tab.002"><row><cell/></row></table><p n="p41.50"><s n="s1.2;p41.50">It is interesting to note that the subjects of tests 7 and 8 were black, so this perhaps indicates that an image of a black person is compressed better using this technique than that of a white person. </s><s n="s2.2;p41.50">This is either an indication that there is less variety of tone in a black person's skin, or that the lighting conditions and camera were such that less detail was picked up. </s></p><p n="p42.50"><s rend="italic" n="s1.2;p42.50">Figs. </s><s n="s2.2;p42.50"><hi rend="italic">9 and 10</hi> (overleaf) show an example image before and after compression. </s></p><figure id="BAWE_6101a-pic.007"><head rend="italic">Fig. 9 - Test 1 before compression </head></figure><figure id="BAWE_6101a-pic.008"><head rend="italic">Fig. 10 - Test 1 after compression</head></figure></div2></div1><div1 type="section"><head rend="bold">Comparison With Other Techniques</head><p n="p43.50"><s n="s1.2;p43.50">I shall compare my typical compression ratio with those of other techniques. </s><s n="s2.2;p43.50">First, I will (quite unfairly) compare it to lossless techniques. </s></p><table id="BAWE_6101a-tab.003"><row><cell/></row></table></div1><div1 type="section"><head rend="bold">Future Development</head><p n="p44.50"/></div1><div1 type="section"><head rend="bold">Conclusions</head><p n="p45.50"><s n="s1.1;p45.50">If one were to put an original image next to an image that has been compressed then decompressed under my technique, the only difference that would be noticeable would be the stepped gradient in the background, as opposed to the gradual gradient of the original. </s></p><p n="p46.50"><s n="s1.1;p46.50">As has been discussed before, the background is not of huge concern, so therefore the compressed images are valid. </s></p><p n="p47.50"><s n="s1.1;p47.50">The compression ratio of between 7 and 13% is very good, and a computing time of between 0.1 and 0.2 seconds for both compression and decompression is negligible. </s></p><p n="p48.50"><s n="s1.2;p48.50">The compression ratio is respectable, relative to the well-used standard techniques. </s><s n="s2.2;p48.50">It is obviously not as good as the JPEG standards, but that is far more complex than mine. </s></p><p n="p49.50"><s n="s1.1;p49.50">I have no statistics to this effect, but I would guess my technique to be faster than some of the others that achieve better compression ratios. </s></p><p n="p50.50"><s n="s1.1;p50.50">As 1 : 13 is a good rate of compression without losing psychovisually important information of the face, and as the compression time is very small, I therefore deem the project to be a success. </s></p></div1></body><back><div1 type="back text"><head rend="bold">Acknowledgements</head><p>As I used Windows bitmap images for this project, I used the open source EasyBMP library for their import and export.</p><p>I based functions that I wrote to display the bits within a byte of data, and to convert an integer into a string of binary characters, loosely upon ones from the Internet.</p><p>The entirety of the rest of the code was mine.</p></div1><div1 type="appendix"><head rend="bold">Appendix: Code Listing</head><p/></div1><div1 type="missing or external appendix"><p/></div1></back></text></TEI.2>
