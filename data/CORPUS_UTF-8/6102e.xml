<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE TEI.2 SYSTEM "tei_bawe.dtd"><TEI.2 id="_6102e" n="version 1.0"><teiHeader><fileDesc><titleStmt><title>FLYING BRAIN</title></titleStmt><extent/><publicationStmt><distributor>British Academic Written English (BAWE) corpus</distributor><availability><p>The British Academic Written English (BAWE) corpus was developed at the Universities of Warwick, Reading and Oxford Brookes, under the directorship of Hilary Nesi and Sheena Gardner (formerly of the Centre for Applied Linguistics [previously called CELTE], Warwick), Paul Thompson (Department of Applied Linguistics, Reading) and Paul Wickens (Westminster Institute of Education, Oxford Brookes), with funding from the ESRC. Subject to the rights of the these institutions in the BAWE corpus, and pursuant to the ESRC agreement, the BAWE corpus is available to researchers for research purposes PROVIDED THAT the following conditions are met:</p><p>1. The corpus files are not distributed in either their original form or in modified form.</p><p>2. The texts are used for research purposes only; they should not be reproduced in teaching materials.</p><p>3. The texts are not reproduced in full for a wider audience/readership, although researchers are free to quote short passages of text (up to 200 running words from any given text).</p><p>4. The BAWE corpus developers (contact: Hilary Nesi) are informed of all projects, dissertations, theses, presentations or publications arising from analysis of the corpus.</p><p>5. Researchers acknowledge their use of the corpus using the following form of words: "The data in this study come from the British Academic Written English (BAWE) corpus, which was developed at the Universities of Warwick, Reading and Oxford Brookes under the directorship of Hilary Nesi and Sheena Gardner (formerly of the Centre for Applied Linguistics [previously called CELTE], Warwick), Paul Thompson (Department of Applied Linguistics, Reading) and Paul Wickens (Westminster Institute of Education, Oxford Brookes), with funding from the ESRC (RES-000-23-0800)."</p></availability></publicationStmt><notesStmt><note resp="British Academic Written English (BAWE) corpus project">Language used in quote: <foreign id="English">English</foreign></note></notesStmt><sourceDesc><p n="level">3</p><p n="date">2006-03</p><p n="module title">3rd year project</p><p n="module code">CY3P2</p><p n="genre family">Research report</p><p n="discipline">Cybernetics &amp; Electronic Engineering</p><p n="disciplinary group">PS</p><p n="grade">D</p><p n="number of authors">1</p><p n="number of words">2260</p><p n="number of s-units">91</p><p n="number of p">22</p><p n="number of tables">1</p><p n="number of figures">2</p><p n="number of block quotes">1</p><p n="number of formulae">1</p><p n="number of lists">0</p><p n="number of paragraphs formatted like lists">0</p><p n="abstract present">abstract present</p><p n="average words per s-unit">24.8</p><p n="average s-units per p">4.1</p><p n="macrotype of assignment">simple assignment</p></sourceDesc></fileDesc><encodingDesc><p>TEI P4 (documented in: BAWE.documentation.pdf)</p></encodingDesc><profileDesc><particDesc><person><p n="gender">m</p><p n="year of birth">1983</p><p n="first language">Chinese unspecified</p><p n="education">OSa</p><p n="course">MEng</p><p n="student ID">6102</p></person></particDesc></profileDesc></teiHeader><text><front><titlePage><docTitle><titlePart rend="bold">FLYING BRAIN</titlePart></docTitle><titlePart><hi rend="italic"><name type="student name"/></hi> MEng Computer Science and Cybernetics, <name type="other"/></titlePart></titlePage></front><body><div1 type="abstract"><head rend="bold">ABSTRACT</head><p n="p1.22"><s n="s1.5;p1.22">Flying Brain refers to an Unmanned Aerial Vehicle (UAV), which can remain balance autonomously when encountering unpredictable disturbances. </s><s n="s2.5;p1.22">This means that the UAV can regulate its attitude in an environment immediately. </s><s n="s3.5;p1.22">Civil and military applications of autonomous flying vehicles have been steadily increasing over the last few years. </s><s n="s4.5;p1.22">Traffic surveillance, air pollution monitoring, area mapping, agricultural application, and the remote inspection require high maneuverability and robustness with respect to disturbances [1]. For an UAV, the ability of keeping balance by itself instead of been manipulated by a human can make it more intelligent to meet the requirement of the task. </s><s n="s5.5;p1.22">A novel method of using a microcontroller as control centre to process the control loop between a remote controlled helicopter Draganflyer IV （Figure 1.）and a camera based inclinometer sensor will be displayed in this paper. </s></p></div1><div1 type="section"><head rend="bold">INTRODUCTION </head><p n="p2.22"><s n="s1.8;p2.22">Control of UAVs in flight is a large area of interest in current research. </s><s n="s2.8;p2.22">For landed system vehicles, only the forward and backward movement associated with the steering control need be considered. </s><s n="s3.8;p2.22">Whole the system can be supported into a stationary position by the wheels or some similar stuff. </s><s n="s4.8;p2.22">However, for an air dynamic system without ground support, more complicated freedom degrees are on the object. </s><s n="s5.8;p2.22">Meanwhile, three rotational axis pitch, roll and yaw are engaged into the movement system. </s><s n="s6.8;p2.22">In order to maintain a level hover, UAV need feel the impact which cause the incline in both pitch and roll axis. </s><s n="s7.8;p2.22">Inevitably, the vibration from the motor of UAV can not be ignored and it will cause the error when testing incline. </s><s n="s8.8;p2.22">To ensure the craft could maintain balance with the unpredictable disturbance, a sensor system need give the feedback to the control centre to support the measurement of incline promptly and accurately. </s></p><p n="p3.22"><s n="s1.5;p3.22">The novel solution to the problem described utilises a vision device coupled spirit vial. </s><s n="s2.5;p3.22">The sensors' processor retrieves images from the camera, using them to locate the air bubble within the search environment.[2] This project is partly finished by Chris Vanstone [3], what he has done is that successfully establish a camera based inclinometer. </s><s n="s3.5;p3.22">In order to measure the incline of a UAV in both axis to allow autonomous hover, which require a fresh rate of 10Hz or greater, at a resolution of 6' over a ±5°range. </s><s n="s4.5;p3.22">The prototype built utilises a small black and white CCD array camera to retrieve images in composite video format which are then analysed in a C++ designed program to locate the bubble position in the dual vial. </s><s n="s5.5;p3.22">This inclinometer provided results to indicate that the novel approach could generate measurements with an accuracy and precision sufficient for the application. [2] </s></p><figure id="BAWE_6102e-fig.001"><head><hi rend="bold">Figure 1.</hi> Outline of DraganFlyer features [4]</head></figure><p n="p4.22"><s n="s1.4;p4.22">So the sensor's measurement on the vial which indicates the incline can be produced and displayed on the PC. To control the present UAV which has DC motors coupled with four blades, the feedback information from the camera sensor should be converted into the control signal for the motors. </s><s n="s2.4;p4.22">When the UAV flying, it need test the level from sensor and analyse the result. </s><s n="s3.4;p4.22">However, the information retrieved by the camera attached on the UAV has to be transmitted into a computer (PC), therefore it is not effective enough. </s><s n="s4.4;p4.22">Moreover, the UAV has to carry the power and data tethers, which are for camera interfacing with pc, which make the whole device not entirely mobile. </s></p><p n="p5.22"><s n="s1.4;p5.22">To solve this problem, microcontroller can be utilised to be the control centre replacing PC. It is able to receive the information from inclinometer sensor and design a control algorithm to calculate out four different Pulse-Width-Modulations(PWM) that can manipulate four blades to keep a constant attitude of this UAV. Unfortunately, the camera supported is VCM 3612-00 CCD camera that retrieves images in black and white converting them into composite video format. </s><s n="s2.4;p5.22">It's hard for microcontroller which can process the data format signal to take in the video signal and analyse it. </s><s n="s3.4;p5.22">So the CMUcam (Figure 2.) which is a kind of robot vision sensor is used to substitute for VCM 3612-00. </s><s n="s4.4;p5.22">In the following paragraph, a novel approach working with CMUcam and microcontroller for the UAV will be illuminated. </s></p></div1><div1 type="section"><head rend="bold">DESIGN</head><p n="p6.22"><s n="s1.1;p6.22">In this project, the CMUcam coupled with the dual vial which is the main component of sensor system and PIC (a kind of microcontroller) with its circuit board are mainly introduced. </s></p><figure id="BAWE_6102e-pic.001"><head><hi rend="bold">Figure 2.</hi> CMUcam</head></figure><div2><head rend="bold">2.1. CMUcam on this project</head><quote lang="English"> "CMUcam is a new low-cost, low-power sensor for mobile robots. You can use CMUcam vision system to do many different kinds of on-board, real-time vision processing. Because CMUcam uses a serial port, it can be directly interfaced to other low-power processors such as PIC chips." [5]</quote><p n="p7.22"><s n="s1.4;p7.22">Far vital function of CMUcam is that it can be used to track the color. </s><s n="s2.4;p7.22">The color blob tracking algorithm allows the user to enter a minimum and maximum threshold for each three RGB or YCrCb channel values, depending on how the camera is set at beginning. </s><s n="s3.4;p7.22">Each pixel is compared against the user specified bounds. </s><s n="s4.4;p7.22">The best performance of the CMUcam can be achieved when tracking a colored object in the highly contrasting environment. </s></p><figure id="BAWE_6102e-pic.002"><head><hi rend="bold">Figure 4 .</hi> Tracking colour</head></figure><p n="p8.22"><s n="s1.5;p8.22">CMUcam consists of CMOS camera chip with a built in microcontroller. </s><s n="s2.5;p8.22">"A major advantage of CMOS versus CCD camera technology is the ability to integrate additional circuitry on the same die as the sensor itself. </s><s n="s3.5;p8.22">This makes it possible to integrate the analog to digital converters and associated pixel grabbing circuitry so a separate frame grabber is not needed".[6] Digital format stream is what is needed. </s><s n="s4.5;p8.22">All image data is processed by a high speed, low cost microcontroller which waits for incoming data to stream from the camera and processes it in real time. </s><s n="s5.5;p8.22">And then it can output high level information to the outside world via an asynchronous serial interface, so it is easy for CMUcam to communicate to a processor such as PC and any kind of microcontroller. </s></p><figure id="BAWE_6102e-fig.002"><head><hi rend="bold">Figure 3.</hi> Exploded diagram of the completed inclinometers [2]</head></figure><p n="p9.22"><s n="s1.9;p9.22">To track the position of the air bubble in the vial, after fixing the CMUcam on the above of the dual vial," a dark room" was produced to cover the sensor system. </s><s n="s2.9;p9.22">Furthermore, the bubble is illuminated by the light array from the top of the sin bar which is produced to be the stage in experiment. </s><s n="s3.9;p9.22">So the "eye sight" of the CMUcam now, is the green vial with a black ring indicating the air bubble. </s><s n="s4.9;p9.22">When the sensor which is a part of the UAV is in stationary (balance) position, the black ring should stay at the center of the vial, otherwise the black ring will move around in the vial. </s><s n="s5.9;p9.22">So the configuration of the camera at beginning is set to track the green color and then output the center of the vial which is constant data. </s><s n="s6.9;p9.22">It is used to compare with the center of the bubble to locate it. </s><s n="s7.9;p9.22">Setting the black color for the camera will get the centroid of the bubble. </s><s n="s8.9;p9.22">It is easy for the controller to get the error between them. </s><s n="s9.9;p9.22">After many experiments the threshold of these two colors is figured out. </s></p><table id="BAWE_6102e-tab.001"><row><cell/></row></table><p n="p10.22"><s n="s1.3;p10.22">When it is tracking, the output from the CMUcam is a data stream at 17 frames per second, which is produced by the camera built in microcontroller. </s><s n="s2.3;p10.22">In each data package, only first 2 data is useful for the UAV. They are the centroid of the tracked object. </s><s n="s3.3;p10.22">So they need to be stored and calculate the control signal. </s></p><p n="p11.22"><s n="s1.1;p11.22">For an UAV, all the above actions should be implemented automatically by embedded system, so an advanced microcontroller is applied for to process the communication with CMUcam and PWM stuff for the motors. </s></p></div2><div2><head rend="bold">2.2. Microcontroller </head><p n="p12.22"><s n="s1.4;p12.22">PIC18f458 is selected to process the control actions. </s><s n="s2.4;p12.22">"This powerful 10 MIPS (100 nanosecond instruction execution) yet easy-to-program (only 77 single word instructions) CMOS Flash-based 8-bit microcontroller packs Microchip's powerful PIC® architecture with a Controller Area Network (CAN 2.0B) peripheral interface into a 64-pin package and is upwards compatible with the PIC16C5X, PIC12CXXX, PIC16CXX and PIC17CXX devices, providing a seamless migration path of software code to higher levels of hardware integration. </s><s n="s3.4;p12.22">The PIC18F6680 features a C compiler-friendly development environment, 1024 bytes of EEPROM, self-programming, an ICD, 2 capture/compare/PWM functions, 12 channels of 10-bit Analog-to-Digital (A/D) converter, 2 comparators, the synchronous serial port can be configured as either 3-wire Serial Peripheral Interface (SPI™) or the 2-wire Inter-Integrated Circuit (I²C™) bus and Addressable Universal Asynchronous Receiver Transmitter (AUSART). </s><s n="s4.4;p12.22">All of these features make it ideal for automotive and industrial applications." [6] </s></p><p n="p13.22"><s n="s1.2;p13.22">For the communication, PIC need send the commands to the CMUcam to set it in roll mode and tell it the threshold of color to track the object after giving the power. </s><s n="s2.2;p13.22">All commands are sent using visible ASCII characters (123 is 3 bytes "123"). </s></p><p n="p14.22"><s n="s1.11;p14.22">The command which is necessary for the CMUcam is RM and TC. RM is used to engage the raw serial transfer Mode. </s><s n="s2.11;p14.22">It reads the bit values of the first 3 (lsb) bits to configure settings. </s><s n="s3.11;p14.22">All bits cleared set the default visible ascii mode. </s><s n="s4.11;p14.22">If bit 0 is set, then all output from the camera is in raw byte packets. </s><s n="s5.11;p14.22">The format of the data packets will be changed so as not to include spaces or be formatted as readable ASCII text. </s><s n="s6.11;p14.22">Instead you will receive a 255 valued byte at the beginning of each packet, the packet identifying character (i.e. </s><s n="s7.11;p14.22">C for a color packet) and finally the packet. </s><s n="s8.11;p14.22">There is no \r sent after each packet, so you must use the 255 to synchronize the incoming data. </s><s n="s9.11;p14.22">Any 255 valued bytes that may be sent as part of the packet are set to 254 to avoid confusion. </s><s n="s10.11;p14.22">If bit 1 is set, the "ACK\r" and "NCK\r" confirmations are not sent. </s><s n="s11.11;p14.22">If bit 3 is set, input will be read as raw byte values, too. [7] </s></p><p n="p15.22"><s n="s1.2;p15.22">The "TC" command begins to track a color. </s><s n="s2.2;p15.22">It takes in the minimum and maximum RGB (CrYCb) values and outputs a type M packet in which the first 2 data is the center position needed. </s></p><p n="p16.22"><s n="s1.6;p16.22">If the camera can receive the commands, it will search the object and track it. </s><s n="s2.6;p16.22">Then, it outputs the data stream to PIC. When the PIC get the each data packet, it need pick up the first 2 data and then send it out to the control section to analyze the bubble location and calculate out control signal. </s><s n="s3.6;p16.22">The communication between PIC and camera is supported by the USART which is used to communicate from the microcontroller to various other devices. </s><s n="s4.6;p16.22">The data is changed all the time. </s><s n="s5.6;p16.22">It refers that the buffer is refreshed quickly when receives the new data packet, so the register should be set bit by bit for buffer to realize this action. </s><s n="s6.6;p16.22">Simultaneously the data should be calculated to get the current position and the movement direction. </s></p><p n="p17.22"><s n="s1.1;p17.22"><formula notation="" id="BAWE_6102e-form.001"/> </s></p><p n="p18.22"><s n="s1.3;p18.22">are used to constraint the control signal. </s><s n="s2.3;p18.22">X and Y are first 2 data in one data stream, which reflects the position of the bubble. △X and △Y are the error between the two near data packet, so it can indicate the direction of movement. </s><s n="s3.3;p18.22">If the data received are not in this area, the PWM has to be set to control the DC motors. </s></p><p n="p19.22"><s n="s1.5;p19.22">Draganfly is a novel helicopter which has four blades, so four PWM are needed. </s><s n="s2.5;p19.22">However one PIC can only provide two channels PWM from CCP and ECCP respectively, so two PIC s are used in this project. </s><s n="s3.5;p19.22">Both two PIC s receive the same digital signal from CMUcam and process exactly same except the PWM stuff. </s><s n="s4.5;p19.22">One PIC control 2 motors locating at the same axis with the pre-programmed PWM duty. </s><s n="s5.5;p19.22">In each PIC, the CCS which is the compiler used for this project has a special command to set CCP channel PWM, but for ECCP channel, it has to be set bit by bit on ECCP register. </s></p></div2><div2><head rend="bold">2.3. Dual axis vial spirit </head><p n="p20.22"><s n="s1.5;p20.22">The present vial is a circular dual axis vial measuring 11mm across with the glass plates taking only 6mm of this. </s><s n="s2.5;p20.22">The top layer is uniformly curved to produce a 1.5º movement in all directions. </s><s n="s3.5;p20.22">This device will show how the dual axis version would work and will give an idea of how good the resolution can get using this method. </s><s n="s4.5;p20.22">The vial contains clear Hexane fluid and is transparent on both sides to allow permeation of light. </s><s n="s5.5;p20.22">Both of the vials used are manufactured and provided by "Level Developments". [8] </s></p></div2></div1><div1 type="section"><head rend="bold">CONCLUSION</head><p n="p21.22"><s n="s1.1;p21.22">The application of CMUcam can make the bubble tracking much easier and it helps calculate out the location of the bubble to share some burdens of PIC. Moreover, it can directly output the digital signal to PIC. From the experiments, it can be seen that the 17 frames per second speed of capture the image is good enough for transmitting movement information of bubble, because when change the level of dual vial, correspondingly the motors speed change swiftly. </s></p><p n="p22.22"><s n="s1.2;p22.22">However, the control algorithm designed in PIC is not optimized, because the finished algorithm can only change the duty of PWM directly to modify the motor speed when the error of location is in the control area without searching the best path to realize the duty change, so for the air system, it is easy to lead the four blades helicopter into pendulum condition. </s><s n="s2.2;p22.22">So the genetic algorithm could be introduced into the current programming to optimize the most effective path to generate the four PWM duty changes. </s></p></div1></body><back><div1 type="back text"><head rend="bold">Acknowledgement: </head><p>William Browne and Victor Becerra are greatly appreciated on this project. They give a lot of brilliant advices on each step both hardware and software. Also all the stuffs in the MADLAB give many assistances on this project. </p></div1><div1 type="bibliography"><head rend="bold">4. REFERENCES</head><p>Redro Castillo, 'Stabilization of a Mini Rotorcraft with Four Rotors', IEEE Control System Magazine </p><p>[2] Chris Vanstone, 'CAMERA BASED INCLINOMETER', Final Report, Reading University, 2005</p><p>[3] Chris Vanstone, MEng Cybernetics, Reading University, siu01cav@rdg.ac.uk</p><p>[4] RCToys, DraganFlyer IV, [Online]. Available: <seg type="URL" n="http://www.rctoys.com/draganflyer4.php"/></p><p>[5] A Low Cost Embedded Color Vision System</p><p>Anthony Rowe1, Charles Rosenberg2, Illah Nourbakhsh [Online]. Available:</p><p><seg type="URL" n="http://www.cs.cmu.edu/~cmucam/Publications/iros-2002.pdf"/> </p><p>[6] MICROCHIP, [Online]. Available: <seg type="URL" n="http://www.microchip.com/"/></p><p>[7] CMUcam, CMUcam vision board <seg type="URL" n="http://www.cs.cmu.edu/~cmucam/Downloads/CMUcamManual_1_8.pdf"/> </p><p>[8] Martin Jones, 2005, Level Developments Ltd,Croyden, UK [Online]. Available: <seg type="URL" n="http://www.leveldevelopments.com"/> </p></div1></back></text></TEI.2>
